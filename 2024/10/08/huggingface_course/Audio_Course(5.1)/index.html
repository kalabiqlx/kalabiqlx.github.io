<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Audio课程（五）- 自动语音识别(ASR) | HUI</title><meta name="author" content="HUI"><meta name="copyright" content="HUI"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;audio-course&#x2F;en&#x2F;  Audio课程（五）- 自动语音识别(ASR) 在本节中，我们将探讨如何使用 Transformers 将语音转换为文本，这一任务被称为语音识别。  语音识别，也称为自动语音识别（ASR）或语音转文本（STT），是最受欢迎和令人兴奋的语音处理任务之一。 它广泛应用于包括口述、语音助手、视频字幕和会">
<meta property="og:type" content="article">
<meta property="og:title" content="Audio课程（五）- 自动语音识别(ASR)">
<meta property="og:url" content="http://example.com/2024/10/08/huggingface_course/Audio_Course(5.1)/index.html">
<meta property="og:site_name" content="HUI">
<meta property="og:description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;audio-course&#x2F;en&#x2F;  Audio课程（五）- 自动语音识别(ASR) 在本节中，我们将探讨如何使用 Transformers 将语音转换为文本，这一任务被称为语音识别。  语音识别，也称为自动语音识别（ASR）或语音转文本（STT），是最受欢迎和令人兴奋的语音处理任务之一。 它广泛应用于包括口述、语音助手、视频字幕和会">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/43670b.png">
<meta property="article:published_time" content="2024-10-08T14:50:33.000Z">
<meta property="article:modified_time" content="2024-10-08T14:41:47.274Z">
<meta property="article:author" content="HUI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/43670b.png"><link rel="shortcut icon" href="/img/122061154_p0_master1200.jpg"><link rel="canonical" href="http://example.com/2024/10/08/huggingface_course/Audio_Course(5.1)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Audio课程（五）- 自动语音识别(ASR)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-08 22:41:47'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/bronya.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/87788970_p0_master1200.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/43670b.png')"><nav id="nav"><span id="blog-info"><a href="/" title="HUI"><img class="site-icon" src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"/><span class="site-name">HUI</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Audio课程（五）- 自动语音识别(ASR)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-08T14:50:33.000Z" title="发表于 2024-10-08 22:50:33">2024-10-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-08T14:41:47.274Z" title="更新于 2024-10-08 22:41:47">2024-10-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/">HuggingFace</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/Audio/">Audio</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Audio课程（五）- 自动语音识别(ASR)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/10/08/huggingface_course/Audio_Course(5.1)/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/10/08/huggingface_course/Audio_Course(5.1)/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>转载自：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/audio-course/en/">https://huggingface.co/learn/audio-course/en/</a></p>
<h1 id="audio课程五-自动语音识别asr"><a class="markdownIt-Anchor" href="#audio课程五-自动语音识别asr"></a> Audio课程（五）- 自动语音识别(ASR)</h1>
<p>在本节中，我们将探讨如何使用 Transformers 将语音转换为文本，这一任务被称为<strong>语音识别</strong>。</p>
<p><img src="asr_diagram.png" alt="语音到文本的示意图"></p>
<p>语音识别，也称为<strong>自动语音识别</strong>（ASR）或<strong>语音转文本</strong>（STT），是最受欢迎和令人兴奋的语音处理任务之一。 它广泛应用于包括口述、语音助手、视频字幕和会议记录在内的多种应用中。</p>
<p>您可能在不知不觉中多次使用过语音识别系统，比如说您智能手机中的数字助手（Siri、Google Assistant、Alexa）！ 当您使用这些助手时，它们首先要做的就是将您的语音转写为书面文本，准备用于各种下游任务（比如为您查询天气预报 🌤️）。</p>
<p>试试下面的语音识别 demo。您可以使用麦克风录制自己的声音，或拖放音频样本文件进行转写：</p>
<iframe src="https://course-demos-whisper-small.hf.space/" frameborder="0" width="850" height="450" data-svelte-h="svelte-aw0ubw" style="box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgb(229, 231, 235); --tw-border-spacing-x: 0; --tw-border-spacing-y: 0; --tw-translate-x: 0; --tw-translate-y: 0; --tw-rotate: 0; --tw-skew-x: 0; --tw-skew-y: 0; --tw-scale-x: 1; --tw-scale-y: 1; --tw-pan-x: ; --tw-pan-y: ; --tw-pinch-zoom: ; --tw-scroll-snap-strictness: proximity; --tw-gradient-from-position: ; --tw-gradient-via-position: ; --tw-gradient-to-position: ; --tw-ordinal: ; --tw-slashed-zero: ; --tw-numeric-figure: ; --tw-numeric-spacing: ; --tw-numeric-fraction: ; --tw-ring-inset: ; --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgb(59 130 246 / .5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; --tw-shadow: 0 0 #0000; --tw-shadow-colored: 0 0 #0000; --tw-blur: ; --tw-brightness: ; --tw-contrast: ; --tw-grayscale: ; --tw-hue-rotate: ; --tw-invert: ; --tw-saturate: ; --tw-sepia: ; --tw-drop-shadow: ; --tw-backdrop-blur: ; --tw-backdrop-brightness: ; --tw-backdrop-contrast: ; --tw-backdrop-grayscale: ; --tw-backdrop-hue-rotate: ; --tw-backdrop-invert: ; --tw-backdrop-opacity: ; --tw-backdrop-saturate: ; --tw-backdrop-sepia: ; --tw-contain-size: ; --tw-contain-layout: ; --tw-contain-paint: ; --tw-contain-style: ; display: block; vertical-align: middle; margin-top: 2.5rem; margin-bottom: 2.5rem; overflow: hidden; border-radius: 0.5rem; max-width: 100%; color: rgb(75, 85, 99); font-family: &quot;Source Sans Pro&quot;, ui-sans-serif, system-ui, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;; font-size: 16.8px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal; background-color: rgb(255, 255, 255); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></iframe>
<p>语音识别是一项具有挑战性的任务，它需要对音频和文本都有所了解。<font color="red">输入的音频可能有很多背景噪音，并且可能由具有各种口音的说话人发出，这使得从中识别出语音变得困难。 书面文本可能包含无声音的字符，如标点符号，这些信息仅从音频中推断很困难。这些都是我们在构建有效的语音识别系统时必须克服的障碍！</font></p>
<p>现在我们已经定义了我们的任务，我们可以开始更详细地研究语音识别。通过本单元的学习，您将对各种可用的预训练语音识别模型有一个良好的基本理解，并了解如何通过 🤗 Transformers 库使用它们。 您还将了解对一个领域或某种特定语言微调 ASR 模型的过程，使您能够为遇到的任何任务构建一个高效的系统。您将能够向您的亲朋好友现场演示您的模型，一个能够将任何语音转换为文本的模型！</p>
<h2 id="语音识别的预训练模型"><a class="markdownIt-Anchor" href="#语音识别的预训练模型"></a> 语音识别的预训练模型</h2>
<p>在本节中，我们将介绍如何使用 <code>pipeline()</code> 来使用预训练的语音识别模型。 在<a target="_blank" rel="noopener" href="https://huggingface.co/learn/audio-course/zh-CN/chapter2/asr_pipeline">第 2 单元</a>中，我们介绍了 <code>pipeline()</code> 这种执行语音识别任务的简便方法，<font color="red"> 所有的预处理和后处理都在幕后进行，而且它还可以灵活地快速实验 Hugging Face Hub 上的任何预训练检查点。 </font>在本单元中，我们将更深入地探索语音识别模型的不同属性，以及利用它们来处理不同的任务的方法。</p>
<p>如第 3 单元详细介绍的那样，语音识别模型大致分为两类：</p>
<ol>
<li><strong>连接时序分类</strong>（Connectionist Temporal Classification, CTC）：<strong>仅编码器</strong>（encoder-only）的模型，顶部带有线性分类（CTC）头</li>
<li><strong>序列到序列</strong>（Sequence-to-sequence, Seq2Seq）：<strong>编码器-解码器</strong>（encoder-decoder）模型，编码器和解码器之间带有交叉注意力机制</li>
</ol>
<p>在 2022 年之前，CTC 是这两种架构中更受欢迎的一种，以 encoder-only 模型为主，例如 Wav2Vec2、HuBERT 和 XLSR 在语音的预训练/微调范式中取得了突破。 大公司如 Meta 和 Microsoft 在大量无标签音频数据上对编码器进行了多天甚至数周的预训练。 用户采用一个预训练的检查点，并在少至 <strong>10 分钟</strong> 的有标注的语音数据上进行微调，就可以在下游语音识别任务中取得强大的性能。</p>
<p><font color="red">然而，CTC 模型也有其缺点。在编码器上附加一个简单的线性层可以得到一个小巧、快速的完整模型，但可能容易出现语音拼写错误。</font>我们将用 Wav2Vec2 模型演示这一点。</p>
<h3 id="探索-ctc-模型"><a class="markdownIt-Anchor" href="#探索-ctc-模型"></a> 探索 CTC 模型</h3>
<p>让我们加载 <a target="_blank" rel="noopener" href="https://huggingface.co/learn/audio-course/zh-CN/chapter5/hf-internal-testing/librispeech_asr_dummy">LibriSpeech ASR</a> 数据集的一小部分，以展示 Wav2Vec2 的语音转写能力：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(</span><br><span class="line">    <span class="string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="string">&quot;clean&quot;</span>, split=<span class="string">&quot;validation&quot;</span></span><br><span class="line">)</span><br><span class="line">dataset</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;file&#x27;</span>, <span class="string">&#x27;audio&#x27;</span>, <span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;speaker_id&#x27;</span>, <span class="string">&#x27;chapter_id&#x27;</span>, <span class="string">&#x27;id&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">73</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>我们可以选择 73 个音频样本中的一个，检查音频样本及其转写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Audio</span><br><span class="line"></span><br><span class="line">sample = dataset[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sample[<span class="string">&quot;text&quot;</span>])</span><br><span class="line">Audio(sample[<span class="string">&quot;audio&quot;</span>][<span class="string">&quot;array&quot;</span>], rate=sample[<span class="string">&quot;audio&quot;</span>][<span class="string">&quot;sampling_rate&quot;</span>])</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND</span><br></pre></td></tr></table></figure>
<p>选择了一个数据样本后，我们现在将一个微调过的检查点加载到 <code>pipeline()</code> 中。 为此，我们将使用官方在 100 小时 LibriSpeech 数据上微调的 <a target="_blank" rel="noopener" href="https://huggingface.co/learn/audio-course/zh-CN/chapter5/facebook/wav2vec2-base-100h">Wav2Vec2 base</a> 检查点：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">pipe = pipeline(<span class="string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="string">&quot;facebook/wav2vec2-base-100h&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们将从数据集中取一个示例，并将其数据传递给 pipeline。由于 <code>pipeline</code> 会 <strong>消耗</strong> 我们传入的字典（意味着它无法被多次读取）， 我们将传递数据的<strong>副本</strong>。这样，我们可以安全地在以下示例中重复使用同一个音频样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe(sample[<span class="string">&quot;audio&quot;</span>].copy())</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;text&quot;</span>: <span class="string">&quot;HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAUS AND ROSE BEEF LOOMING BEFORE US SIMALYIS DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到 Wav2Vec2 模型在转写这个样本方面做得相当好——乍一看似乎是正确的。我们将目标和预测放在一起，突出两者的区别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Target:      HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND</span><br><span class="line">Prediction:  HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH **CHRISTMAUS** AND **ROSE** BEEF LOOMING BEFORE US **SIMALYIS** DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND</span><br></pre></td></tr></table></figure>
<p>将目标文本与预测出的转写进行比较，我们可以看到所有单词 <strong>听起来</strong> 都是正确的，但有些拼写不准确。例如：</p>
<ul>
<li><em>CHRISTMAUS</em>  vs. <em>CHRISTMAS</em></li>
<li><em>ROSE</em>  vs. <em>ROAST</em></li>
<li><em>SIMALYIS</em>  vs. <em>SIMILES</em></li>
</ul>
<p>这突显出了 CTC 模型的缺点:<font color="red">CTC 模型本质上是一个“仅声学”的模型：它由一个编码器组成，该编码器通过输入的音频计算出隐藏状态，并且由一个线性层将隐藏状态映射到字符：</font></p>
<p><font color="red">这意味着该系统几乎完全基于它的声学输入（音频中的语音部分）进行预测，因此倾向于以<strong>表音</strong>的方式进行转写（例如 <strong>CHRISTMAUS</strong>）。 它对上下文的语言建模重视程度较低，因此容易出现语音拼写错误。一个更智能的模型应该发现 <strong>CHRISTMAUS</strong> 不是英语中的有效单词，并在进行预测时将其纠正为 <strong>CHRISTMAS</strong>。 我们的预测还缺少两个重要特征——<strong>大小写</strong>和<strong>标点符号</strong>。这限制了该模型的转写的现实应用。</font></p>
<h3 id="过渡到-seq2seq"><a class="markdownIt-Anchor" href="#过渡到-seq2seq"></a> 过渡到 Seq2Seq</h3>
<p>Seq2Seq 模型来了！正如第 3 单元所概述的，Seq2Seq 模型由<strong>编码器</strong>和<strong>解码器</strong>通过交叉注意力机制相连构成。 编码器的作用与以前相同，负责计算音频输入的隐藏状态表示，而解码器的作用则是 <strong>语言模型</strong>。 解码器处理编码器计算出的整个隐藏状态表示序列，并生成相应的文本转写。它会通过语言建模，在推理时通过输入音频的全局上下文实时纠正拼写错误，避免语音预测的问题。</p>
<p>Seq2Seq 模型有两个<strong>缺点</strong>：</p>
<ol>
<li>解码较慢，因为解码过程是逐步进行的，而不是一次性完成的</li>
<li>需要更多的训练数据才能收敛</li>
</ol>
<p>实际上，大量的训练数据一直是语音领域 Seq2Seq 架构进步的瓶颈。带标注的语音数据很难找到，目前最大的带标注的数据集仅有 10,000 小时。 这一切都在 2022 年 <strong>Whisper</strong> 发布时发生了改变。Whisper 是由 OpenAI 的 Alec Radford 等作者在 <a target="_blank" rel="noopener" href="https://openai.com/blog/whisper/">2022 年 9 月</a> 发布的用于语音识别的预训练模型。 与之前完全在 <strong>无标注</strong> 音频数据上预训练的 CTC 前辈不同，Whisper 是在大量 <strong>有标注</strong> 的音频-转写数据上预训练的，整整有 680,000 小时。</p>
<p>这是比用于训练 Wav2Vec 2.0 的无标签音频数据（60,000 小时）多一个数量级的数据。更重要的是，这些预训练数据中有 117,000 小时是多语言（或“非英语”）数据。 这导致了预训练出的检查点可以应用于 96 种以上语言，其中包括许多 <em>低资源</em> 的语言，即这些语言缺乏适合训练的大型数据集。</p>
<p>当扩展到 680,000 小时有标注的预训练数据时，Whisper 模型显示出强大的泛化能力，适用于许多数据集和领域。 预训练检查点取得了能与最先进的语音识别系统相媲美的结果，在 LibriSpeech 的 test-clean 子集上实现了接近 3% 的词错误率（WER）， 并在 TED-LIUM 上创造了新的最佳记录，WER 为 4.7%（参考 <a target="_blank" rel="noopener" href="https://cdn.openai.com/papers/whisper.pdf">Whisper 论文</a> 中的表 8）。</p>
<p>最重要的是 Whisper 能处理长音频样本、对输入噪声有鲁棒性，且能预测带大小写和标点符号的转写。这使其成为实际应用中可选的语音识别系统之一。</p>
<p>本节的剩余部分将向您展示如何使用 🤗 Transformers 中的预训练 Whisper 模型进行语音识别。 在许多情况下，预训练的 Whisper 检查点表现出色，我们建议在解决任何语音识别问题时第一步先考虑用它。 通过微调，预训练检查点可以针对特定数据集和语言进行调整，以进一步优化效果，我们将在接下来的 <a target="_blank" rel="noopener" href="https://huggingface.co/learn/audio-course/zh-CN/chapter5/fine-tuning">微调</a> 小节中演示如何做到这一点。</p>
<p>Whisper 检查点有五种配置，每种配置的模型大小不同。最小的四个分别在仅英语和多语言数据集上训练，而最大的检查点仅有多语言。 一共九个预训练检查点都可以在 <a target="_blank" rel="noopener" href="https://huggingface.co/models?search=openai/whisper">Hugging Face Hub</a> 上找到。 以下表格总结了这些检查点，并附有 Hub 上模型的链接。“VRAM”表示以最小批次大小（batch size）为 1 时运行模型所需的 GPU 内存。 “Rel Speed”是与最大型号相比的相对速度。根据这些信息，您可以选择最适合您硬件的检查点。</p>
<table>
<thead>
<tr>
<th>大小</th>
<th>参数量</th>
<th>VRAM / GB</th>
<th>相对速度</th>
<th>仅英语</th>
<th>多语言</th>
</tr>
</thead>
<tbody>
<tr>
<td>tiny</td>
<td>39 M</td>
<td>1.4</td>
<td>32</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-tiny.en">✓</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-tiny">✓</a></td>
</tr>
<tr>
<td>base</td>
<td>74 M</td>
<td>1.5</td>
<td>16</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-base.en">✓</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-base">✓</a></td>
</tr>
<tr>
<td>small</td>
<td>244 M</td>
<td>2.3</td>
<td>6</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-small.en">✓</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-small">✓</a></td>
</tr>
<tr>
<td>mdeium</td>
<td>769 M</td>
<td>4.2</td>
<td>2</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-medium.en">✓</a></td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-medium">✓</a></td>
</tr>
<tr>
<td>large</td>
<td>1550 M</td>
<td>7.5</td>
<td>1</td>
<td>x</td>
<td><a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-large-v2">✓</a></td>
</tr>
</tbody>
</table>
<p>让我们加载 <a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-base">Whisper Base</a> 检查点，它的大小与我们之前使用的 Wav2Vec2 检查点相当。 考虑到我们接下来会做多语言语音识别，我们将加载基础检查点的多语言变体。我们还将模型加载到 GPU（如果有）或 CPU 上，如有必要的话 <code>pipeline()</code> 随后会负责将所有输入/输出从 CPU 移动到 GPU：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">device = <span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">pipe = pipeline(</span><br><span class="line">    <span class="string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="string">&quot;openai/whisper-base&quot;</span>, device=device</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>好！现在让我们像之前一样转写音频。我们唯一改变的是传递一个额外的参数 <code>max_new_tokens</code>，它告诉模型在进行预测时生成的最大词元数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe(sample[<span class="string">&quot;audio&quot;</span>], max_new_tokens=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27; He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>轻松搞定！您首先注意到的是大小写和标点符号，这显然比 Wav2Vec2 不区分大小写也不含标点的转写更易读。让我们对比一下转写与目标输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Target:     HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND</span><br><span class="line">Prediction: He tells us that at this festive season of the year, <span class="keyword">with</span> **Christmas** <span class="keyword">and</span> **roast** beef looming before us, **similarly** <span class="keyword">is</span> drawn <span class="keyword">from</span> eating <span class="keyword">and</span> its results occur most readily to the mind.</span><br></pre></td></tr></table></figure>
<p>Whisper 很好地纠正了我们在 Wav2Vec2 那里看到的语音错误—— <em>Christmas</em> 和 <em>roast</em> 都拼写正确。我们看到模型在转写 <em>SIMILES</em> 时出了错， 误写为 <em>similarly</em>，但这次预测是英语中的有效单词。使用更大的 Whisper 检查点可以帮助进一步减少转写错误，但代价是需要更多的计算和更长的转写时间。</p>
<p>该模型应当有能力处理 96 种语言，所以现在让我们暂时离开英语语音识别，走向全球 🌎！ <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/facebook/multilingual_librispeech">Multilingual LibriSpeech</a> (MLS) 数据集是 LibriSpeech 数据集的多语言版本， 包含六种语言的带标注音频数据。我们将从 MLS 数据集的西班牙语子集中加载一个样本，使用 <em>流式</em>（streaming）模式，这样我们就不必下载整个数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dataset = load_dataset(</span><br><span class="line">    <span class="string">&quot;facebook/multilingual_librispeech&quot;</span>, <span class="string">&quot;spanish&quot;</span>, split=<span class="string">&quot;validation&quot;</span>, streaming=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">sample = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataset))</span><br></pre></td></tr></table></figure>
<p>我们再次检查文本转写并试听音频片段：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sample[<span class="string">&quot;text&quot;</span>])</span><br><span class="line">Audio(sample[<span class="string">&quot;audio&quot;</span>][<span class="string">&quot;array&quot;</span>], rate=sample[<span class="string">&quot;audio&quot;</span>][<span class="string">&quot;sampling_rate&quot;</span>])</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">entonces te delelitarás en jehová y yo te haré subir sobre las alturas de la tierra y te daré á comer la heredad de jacob tu padre porque la boca de jehová lo ha hablado</span><br></pre></td></tr></table></figure>
<p>这是我们希望通过 Whisper 转写出的目标文本，虽然我们现在知道我们预测的结果可能会比这更好，因为我们的模型还会预测标点和大小写，而参考中没有这些。 让我们将音频样本输入 pipeline 以获得文本预测。需要注意的是，pipeline 会 <em>消耗</em> 我们输入的音频输入字典，即字典不能被重复读取。 为了解决这个问题，我们将传递一个音频样本的 <em>副本</em>，这样我们就可以在后续代码示例中重复使用同一个音频样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe(sample[<span class="string">&quot;audio&quot;</span>].copy(), max_new_tokens=<span class="number">256</span>, generate_kwargs=&#123;<span class="string">&quot;task&quot;</span>: <span class="string">&quot;transcribe&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27; Entonces te deleitarás en Jehová y yo te haré subir sobre las alturas de la tierra y te daré a comer la heredad de Jacob tu padre porque la boca de Jehová lo ha hablado.&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>太好了——这看起来与我们的参考文本非常相似（可以说更好，因为它有标点和大小写！）。您会注意到，我们以 <em>生成关键字参数</em>（generate kwarg）的形式传递了 <code>&quot;task&quot;</code>。 将 <code>&quot;task&quot;</code> 设置为 <code>&quot;transcribe&quot;</code> 会迫使 Whisper 执行 <em>语音识别</em> 任务，即转写的语言就是音频中使用的语音。 Whisper 还能够执行与之密切相关的 <em>语音翻译</em> 任务，西班牙语音频可以被翻译成英语文本。要实现这一点，我们将 <code>&quot;task&quot;</code> 设置为 <code>&quot;translate&quot;</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe(sample[<span class="string">&quot;audio&quot;</span>], max_new_tokens=<span class="number">256</span>, generate_kwargs=&#123;<span class="string">&quot;task&quot;</span>: <span class="string">&quot;translate&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27; So you will choose in Jehovah and I will raise you on the heights of the earth and I will give you the honor of Jacob to your father because the voice of Jehovah has spoken to you.&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>现在我们知道可以在语音识别和语音翻译之间切换，我们可以根据需要选择任务。 要么我们从 X 语言的音频识别到同一语言 X 的文本（例如，西班牙语音频到西班牙语文本），要么我们从任何语言 X 的音频翻译到英文文本（例如，西班牙语音频到英文文本）。</p>
<p>要了解更多关于如何使用 <code>&quot;task&quot;</code> 参数来控制生成文本的属性，请参阅 Whisper base 的 <a target="_blank" rel="noopener" href="https://huggingface.co/openai/whisper-base#usage">模型卡片</a>。</p>
<h3 id="长篇转写和时间戳"><a class="markdownIt-Anchor" href="#长篇转写和时间戳"></a> 长篇转写和时间戳</h3>
<p>到目前为止，我们一直专注于转写不到 30 秒的短音频样本。但我们也提到过 Whisper 的一个优势是可以处理长音频样本，我们将在这里处理这个任务！</p>
<p>让我们通过拼接 MLS 数据集中连续几条样本来创建一个长音频文件。因为 MLS 数据集是通过将长有声读物录音分割成较短片段来整理的， 所以拼接样本是重构较长有声读物段落的一种方法，拼接出的音频在各个样本间应该是连贯的。</p>
<p>我们将目标音频长度设为 5 分钟，并在达到这个值时停止拼接样本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">target_length_in_m = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将分钟数转为秒数（* 60），再转为采用点数（* sampling rate）</span></span><br><span class="line">sampling_rate = pipe.feature_extractor.sampling_rate</span><br><span class="line">target_length_in_samples = target_length_in_m * <span class="number">60</span> * sampling_rate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历数据集，拼接样本直到长度达到目标</span></span><br><span class="line">long_audio = []</span><br><span class="line"><span class="keyword">for</span> sample <span class="keyword">in</span> dataset:</span><br><span class="line">    long_audio.extend(sample[<span class="string">&quot;audio&quot;</span>][<span class="string">&quot;array&quot;</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(long_audio) &gt; target_length_in_samples:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">long_audio = np.asarray(long_audio)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果如何？</span></span><br><span class="line">seconds = <span class="built_in">len</span>(long_audio) / <span class="number">16000</span></span><br><span class="line">minutes, seconds = <span class="built_in">divmod</span>(seconds, <span class="number">60</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Length of audio sample is <span class="subst">&#123;minutes&#125;</span> minutes <span class="subst">&#123;seconds:<span class="number">.2</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Length of audio sample <span class="keyword">is</span> <span class="number">5.0</span> minutes <span class="number">17.22</span> seconds</span><br></pre></td></tr></table></figure>
<p>好的！有 5 分钟 17 秒的音频要转写。直接将这个长音频样本输入给模型有两个问题：</p>
<ol>
<li>Whisper 被设计用于处理 30 秒的样本：少于 30 秒的任何内容都会用静音填充至 30 秒，超过 30 秒的任何内容都会通过切掉多余音频缩短至 30 秒，所以如果我们直接输入我们的音频，我们只会得到前 30 秒的转写</li>
<li>Transformer 网络中的内存随序列长度平方增长：输入长度加倍会使内存需求增加四倍，所以传递超长音频文件会导致内存不足（out-of-memory, OOM）错误</li>
</ol>
<p>在 🤗 Transformers 中进行长篇转写的方式是将输入音频 <em>分块</em>（chunking）为更小、更易管理的段落，每个段落与前一个段落有少量重叠。 这样，我们可以在段落边界处准确地将段落拼接在一起，因为我们可以找到段落之间的重叠并相应地合并转写：</p>
<p><img src="Striding.png" alt></p>
<p>给样本分块的优势在于，转写后续的块i+1<em>i</em>+1 不需要使用之前的块i<em>i</em> 的结果。拼接是在我们已经转写了所有块之后在块边界处进行的，所以转写的顺序并不重要。 该算法完全是 <strong>无状态的</strong>，所以我们甚至可以同时转写块i+1<em>i</em>+1 和块i<em>i</em>！这允许我们 <em>批量</em> 处理块，并通过并行运行模型，与顺序转写相比大大提升计算速度。 要了解更多关于 🤗 Transformers 中分块的知识，请参阅这篇 <a target="_blank" rel="noopener" href="https://huggingface.co/blog/asr-chunking">博客</a>。</p>
<p>要使用长篇转写，我们在调用 pipeline 时需要添加一个额外参数。这个参数，<code>chunk_length_s</code>，控制分块段落的长度（以秒为单位）。对于 Whisper，30 秒的块是最佳的，因为这符合 Whisper 期望的输入长度。</p>
<p>要使用批处理，我们需要将 <code>batch_size</code> 参数传递给 pipeline。将所有这些放在一起，我们可以像下面这样转写长音频样本，并进行分块和批处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pipe(</span><br><span class="line">    long_audio,</span><br><span class="line">    max_new_tokens=<span class="number">256</span>,</span><br><span class="line">    generate_kwargs=&#123;<span class="string">&quot;task&quot;</span>: <span class="string">&quot;transcribe&quot;</span>&#125;,</span><br><span class="line">    chunk_length_s=<span class="number">30</span>,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;text&#x27;: &#x27; Entonces te deleitarás en Jehová, y yo te haré subir sobre las alturas de la tierra, y te daré a comer la</span><br><span class="line">heredad de Jacob tu padre, porque la boca de Jehová lo ha hablado. nosotros curados. Todos nosotros nos descarriamos</span><br><span class="line">como bejas, cada cual se apartó por su camino, mas Jehová cargó en él el pecado de todos nosotros...</span><br></pre></td></tr></table></figure>
<p>由于输出相当长（总共 312 个词），我们在这里不会打印整个输出！在 16GB V100 GPU 上，预计上面的代码要运行 3.45 秒，对于 317 秒的音频样本来说相当不错。在 CPU 上，预期大约要跑 30 秒。</p>
<p>Whisper 还能够预测音频数据的片段级 <em>时间戳</em>。这些时间戳指示音频的短片段的开始和结束时间，对于将转写与输入音频对齐特别有用。 比方说我们想为视频提供字幕（closed caption）——我们需要这些时间戳来知道转写的哪一部分对应于视频的某个段落，以便在那个时间显示正确的转写。</p>
<p>启用时间戳预测很简单，我们只需设置参数 <code>return_timestamps=True</code>。时间戳与我们之前使用的分块和批处理方法兼容，所以我们可以直接将时间戳参数附加到我们之前的代码上：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pipe(</span><br><span class="line">    long_audio,</span><br><span class="line">    max_new_tokens=<span class="number">256</span>,</span><br><span class="line">    generate_kwargs=&#123;<span class="string">&quot;task&quot;</span>: <span class="string">&quot;transcribe&quot;</span>&#125;,</span><br><span class="line">    chunk_length_s=<span class="number">30</span>,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">    return_timestamps=<span class="literal">True</span>,</span><br><span class="line">)[<span class="string">&quot;chunks&quot;</span>]</span><br></pre></td></tr></table></figure>
<p><strong>输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;timestamp&#x27;</span>: (<span class="number">0.0</span>, <span class="number">26.4</span>),</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27; Entonces te deleitarás en Jehová, y yo te haré subir sobre las alturas de la tierra, y te daré a comer la heredad de Jacob tu padre, porque la boca de Jehová lo ha hablado. nosotros curados. Todos nosotros nos descarriamos como bejas, cada cual se apartó por su camino,&#x27;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;timestamp&#x27;</span>: (<span class="number">26.4</span>, <span class="number">32.48</span>),</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27; mas Jehová cargó en él el pecado de todos nosotros. No es que partas tu pan con el&#x27;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;timestamp&#x27;</span>: (<span class="number">32.48</span>, <span class="number">38.4</span>),</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27; hambriento y a los hombres herrantes metas en casa, que cuando vieres al desnudo lo cubras y no&#x27;</span>&#125;,</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure>
<p>瞧！我们有了预测的文本以及相应的时间戳。</p>
<h3 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h3>
<p>Whisper 是一个强大的用于语音识别和翻译的预训练模型。与 Wav2Vec2 相比，它具有更高的转写准确性，输出包含标点和大小写。 它可用于转写英语和其他 96 种语言的语音，既能处理短音频片段又能通过 <em>分块</em> 处理的较长片段。这些属性使其可以直接用于许多语音识别和翻译任务，无需微调。 <code>pipeline()</code> 方法提供了一种使用一行 API 调用轻松运行推理的方法，还能对生成的预测进行控制。</p>
<p>尽管 Whisper 模型在许多高资源语言上表现极佳，但它对于低资源语言，即那些缺乏容易获得的训练数据的语言，转写和翻译准确性较低。 在某些语言的不同口音和方言，包括不同性别、种族、年龄或其他人口统计标准的发言者身上的表现也存在差异（参见 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2212.04356.pdf">Whisper 论文</a>）。</p>
<p>为了提高在低资源语言、口音或方言上的性能，我们可以将预训练的 Whisper 模型在一小部分适当选择的数据上继续训练，这个过程称为 <em>微调</em>。 我们将展示如何仅使用十小时的额外数据，就能将 Whisper 模型在低资源语言上的性能提高 100% 以上。在下一节中，我们将介绍选择微调数据集的过程。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">HUI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/08/huggingface_course/Audio_Course(5.1)/">http://example.com/2024/10/08/huggingface_course/Audio_Course(5.1)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">HUI</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/43670b.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/10/exercitation/bagu_NLP(1)/" title="八股文-自然语言处理(一)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">八股文-自然语言处理(一)</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/08/huggingface_course/Audio_Course(4)/" title="Audio课程（四）- 构建音频流派分类器"><img class="cover" src="/img/43670b.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Audio课程（四）- 构建音频流派分类器</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/87788970_p0_master1200.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HUI</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">58</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#audio%E8%AF%BE%E7%A8%8B%E4%BA%94-%E8%87%AA%E5%8A%A8%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%ABasr"><span class="toc-text"> Audio课程（五）- 自动语音识别(ASR)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 语音识别的预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2-ctc-%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 探索 CTC 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%B8%A1%E5%88%B0-seq2seq"><span class="toc-text"> 过渡到 Seq2Seq</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E7%AF%87%E8%BD%AC%E5%86%99%E5%92%8C%E6%97%B6%E9%97%B4%E6%88%B3"><span class="toc-text"> 长篇转写和时间戳</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text"> 总结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/12/10/Multimodel/MM-LLMs-survey/" title="MM-LLMs综述(腾讯)"><img src="/img/image-20241114140927375.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MM-LLMs综述(腾讯)"/></a><div class="content"><a class="title" href="/2024/12/10/Multimodel/MM-LLMs-survey/" title="MM-LLMs综述(腾讯)">MM-LLMs综述(腾讯)</a><time datetime="2024-12-10T12:40:00.000Z" title="发表于 2024-12-10 20:40:00">2024-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/10/Multimodel/BiRD/" title="MICCAI2024(2)-BIRD"><img src="/img/image-20241104164541942.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MICCAI2024(2)-BIRD"/></a><div class="content"><a class="title" href="/2024/12/10/Multimodel/BiRD/" title="MICCAI2024(2)-BIRD">MICCAI2024(2)-BIRD</a><time datetime="2024-12-10T12:36:38.000Z" title="发表于 2024-12-10 20:36:38">2024-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/10/Multimodel/LLaVA-V1.5/" title="LLaVA(2)-Improved Baselines with Visual Instruction Tuning"><img src="/img/image-20241123214150206.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLaVA(2)-Improved Baselines with Visual Instruction Tuning"/></a><div class="content"><a class="title" href="/2024/12/10/Multimodel/LLaVA-V1.5/" title="LLaVA(2)-Improved Baselines with Visual Instruction Tuning">LLaVA(2)-Improved Baselines with Visual Instruction Tuning</a><time datetime="2024-12-10T12:30:38.000Z" title="发表于 2024-12-10 20:30:38">2024-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/10/Multimodel/LLaVA/" title="LLaVA(1)-Visual Instruction Tuning"><img src="/img/image-20241122154508143.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="LLaVA(1)-Visual Instruction Tuning"/></a><div class="content"><a class="title" href="/2024/12/10/Multimodel/LLaVA/" title="LLaVA(1)-Visual Instruction Tuning">LLaVA(1)-Visual Instruction Tuning</a><time datetime="2024-12-10T12:25:38.000Z" title="发表于 2024-12-10 20:25:38">2024-12-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/12/10/Multimodel/PMC-CLIP/" title="MICCAI2024(1)-PMC-CLIP"><img src="/img/image-20241111141331709.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MICCAI2024(1)-PMC-CLIP"/></a><div class="content"><a class="title" href="/2024/12/10/Multimodel/PMC-CLIP/" title="MICCAI2024(1)-PMC-CLIP">MICCAI2024(1)-PMC-CLIP</a><time datetime="2024-12-10T12:12:38.000Z" title="发表于 2024-12-10 20:12:38">2024-12-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/43670b.png')"><div id="footer-wrap"><div class="copyright">&copy;2024 By HUI</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2024/10/08/huggingface_course/Audio_Course(5.1)/'
    this.page.identifier = '/2024/10/08/huggingface_course/Audio_Course(5.1)/'
    this.page.title = 'Audio课程（五）- 自动语音识别(ASR)'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>