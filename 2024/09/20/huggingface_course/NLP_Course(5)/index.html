<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLP课程（五）- Datasets库 | HUI</title><meta name="author" content="HUI"><meta name="copyright" content="HUI"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（五）- Datasets库 在微调模型时有三个主要步骤：  从hugs Face Hub加载一个数据集。 使用Dataset.map()对数据进行预处理。 加载和计算指标(特征)。   数据集不在 Hub    Data fo">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP课程（五）- Datasets库">
<meta property="og:url" content="http://example.com/2024/09/20/huggingface_course/NLP_Course(5)/index.html">
<meta property="og:site_name" content="HUI">
<meta property="og:description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（五）- Datasets库 在微调模型时有三个主要步骤：  从hugs Face Hub加载一个数据集。 使用Dataset.map()对数据进行预处理。 加载和计算指标(特征)。   数据集不在 Hub    Data fo">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/2021060111433875004.jpg">
<meta property="article:published_time" content="2024-09-20T14:33:33.000Z">
<meta property="article:modified_time" content="2024-10-08T14:25:34.918Z">
<meta property="article:author" content="HUI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/2021060111433875004.jpg"><link rel="shortcut icon" href="/img/122061154_p0_master1200.jpg"><link rel="canonical" href="http://example.com/2024/09/20/huggingface_course/NLP_Course(5)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP课程（五）- Datasets库',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-08 22:25:34'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/bronya.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/87788970_p0_master1200.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/2021060111433875004.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="HUI"><img class="site-icon" src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"/><span class="site-name">HUI</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP课程（五）- Datasets库</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-20T14:33:33.000Z" title="发表于 2024-09-20 22:33:33">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-08T14:25:34.918Z" title="更新于 2024-10-08 22:25:34">2024-10-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/">HuggingFace</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">16.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>71分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP课程（五）- Datasets库"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/09/20/huggingface_course/NLP_Course(5)/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/09/20/huggingface_course/NLP_Course(5)/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>转载自：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/">https://huggingface.co/learn/nlp-course/zh-CN/</a></p>
<p><strong>原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。</strong></p>
<h1 id="nlp课程五-datasets库"><a class="markdownIt-Anchor" href="#nlp课程五-datasets库"></a> NLP课程（五）- Datasets库</h1>
<p>在微调模型时有三个主要步骤：</p>
<ol>
<li>从hugs Face Hub加载一个数据集。</li>
<li>使用Dataset.map()对数据进行预处理。</li>
<li>加载和计算指标(特征)。</li>
</ol>
<h2 id="数据集不在-hub"><a class="markdownIt-Anchor" href="#数据集不在-hub"></a> 数据集不在 Hub</h2>
<table>
<thead>
<tr>
<th>Data format</th>
<th>Loading script</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>CSV &amp; TSV</td>
<td><code>csv</code></td>
<td><code>load_dataset(&quot;csv&quot;, data_files=&quot;my_file.csv&quot;)</code></td>
</tr>
<tr>
<td>Text files</td>
<td><code>text</code></td>
<td><code>load_dataset(&quot;text&quot;, data_files=&quot;my_file.txt&quot;)</code></td>
</tr>
<tr>
<td>JSON &amp; JSON Lines</td>
<td><code>json</code></td>
<td><code>load_dataset(&quot;json&quot;, data_files=&quot;my_file.jsonl&quot;)</code></td>
</tr>
<tr>
<td>Pickled DataFrames</td>
<td><code>pandas</code></td>
<td><code>load_dataset(&quot;pandas&quot;, data_files=&quot;my_dataframe.pkl&quot;)</code></td>
</tr>
</tbody>
</table>
<p>对于每种数据格式, 我们只需要使用 <code>load_dataset()</code> 函数, 使用 <code>data_files</code> 指定一个或多个文件的路径的参数</p>
<h3 id="1加载本地数据集"><a class="markdownIt-Anchor" href="#1加载本地数据集"></a> 1.加载本地数据集</h3>
<p>以<a target="_blank" rel="noopener" href="https://github.com/crux82/squad-it/">SQuAD_it dataset</a>为例，下载并且解压后可以得到SQuAD_it-train.json和SQuAD_it-test.json</p>
<p>使用<code>load_dataset()</code>函数来加载JSON文件, <font color="red">我们只需要知道我们是在处理普通的 JSON(类似于嵌套字典)还是 JSON 行(行分隔的 JSON)。像许多问答数据集一样, SQuAD-it 使用嵌套格式,所有文本都存储在 <code>data</code>文件中。这意味着我们可以通过指定参数<code>field</code>来加载数据集,如下所示:</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">squad_it_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=<span class="string">&quot;SQuAD_it-train.json&quot;</span>, field=<span class="string">&quot;data&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>默认情况下, 加载本地文件会创建一个带有<code>train</code>的<code>DatasetDict</code> 对象。 我们可以通过 <code>squad_it_dataset</code>查看:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">squad_it_dataset</span><br><span class="line"></span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;paragraphs&#x27;</span>], <span class="comment"># 列名</span></span><br><span class="line">        num_rows: <span class="number">442</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>这向我们显示了与训练集相关联的行数和列名。我们可以通过索引到 <code>train</code> 查看示例，如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">squad_it_dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>] <span class="comment"># 训练集的2第一行</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;title&quot;</span>: <span class="string">&quot;Terremoto del Sichuan del 2008&quot;</span>,</span><br><span class="line">    <span class="string">&quot;paragraphs&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;context&quot;</span>: <span class="string">&quot;Il terremoto del Sichuan del 2008 o il terremoto...&quot;</span>,</span><br><span class="line">            <span class="string">&quot;qas&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;answers&quot;</span>: [&#123;<span class="string">&quot;answer_start&quot;</span>: <span class="number">29</span>, <span class="string">&quot;text&quot;</span>: <span class="string">&quot;2008&quot;</span>&#125;],</span><br><span class="line">                    <span class="string">&quot;id&quot;</span>: <span class="string">&quot;56cdca7862d2951400fa6826&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;question&quot;</span>: <span class="string">&quot;In quale anno si è verificato il terremoto nel Sichuan?&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                ...</span><br><span class="line">            ],</span><br><span class="line">        &#125;,</span><br><span class="line">        ...</span><br><span class="line">    ],</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><font size="5">同时加载训练集与测试集</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data_files = &#123;<span class="string">&quot;train&quot;</span>: <span class="string">&quot;SQuAD_it-train.json&quot;</span>, <span class="string">&quot;test&quot;</span>: <span class="string">&quot;SQuAD_it-test.json&quot;</span>&#125;</span><br><span class="line">squad_it_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=data_files, field=<span class="string">&quot;data&quot;</span>)</span><br><span class="line">squad_it_dataset</span><br><span class="line"></span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;paragraphs&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">442</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;paragraphs&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">48</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><font color="red"><code>load_dataset()</code>函数的<code>data_files</code>参数非常灵活并且可以是单个文件路径、文件路径列表或将分割后的名称映射到文件路径的字典。您还可以根据Unix shell使用的规则对与指定模式匹配的文件进行全局定位（例如，您可以通过设置’data_files=“*.JSON”‘将目录中的所有JSON文件作为单个拆分进行全局定位）。有关更多详细信息，请参阅🤗Datasets 文档。</font></p>
<p>🤗 Datasets实际上支持输入文件的自动解压,所以我们可以跳过使用<code>gzip</code>,直接设置 <code>data_files</code>参数传递压缩文件:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_files = &#123;<span class="string">&quot;train&quot;</span>: <span class="string">&quot;SQuAD_it-train.json.gz&quot;</span>, <span class="string">&quot;test&quot;</span>: <span class="string">&quot;SQuAD_it-test.json.gz&quot;</span>&#125;</span><br><span class="line">squad_it_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=data_files, field=<span class="string">&quot;data&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2加载远程数据集"><a class="markdownIt-Anchor" href="#2加载远程数据集"></a> 2.加载远程数据集</h3>
<p>如果你在公司担任数据研究员或编码员,那么你要分析的数据集很有可能存储在某个远程服务器上。幸运的是,加载远程文件就像加载本地文件一样简单!我们没有提供本地文件的路径, 而是将<code>load_dataset()</code>的<code>data_files</code>参数指向存储远程文件的一个或多个URL。例如, 对于托管在 GitHub 上的 SQuAD-it 数据集, 我们可以将 <code>data_files</code> 指向 <em>SQuAD_it-*.json.gz</em> 的网址,如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;https://github.com/crux82/squad-it/raw/master/&quot;</span></span><br><span class="line">data_files = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: url + <span class="string">&quot;SQuAD_it-train.json.gz&quot;</span>,</span><br><span class="line">    <span class="string">&quot;test&quot;</span>: url + <span class="string">&quot;SQuAD_it-test.json.gz&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">squad_it_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=data_files, field=<span class="string">&quot;data&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="切片数据清洗"><a class="markdownIt-Anchor" href="#切片数据清洗"></a> 切片（数据清洗）</h2>
<h3 id="1切片与切分数据"><a class="markdownIt-Anchor" href="#1切片与切分数据"></a> 1.切片与切分数据</h3>
<p>以<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/index.php">加州大学欧文分校机器学习存储库</a>的<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+(Drugs.com)">药物审查数据集</a>为例，其中包含患者对各种药物的评论，以及正在治疗的病情和患者满意度的 10 星评级。</p>
<p>由于 TSV 只是使用制表符而不是逗号作为分隔符的 CSV 变体，我们可以使用加载<strong>csv</strong>文件的**load_dataset()**函数并指定分隔符 示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">data_files = &#123;<span class="string">&quot;train&quot;</span>: <span class="string">&quot;drugsComTrain_raw.tsv&quot;</span>, <span class="string">&quot;test&quot;</span>: <span class="string">&quot;drugsComTest_raw.tsv&quot;</span>&#125;</span><br><span class="line"><span class="comment"># \t is the tab character in Python</span></span><br><span class="line">drug_dataset = load_dataset(<span class="string">&quot;csv&quot;</span>, data_files=data_files, delimiter=<span class="string">&quot;\t&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font color="red">在进行任何类型的数据分析时，一个好的做法是抽取一个小的随机样本，以快速了解您正在处理的数据类型。</font>在🤗数据集中，我们可以通过链接 <strong>Dataset.shuffle()</strong> 和 <strong>Dataset.select()</strong> 共同来完成抽取：</p>
<p>（<strong>Dataset.shuffle()</strong> 和 **Dataset.select()**见<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/v2.21.0/process%EF%BC%89">https://huggingface.co/docs/datasets/v2.21.0/process）</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">drug_sample = drug_dataset[<span class="string">&quot;train&quot;</span>].shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">1000</span>))</span><br><span class="line"><span class="comment"># Peek at the first few examples</span></span><br><span class="line">drug_sample[:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;Unnamed: 0&#x27;</span>: [<span class="number">87571</span>, <span class="number">178045</span>, <span class="number">80482</span>],</span><br><span class="line"> <span class="string">&#x27;drugName&#x27;</span>: [<span class="string">&#x27;Naproxen&#x27;</span>, <span class="string">&#x27;Duloxetine&#x27;</span>, <span class="string">&#x27;Mobic&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;condition&#x27;</span>: [<span class="string">&#x27;Gout, Acute&#x27;</span>, <span class="string">&#x27;ibromyalgia&#x27;</span>, <span class="string">&#x27;Inflammatory Conditions&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;review&#x27;</span>: [<span class="string">&#x27;&quot;like the previous person mention, I&amp;#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!&quot;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;&quot;I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.&quot;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;&quot;I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.&quot;&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;rating&#x27;</span>: [<span class="number">9.0</span>, <span class="number">3.0</span>, <span class="number">10.0</span>],</span><br><span class="line"> <span class="string">&#x27;date&#x27;</span>: [<span class="string">&#x27;September 2, 2015&#x27;</span>, <span class="string">&#x27;November 7, 2011&#x27;</span>, <span class="string">&#x27;June 5, 2013&#x27;</span>],</span><br><span class="line"> <span class="string">&#x27;usefulCount&#x27;</span>: [<span class="number">36</span>, <span class="number">13</span>, <span class="number">128</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>在**Dataset.shuffle()**选取了固定的随机数种子。 <strong>Dataset.select()</strong> 需要一个可迭代的索引，所以我们已经通过了 <strong>range(1000)</strong> 从随机打乱的数据集中选取前 1,000 个示例。从抽取的数据中，我们已经可以看到我们数据集的一些特点：</p>
<ul>
<li><strong>Unnamed: 0</strong>这列看起来很像每个患者的匿名 ID。</li>
<li><strong>condition</strong> 这列包含有描述健康状况的标签。</li>
<li>评论长短不一，混合有 Python 行分隔符 (<strong>\r\n</strong>) 以及 HTML 字符代码，如**'**。</li>
</ul>
<p><font size="5">Dataset.unique() </font></p>
<p>为了验证<strong>Unnamed: 0</strong> 列存储的是患者 ID的猜想，我们可以使用 <strong>Dataset.unique()</strong> 函数来验证匿名ID 的数量是否与拆分后每部分中的行数匹配：</p>
<p><font color="red">Dataset.unique()用于去重</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> split <span class="keyword">in</span> drug_dataset.keys():</span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(drug_dataset[split]) == <span class="built_in">len</span>(drug_dataset[split].unique(<span class="string">&quot;Unnamed: 0&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>这似乎证实了我们的假设，所以让我们把 <strong>Unnamed: 0</strong> 列重命名为患者的id。我们可以使用 **DatasetDict.rename_column()**函数一次性重命名DatasetDict中共有的列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.rename_column(</span><br><span class="line">    original_column_name=<span class="string">&quot;Unnamed: 0&quot;</span>, new_column_name=<span class="string">&quot;patient_id&quot;</span></span><br><span class="line">)</span><br><span class="line">drug_dataset</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">161297</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">53766</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>使用 <strong>Dataset.map()<strong>标准化所有 <strong>condition</strong> 标签 .正如我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第三章</a>中所做的那样，我们可以定义一个简单的函数，可以将该函数应用于</strong>drug_dataset</strong> 拆分后每部分的所有行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lowercase_condition</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;condition&quot;</span>: example[<span class="string">&quot;condition&quot;</span>].lower()&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">drug_dataset.<span class="built_in">map</span>(lowercase_condition)</span><br><span class="line"></span><br><span class="line">AttributeError: <span class="string">&#x27;NoneType&#x27;</span> <span class="built_in">object</span> has no attribute <span class="string">&#x27;lower&#x27;</span></span><br></pre></td></tr></table></figure>
<p>哦不，我们的map功能遇到了问题！从错误中我们可以推断出 <strong>condition</strong> 列存在 <strong>None</strong> , 不能转换为小写，因为它们不是字符串。让我们使用 <strong>Dataset.filter()</strong> 删除这些行 ，其工作方式类似于 <strong>Dataset.map()</strong> 。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">filter_nones</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x[<span class="string">&quot;condition&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>然后运行 <strong>drug_dataset.filter(filter_nones)</strong></p>
<p><font size="5">lambda 函数：</font></p>
<blockquote>
<p>我们可以在一行中使用lambda 函数.在 Python 中，lambda 函数是您无需明确命名即可使用的微函数（匿名函数）。它们一般采用如下形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lambda</span> &lt;arguments&gt; : &lt;expression&gt;</span><br></pre></td></tr></table></figure>
<p>其中<strong>lambda</strong> 是 Python 的特殊<a target="_blank" rel="noopener" href="https://docs.python.org/3/reference/lexical_analysis.html#keywords">关键字</a>, <strong>arguments</strong> 是以逗号进行分隔的函数输入的列表/集合， <strong>expression</strong> 代表您希望执行的操作。例如，我们可以定义一个简单的 lambda 函数来对一个数字进行平方，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lambda</span> x : x * x</span><br></pre></td></tr></table></figure>
<p>我们需要将要输入给这个函数值括在括号中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">lambda</span> x: x * x)(<span class="number">3</span>)</span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>类似地，我们可以通过用逗号分隔多个参数来定义 lambda 函数。例如，我们可以按如下方式计算三角形的面积：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">lambda</span> base, height: <span class="number">0.5</span> * base * height)(<span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line"><span class="number">16.0</span></span><br></pre></td></tr></table></figure>
<p>当您想定义小型、一次性使用的函数时，Lambda 函数非常方便</p>
</blockquote>
<p><font size="5">简单的映射和过滤操作</font></p>
<p>在🤗 Datasets 中，我们可以使用 lambda 函数来定义简单的映射和过滤操作，所以让我们使用这个技巧来消除我们数据集中的 <strong>None</strong> 条目：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x[<span class="string">&quot;condition&quot;</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>当 <strong>None</strong> 条目已删除，我们可以标准化我们的 <strong>condition</strong> 列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">map</span>(lowercase_condition)</span><br><span class="line"><span class="comment"># Check that lowercasing worked</span></span><br><span class="line">drug_dataset[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;condition&quot;</span>][:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;left ventricular dysfunction&#x27;</span>, <span class="string">&#x27;adhd&#x27;</span>, <span class="string">&#x27;birth control&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="2创建新的数据列"><a class="markdownIt-Anchor" href="#2创建新的数据列"></a> 2.创建新的数据列</h3>
<p>每当您处理客户评论时，一个好的做法是检查每个评论中的字数。评论可能只是一个词，比如“太棒了！”或包含数千字的完整文章，根据实际的情况，您需要以不同的方式处理这些极端情况。为了计算每条评论中的单词数，我们将使用基于空格分割每个文本的粗略方法。</p>
<blockquote>
<ul>
<li><strong>按空格分割’review’这一列的句子，计算’review’中包含的词数并将其作为新的列。</strong></li>
<li><strong>删除包含少于 30 个单词的评论</strong></li>
<li><strong>评论中可能存在 HTML 字符代码使用 Python 的html模块取消这些字符的转义</strong></li>
</ul>
</blockquote>
<p><strong>按空格分割’review’这一列的句子，计算’review’中包含的词数并将其作为新的列。</strong></p>
<blockquote>
<p>让我们定义一个简单的函数来计算每条评论中的单词数：</p>
<p><font color="red">Dataset.split()默认按空格进行分割，得到一个字符串列表</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_review_length</span>(<span class="params">example</span>):</span><br><span class="line"> <span class="keyword">return</span> &#123;<span class="string">&quot;review_length&quot;</span>: <span class="built_in">len</span>(example[<span class="string">&quot;review&quot;</span>].split())&#125;</span><br></pre></td></tr></table></figure>
<p><strong>与我们的 <code>lowercase_condition()</code> 函数不同，<code>compute_review_length()</code> 返回一个字典，其键与数据集中的列名之一不对应。 在这种情况下，当 <code>compute_review_length()</code> 传递给 <code>Dataset.map()</code> 时，它将应用于数据集中的所有行以创建新的 <code>review_length</code> 列：</strong></p>
<p><font color="red">(Dataset.map()默认按行进行操作，且产生的新的列添加到表格列的末尾)</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">map</span>(compute_review_length)</span><br><span class="line"><span class="comment"># Inspect the first training example</span></span><br><span class="line">drug_dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>] <span class="comment"># 训练集第一行</span></span><br><span class="line">&#123;<span class="string">&#x27;patient_id&#x27;</span>: <span class="number">206461</span>,</span><br><span class="line"><span class="string">&#x27;drugName&#x27;</span>: <span class="string">&#x27;Valsartan&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;condition&#x27;</span>: <span class="string">&#x27;left ventricular dysfunction&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;review&#x27;</span>: <span class="string">&#x27;&quot;It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil&quot;&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;rating&#x27;</span>: <span class="number">9.0</span>,</span><br><span class="line"><span class="string">&#x27;date&#x27;</span>: <span class="string">&#x27;May 20, 2012&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;usefulCount&#x27;</span>: <span class="number">27</span>,</span><br><span class="line"><span class="string">&#x27;review_length&#x27;</span>: <span class="number">17</span>&#125;</span><br></pre></td></tr></table></figure>
<p>以使用 **Dataset.sort()**对这个新列进行排序，然后查看极端长度的评论的样子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset[<span class="string">&quot;train&quot;</span>].sort(<span class="string">&quot;review_length&quot;</span>)[:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;patient_id&#x27;</span>: [<span class="number">103488</span>, <span class="number">23627</span>, <span class="number">20558</span>],</span><br><span class="line"><span class="string">&#x27;drugName&#x27;</span>: [<span class="string">&#x27;Loestrin 21 1 / 20&#x27;</span>, <span class="string">&#x27;Chlorzoxazone&#x27;</span>, <span class="string">&#x27;Nucynta&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;condition&#x27;</span>: [<span class="string">&#x27;birth control&#x27;</span>, <span class="string">&#x27;muscle spasm&#x27;</span>, <span class="string">&#x27;pain&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;review&#x27;</span>: [<span class="string">&#x27;&quot;Excellent.&quot;&#x27;</span>, <span class="string">&#x27;&quot;useless&quot;&#x27;</span>, <span class="string">&#x27;&quot;ok&quot;&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;rating&#x27;</span>: [<span class="number">10.0</span>, <span class="number">1.0</span>, <span class="number">6.0</span>],</span><br><span class="line"><span class="string">&#x27;date&#x27;</span>: [<span class="string">&#x27;November 4, 2008&#x27;</span>, <span class="string">&#x27;March 24, 2017&#x27;</span>, <span class="string">&#x27;August 20, 2016&#x27;</span>],</span><br><span class="line"><span class="string">&#x27;usefulCount&#x27;</span>: [<span class="number">5</span>, <span class="number">2</span>, <span class="number">10</span>],</span><br><span class="line"><span class="string">&#x27;review_length&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>
<p>正如我们所猜想的那样，一些评论只包含一个词，虽然这对于情感分析来说可能没问题，但如果我们想要预测病情，这些评论可能并不适合。</p>
</blockquote>
<p><strong>删除包含少于 30 个单词的评论</strong></p>
<blockquote>
<p>让我们使用 <strong>Dataset.filter()</strong> 功能来删除包含少于 30 个单词的评论。与我们对 <strong>condition</strong> 列的处理相似，我们可以通过选取评论的长度高于此阈值来过滤掉非常短的评论：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x[<span class="string">&quot;review_length&quot;</span>] &gt; <span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(drug_dataset.num_rows)</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;train&#x27;</span>: <span class="number">138514</span>, <span class="string">&#x27;test&#x27;</span>: <span class="number">46108</span>&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>评论中可能存在 HTML 字符代码使用 Python 的html模块取消这些字符的转义</strong></p>
<blockquote>
<p>如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> html</span><br><span class="line"></span><br><span class="line">text = <span class="string">&quot;I&amp;#039;m a transformer called BERT&quot;</span></span><br><span class="line">html.unescape(text)</span><br><span class="line"><span class="string">&quot;I&#x27;m a transformer called BERT&quot;</span></span><br></pre></td></tr></table></figure>
<p>我们将使用 <strong>Dataset.map()</strong> 对我们语料库中的所有 HTML 字符进行转义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset = drug_dataset.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: &#123;<span class="string">&quot;review&quot;</span>: html.unescape(x[<span class="string">&quot;review&quot;</span>])&#125;)</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="3加速map方法"><a class="markdownIt-Anchor" href="#3加速map方法"></a> 3.加速map方法</h3>
<p><font size="5">Dataset.map()函数时指定 batched=True。</font></p>
<blockquote>
<p><strong>Dataset.map()</strong> 方法有一个 <strong>batched</strong> 参数，如果设置为 <strong>True</strong> , map 函数将会分批执行所需要进行的操作（批量大小是可配置的，但默认为 1,000）。可以通过使用列表推导同时处理多个元素来加快速度。</p>
<p>当您在使用 **Dataset.map()**函数时指定 <strong>batched=True</strong>。<font color="red">该函数会接收一个包含数据集字段的字典，每个值都是一个列表，而不仅仅是单个值。<strong>Dataset.map()</strong> 的返回值应该是相同的：一个包含我们想要更新或添加到数据集中的字段的字典，字典的键是要添加的字段，字典的值是结果的列表。</font></p>
<p>例如，这是使用 <strong>batched=True</strong>对所有 HTML 字符进行转义的方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new_drug_dataset = drug_dataset.<span class="built_in">map</span>(</span><br><span class="line"> <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;review&quot;</span>: [html.unescape(o) <span class="keyword">for</span> o <span class="keyword">in</span> x[<span class="string">&quot;review&quot;</span>]]&#125;, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>如果您运行此代码，您会看到此命令的执行速度比前一个命令快得多。这是因为列表推导式通常比在同一代码中用 <strong>for</strong> 循环执行相同的代码更快，并且我们还通过同时访问多个元素而不是一个一个来处理来提高处理的速度。</p>
</blockquote>
<p><font size="5">快速分词器：</font></p>
<blockquote>
<p>例如，要使用快速标记器标记所有药物评论（<strong>AutoTokenizer</strong> 的默认设置是<strong>use_fast=True</strong>，也就是快速标记，可以设置use_fast=True来对比速度。他们能够实现这样的加速，因为在底层的标记化代码是在 Rust 中执行的，Rust 是一种可以轻松并行化执行的语言），我们可以使用这样的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line"> <span class="keyword">return</span> tokenizer(examples[<span class="string">&quot;review&quot;</span>], truncation=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>正如你在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第三章</a>所看到的，我们原本就可以将一个或多个示例传递给分词器，因此在<strong>batched=True</strong>是一个非必须的选项。在笔记本中，您可以在您要测量的代码行之前添加 <strong>%time</strong>来测试改行运行所消耗的时间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%time tokenized_dataset = drug_dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<p><font size="5">多线程处理</font></p>
<blockquote>
<p><strong>Dataset.map()</strong> 也有一些自己的并行化能力。由于它们不受 Rust 的支持，因此慢速分词器的速度赶不上快速分词器，但它们仍然会更快一些（尤其是当您使用没有快速版本的分词器时）。要启用多处理，请在**Dataset.map()**时使用 <strong>num_proc</strong> 参数并指定要在调用中使用的进程数 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">slow_tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>, use_fast=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slow_tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line"> <span class="keyword">return</span> slow_tokenizer(examples[<span class="string">&quot;review&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_dataset = drug_dataset.<span class="built_in">map</span>(slow_tokenize_function, batched=<span class="literal">True</span>, num_proc=<span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<p>PS：除了 <strong>num_proc=8</strong>，我们的测试表明，使用<strong>batched=True</strong>而不带有<strong>num_proc</strong>参数的选项处理起来更快。通常，我们不建议将 Python 多线程处理用于具有<strong>batched=True</strong>功能的快速标记器 .</p>
</blockquote>
<p><font size="5">从单个示例的一列中提取多个特征</font></p>
<p><strong>一个例子通常可以为我们的模型提供一组特征。在某些情况下，这些特征会储存在数据集的几个列，但在其他情况下（例如此处的例子和用于问答的数据），可以从单个示例的一列中提取多个特征</strong></p>
<blockquote>
<p>让我们来看看它是如何工作的！在这里，我们将标记化我们的示例并将最大截断长度设置128，但我们将要求标记器返回全部文本块，而不仅仅是第一个。这可以用 <strong>return_overflowing_tokens=True</strong> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_and_split</span>(<span class="params">examples</span>):</span><br><span class="line"> <span class="keyword">return</span> tokenizer(</span><br><span class="line">     examples[<span class="string">&quot;review&quot;</span>],</span><br><span class="line">     truncation=<span class="literal">True</span>,</span><br><span class="line">     max_length=<span class="number">128</span>,</span><br><span class="line">     return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line"> )</span><br></pre></td></tr></table></figure>
<p>在使用<strong>Dataset.map()</strong> 正式在整个数据集上开始处理之前让我们先在一个例子上测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">result = tokenize_and_split(drug_dataset[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>])</span><br><span class="line">[<span class="built_in">len</span>(inp) <span class="keyword">for</span> inp <span class="keyword">in</span> result[<span class="string">&quot;input_ids&quot;</span>]]</span><br><span class="line"></span><br><span class="line">[<span class="number">128</span>, <span class="number">49</span>]</span><br></pre></td></tr></table></figure>
<p><font color="red">我们在训练集中的第一个示例变成了两个特征，因为它被标记为超过我们指定的最大截断长度，因此结果被截成了两段：第一段长度为 128 ，第二段长度为 49 。</font>现在让我们对所有元素执行此操作数据集！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenized_dataset = drug_dataset.<span class="built_in">map</span>(tokenize_and_split, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ArrowInvalid: Column <span class="number">1</span> named condition expected length <span class="number">1463</span> but got length <span class="number">1000</span></span><br></pre></td></tr></table></figure>
<p>不好了！它没有起作用！为什么呢？查看错误消息会给我们一个线索：列的长度不匹配，一列长度为 1,463，另一列长度为 1,000。1,000行的”review”给出了 1,463 行的新特征，导致和原本的1000行数据不匹配。</p>
<p><font color="red">问题出在我们试图混合两个不同大小的不同数据集： <strong>drug_dataset</strong> 列将有一定数量的元素（我们错误中的 1,000），但是我们正在构建<strong>tokenized_dataset</strong> 将有更多的元素（错误消息中的 1,463）。这不适用于 <strong>Dataset</strong> ，因此我们需要从旧数据集中删除列或使它们的大小与新数据集中的大小相同。</font>我们可以用 <strong>remove_columns</strong> 参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenized_dataset = drug_dataset.<span class="built_in">map</span>(</span><br><span class="line"> tokenize_and_split, batched=<span class="literal">True</span>, remove_columns=drug_dataset[<span class="string">&quot;train&quot;</span>].column_names</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>现在这个过程没有错误。我们可以通过比较长度来检查新数据集的元素是否比原始数据集多得多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(tokenized_dataset[<span class="string">&quot;train&quot;</span>]), <span class="built_in">len</span>(drug_dataset[<span class="string">&quot;train&quot;</span>])</span><br><span class="line">(<span class="number">206772</span>, <span class="number">138514</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<p><font size="5">长度不匹配的问题</font></p>
<blockquote>
<p>可以通过使旧列与新列的大小相同来处理长度不匹配的问题。</p>
<p>为此，我们可以使用 <strong>overflow_to_sample_mapping</strong> 字段，当我们设置<strong>return_overflowing_tokens=True</strong> .它为我们提供了特征到它所产生的样本的映射。使用这个，我们可以将原始数据集中的每个键关联到一个合适大小的值列表中，通过遍历所有的数据来生成新特性:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_and_split</span>(<span class="params">examples</span>):</span><br><span class="line"> result = tokenizer(</span><br><span class="line">     examples[<span class="string">&quot;review&quot;</span>],</span><br><span class="line">     truncation=<span class="literal">True</span>,</span><br><span class="line">     max_length=<span class="number">128</span>,</span><br><span class="line">     return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line"> )</span><br><span class="line"> <span class="comment"># Extract mapping between new and old indices</span></span><br><span class="line"> sample_map = result.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line"> <span class="keyword">for</span> key, values <span class="keyword">in</span> examples.items():</span><br><span class="line">     result[key] = [values[i] <span class="keyword">for</span> i <span class="keyword">in</span> sample_map]</span><br><span class="line"> <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p>我们可以使用**Dataset.map()**来进行批处理，这样无需我们删除旧列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tokenized_dataset = drug_dataset.<span class="built_in">map</span>(tokenize_and_split, batched=<span class="literal">True</span>)</span><br><span class="line">tokenized_dataset</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line"> train: Dataset(&#123;</span><br><span class="line">     features: [<span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>],</span><br><span class="line">     num_rows: <span class="number">206772</span></span><br><span class="line"> &#125;)</span><br><span class="line"> test: Dataset(&#123;</span><br><span class="line">     features: [<span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>],</span><br><span class="line">     num_rows: <span class="number">68876</span></span><br><span class="line"> &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>我们获得了与以前相同数量的训练特征，但在这里我们保留了所有旧字段。<font color="red">如果您在使用模型计算之后需要它们进行一些后处理，您可能需要使用这种方法。</font></p>
</blockquote>
<h3 id="4datasets和dataframes的相互转换"><a class="markdownIt-Anchor" href="#4datasets和dataframes的相互转换"></a> 4.Datasets和DataFrames的相互转换</h3>
<p><font size="5">Dataset.set_format() </font></p>
<p>为了实现各种第三方库之间的转换，🤗 Datasets 提供了一个 <strong>Dataset.set_format()</strong> 功能。此功能可以通过仅更改输出格式的，轻松切换到另一种格式，而不会影响底层数据格式，即 Apache Arrow。格式化会在数据本身上进行。为了演示，让我们将数据集转换为 Pandas：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset.set_format(<span class="string">&quot;pandas&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>现在，当我们访问数据集的元素时，我们会得到一个 <strong>pandas.DataFrame</strong> 而不是字典：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset[<span class="string">&quot;train&quot;</span>][:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th></th>
<th>patient_id</th>
<th>drugName</th>
<th>condition</th>
<th>review</th>
<th>rating</th>
<th>date</th>
<th>usefulCount</th>
<th>review_length</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>95260</td>
<td>Guanfacine</td>
<td>adhd</td>
<td>“My son is halfway through his fourth week of Intuniv…”</td>
<td>8.0</td>
<td>April 27, 2010</td>
<td>192</td>
<td>141</td>
</tr>
<tr>
<td>1</td>
<td>92703</td>
<td>Lybrel</td>
<td>birth control</td>
<td>“I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects…”</td>
<td>5.0</td>
<td>December 14, 2009</td>
<td>17</td>
<td>134</td>
</tr>
<tr>
<td>2</td>
<td>138000</td>
<td>Ortho Evra</td>
<td>birth control</td>
<td>“This is my first time using any form of birth control…”</td>
<td>8.0</td>
<td>November 3, 2015</td>
<td>10</td>
<td>89</td>
</tr>
</tbody>
</table>
<p>让我们创建一个 <strong>pandas.DataFrame</strong> 来选择 <strong>drug_dataset[train]</strong> 的所有元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_df = drug_dataset[<span class="string">&quot;train&quot;</span>][:]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>🚨 在底层，<code>Dataset.set_format()</code> 改变了数据集的 <code>__getitem__()</code> dunder 方法的返回格式。 这意味着当我们想从 <code>&quot;pandas&quot;</code> 格式的 <code>Dataset</code> 中创建像 <code>train_df</code> 这样的新对象时，我们需要对整个数据集进行切片以获得 <code>pandas.DataFrame</code>。</p>
<p>无论输出格式如何，您都可以自己验证 <code>drug_dataset[&quot;train&quot;]</code> 的类型依然还是 <code>Dataset</code>。</p>
</blockquote>
<p>从这里我们可以使用我们想要的所有 Pandas 功能。例如，我们可以通过花式链接来计算 <strong>condition</strong>类之间的分布 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">frequencies = (</span><br><span class="line">    train_df[<span class="string">&quot;condition&quot;</span>]</span><br><span class="line">    .value_counts()</span><br><span class="line">    .to_frame()</span><br><span class="line">    .reset_index()</span><br><span class="line">    .rename(columns=&#123;<span class="string">&quot;index&quot;</span>: <span class="string">&quot;condition&quot;</span>, <span class="string">&quot;condition&quot;</span>: <span class="string">&quot;frequency&quot;</span>&#125;)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">frequencies.head()</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th></th>
<th>condition</th>
<th>frequency</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>birth control</td>
<td>27655</td>
</tr>
<tr>
<td>1</td>
<td>depression</td>
<td>8023</td>
</tr>
<tr>
<td>2</td>
<td>acne</td>
<td>5209</td>
</tr>
<tr>
<td>3</td>
<td>anxiety</td>
<td>4991</td>
</tr>
<tr>
<td>4</td>
<td>pain</td>
<td>4744</td>
</tr>
</tbody>
</table>
<p>一旦我们完成了 Pandas 分析，我们总是通过使用对象 **Dataset.from_pandas()**方法可以创建一个新的 <strong>Dataset</strong> 如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">freq_dataset = Dataset.from_pandas(frequencies)</span><br><span class="line">freq_dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;frequency&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">819</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><font size="5">Dataset.reset_format()</font></p>
<p>重置格式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset.reset_format()</span><br></pre></td></tr></table></figure>
<h3 id="5创建验证集"><a class="markdownIt-Anchor" href="#5创建验证集"></a> 5.创建验证集</h3>
<p>尽管我们有一个可以用于评估的测试集，但在开发过程中保持测试集不变并创建一个单独的验证集是一个很好的做法。一旦您对模型在测试集上的表现感到满意，您就可以对验证集进行最终的检查。此过程有助于降低您过拟合测试集并部署在现实世界数据上失败的模型的风险。</p>
<p><font size="5">Dataset.train_test_split()</font></p>
<p>Datasets提供了一个基于<strong>scikit-learn</strong>的经典方法<strong>Dataset.train_test_split()</strong> .让我们用它把我们的训练集分成 <strong>train</strong> 和 <strong>validation</strong> （为了可以复现，我们将设置<strong>seed</strong>的值为一个常量）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset_clean = drug_dataset[<span class="string">&quot;train&quot;</span>].train_test_split(train_size=<span class="number">0.8</span>, seed=<span class="number">42</span>) <span class="comment">#  将原训练集的0.8作为新训练集，0.2作为新测试集</span></span><br><span class="line"><span class="comment"># Rename the default &quot;test&quot; split to &quot;validation&quot;</span></span><br><span class="line">drug_dataset_clean[<span class="string">&quot;validation&quot;</span>] = drug_dataset_clean.pop(<span class="string">&quot;test&quot;</span>) <span class="comment"># 将新的测试集作为验证集</span></span><br><span class="line"><span class="comment"># Add the &quot;test&quot; set to our `DatasetDict`</span></span><br><span class="line">drug_dataset_clean[<span class="string">&quot;test&quot;</span>] = drug_dataset[<span class="string">&quot;test&quot;</span>] <span class="comment"># 将原测试集作为新的测试集</span></span><br><span class="line">drug_dataset_clean</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>, <span class="string">&#x27;review_clean&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">110811</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>, <span class="string">&#x27;review_clean&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">27703</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>, <span class="string">&#x27;review_clean&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">46108</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="6保存数据集"><a class="markdownIt-Anchor" href="#6保存数据集"></a> 6.保存数据集</h3>
<p>Datasets 提供了三个主要功能来以不同的格式保存您的数据集：</p>
<table>
<thead>
<tr>
<th>数据格式</th>
<th>对应的方法</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrow</td>
<td><code>Dataset.save_to_disk()</code></td>
</tr>
<tr>
<td>CSV</td>
<td><code>Dataset.to_csv()</code></td>
</tr>
<tr>
<td>JSON</td>
<td><code>Dataset.to_json()</code></td>
</tr>
</tbody>
</table>
<p><font size="5">Arrow 格式</font></p>
<blockquote>
<p>例如，让我们以 <strong>Arrow 格式</strong>保存我们清洗过的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drug_dataset_clean.save_to_disk(<span class="string">&quot;drug-reviews&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将创建一个具有以下结构的目录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">drug-reviews/</span><br><span class="line">├── dataset_dict.json</span><br><span class="line">├── test</span><br><span class="line">│   ├── dataset.arrow</span><br><span class="line">│   ├── dataset_info.json</span><br><span class="line">│   └── state.json</span><br><span class="line">├── train</span><br><span class="line">│   ├── dataset.arrow</span><br><span class="line">│   ├── dataset_info.json</span><br><span class="line">│   ├── indices.arrow</span><br><span class="line">│   └── state.json</span><br><span class="line">└── validation</span><br><span class="line"> ├── dataset.arrow</span><br><span class="line"> ├── dataset_info.json</span><br><span class="line"> ├── indices.arrow</span><br><span class="line"> └── state.json</span><br></pre></td></tr></table></figure>
<p>在那里我们可以看到每个部分.arrow表，以及一些元数据数据集信息.json和状态文件保存在一起.<font color="red">您可以将 Arrow 格式视为一个精美的列和行的表格，它针对构建处理和传输大型数据集的高性能应用程序进行了优化。</font></p>
<p><font size="5">load_from_disk() </font></p>
<p>保存数据集后，我们可以使用 <strong>load_from_disk()</strong> 功能从磁盘读取数据如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_from_disk</span><br><span class="line"></span><br><span class="line">drug_dataset_reloaded = load_from_disk(<span class="string">&quot;drug-reviews&quot;</span>)</span><br><span class="line">drug_dataset_reloaded</span><br><span class="line"></span><br><span class="line">DatasetDict(&#123;</span><br><span class="line"> train: Dataset(&#123;</span><br><span class="line">     features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>],</span><br><span class="line">     num_rows: <span class="number">110811</span></span><br><span class="line"> &#125;)</span><br><span class="line"> validation: Dataset(&#123;</span><br><span class="line">     features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>],</span><br><span class="line">     num_rows: <span class="number">27703</span></span><br><span class="line"> &#125;)</span><br><span class="line"> test: Dataset(&#123;</span><br><span class="line">     features: [<span class="string">&#x27;patient_id&#x27;</span>, <span class="string">&#x27;drugName&#x27;</span>, <span class="string">&#x27;condition&#x27;</span>, <span class="string">&#x27;review&#x27;</span>, <span class="string">&#x27;rating&#x27;</span>, <span class="string">&#x27;date&#x27;</span>, <span class="string">&#x27;usefulCount&#x27;</span>, <span class="string">&#x27;review_length&#x27;</span>],</span><br><span class="line">     num_rows: <span class="number">46108</span></span><br><span class="line"> &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
</blockquote>
<p><font size="5">CSV 和 JSON 格式</font></p>
<blockquote>
<p>对于 CSV 和 JSON 格式，我们必须将每个部分存储为单独的文件。一种方法是迭代<strong>DatasetDict</strong>中的键和值 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> split, dataset <span class="keyword">in</span> drug_dataset_clean.items():</span><br><span class="line"> dataset.to_json(<span class="string">f&quot;drug-reviews-<span class="subst">&#123;split&#125;</span>.jsonl&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将保存每个拆分都是<a target="_blank" rel="noopener" href="https://jsonlines.org/">JSON的标准格式</a>，其中数据集中的每一行都存储为一行 JSON。这是第一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!head -n <span class="number">1</span> drug-reviews-train.jsonl</span><br><span class="line">&#123;<span class="string">&quot;patient_id&quot;</span>:<span class="number">141780</span>,<span class="string">&quot;drugName&quot;</span>:<span class="string">&quot;Escitalopram&quot;</span>,<span class="string">&quot;condition&quot;</span>:<span class="string">&quot;depression&quot;</span>,<span class="string">&quot;review&quot;</span>:<span class="string">&quot;\&quot;I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven&#x27;t worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\&quot;&quot;</span>,<span class="string">&quot;rating&quot;</span>:<span class="number">9.0</span>,<span class="string">&quot;date&quot;</span>:<span class="string">&quot;May 29, 2011&quot;</span>,<span class="string">&quot;usefulCount&quot;</span>:<span class="number">10</span>,<span class="string">&quot;review_length&quot;</span>:<span class="number">125</span>&#125;</span><br></pre></td></tr></table></figure>
<p>然后我们可以使用<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter5/2">第二节</a>学过的技术加载 JSON 文件如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_files = &#123;</span><br><span class="line"> <span class="string">&quot;train&quot;</span>: <span class="string">&quot;drug-reviews-train.jsonl&quot;</span>,</span><br><span class="line"> <span class="string">&quot;validation&quot;</span>: <span class="string">&quot;drug-reviews-validation.jsonl&quot;</span>,</span><br><span class="line"> <span class="string">&quot;test&quot;</span>: <span class="string">&quot;drug-reviews-test.jsonl&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">drug_dataset_reloaded = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=data_files)</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="datasets用于大模型"><a class="markdownIt-Anchor" href="#datasets用于大模型"></a> Datasets用于大模型</h2>
<p>Datasets 旨在克服加载极大数据集的存储限制。它通过将数据集作为内存映射文件来处理，并通过在语料库中流化条目来摆脱硬盘限制, 从而使你避免内存管理问题。</p>
<h3 id="1pile"><a class="markdownIt-Anchor" href="#1pile"></a> 1.Pile</h3>
<p>The Pile 是由<a target="_blank" rel="noopener" href="https://www.eleuther.ai/">EleutherAI</a>创建的一个英语文本语料库, 用于训练大规模语言模型。它包含各种各样的数据集, 涵盖科学文章, GitHub 代码库以及过滤的Web文本。训练语料库在<a target="_blank" rel="noopener" href="https://the-eye.eu/public/AI/pile/">14 GB chunks</a>, 并且你也可以下载几个<a target="_blank" rel="noopener" href="https://the-eye.eu/public/AI/pile_preliminary_components/">单独的组件</a>。 让我们先来看看 PubMed Abstracts 数据集, 它是<a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/">PubMed</a>上的1500万篇生物医学出版物的摘要的语料库。 数据集采用<a target="_blank" rel="noopener" href="https://jsonlines.org/">JSON行格式</a> 并使用<code>zstandard</code>库进行压缩, 所以我们首先需要先安装<code>zstandard</code>库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install zstandard</span><br></pre></td></tr></table></figure>
<p>接下来, 我们可以使用<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter5/2">第二节</a>中所学的加载远程数据集的方法加载数据集:</p>
<p><font color="red">注意该数据集链接已经不能用了</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># This takes a few minutes to run, so go grab a tea or coffee while you wait :)</span></span><br><span class="line">data_files = <span class="string">&quot;https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst&quot;</span></span><br><span class="line">pubmed_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=data_files, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">pubmed_dataset</span><br><span class="line"></span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;meta&#x27;</span>, <span class="string">&#x27;text&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">15518009</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>我们可以看到我们的数据集中有 15,518,009 行和 2 列 — 这是非常多的!</p>
<blockquote>
<p>✎ 默认情况下, 🤗 Datasets 会自动解压加载数据集所需的文件。 如果你想保留硬盘空间, 你可以传递 <code>DownloadConfig(delete_extracted=True)</code> 到 <code>download_config</code> 的 <code>load_dataset()</code>参数. 有关更多详细信息, 请参阅文档](<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig">https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig</a>)。</p>
</blockquote>
<p>让我们看看数据集的第一个元素的内容:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pubmed_dataset[<span class="number">0</span>]</span><br><span class="line">&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409574</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到, 这看起来像是医学文章的摘要。 现在让我们看看我们使用了RAM的多少存储空间来加载数据集!</p>
<h3 id="2内存映射"><a class="markdownIt-Anchor" href="#2内存映射"></a> 2.内存映射</h3>
<p>在 Python 中测量内存使用情况的一个简单的方法是使用<a target="_blank" rel="noopener" href="https://psutil.readthedocs.io/en/latest/"><code>psutil</code></a>库,它可以使用 <code>pip</code>安装, 如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install psutil</span><br></pre></td></tr></table></figure>
<p>它提供了一个 <code>Process</code> 类,这个类允许我们检查当前进程的内存使用情况, 如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> psutil</span><br><span class="line"></span><br><span class="line"><span class="comment"># Process.memory_info is expressed in bytes, so convert to megabytes</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;RAM used: <span class="subst">&#123;psutil.Process().memory_info().rss / (<span class="number">1024</span> * <span class="number">1024</span>):<span class="number">.2</span>f&#125;</span> MB&quot;</span>)</span><br><span class="line"></span><br><span class="line">RAM used: <span class="number">5678.33</span> MB</span><br></pre></td></tr></table></figure>
<p><font color="red">这里的<code>rss</code>属性是指<strong>常驻集</strong>的大小, 它是进程在RAM中占用的内存比例。 这个测量结果也包括了 Python 编译器和我们加载的库所使用的内存, 所以实际上用于加载数据集的内存会更小一些。为了比较, 让我们使用 <code>dataset_size</code> 属性看看数据集在磁盘上有多大。</font>由于结果像之前一样用字节表示, 我们需要手动将其转换为GB:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Number of files in dataset : <span class="subst">&#123;pubmed_dataset.dataset_size&#125;</span>&quot;</span>)</span><br><span class="line">size_gb = pubmed_dataset.dataset_size / (<span class="number">1024</span>**<span class="number">3</span>) <span class="comment"># kb,mb,gb，故为1024的三次方</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dataset size (cache file) : <span class="subst">&#123;size_gb:<span class="number">.2</span>f&#125;</span> GB&quot;</span>)</span><br><span class="line"></span><br><span class="line">Number of files <span class="keyword">in</span> dataset : <span class="number">20979437051</span></span><br><span class="line">Dataset size (cache file) : <span class="number">19.54</span> GB</span><br></pre></td></tr></table></figure>
<p>非常棒 — 尽管它将近20GB, 但我们能够占用很少的RAM空间加载和访问数据集!</p>
<blockquote>
<p>✏️ <strong>试试看!</strong> 从<a target="_blank" rel="noopener" href="https://the-eye.eu/public/AI/pile_preliminary_components/">subsets</a>中选择一个大于你的笔记本或者台式机的RAM大小的子集, 用 🤗 Datasets加载这个数据集, 并且测量RAM的使用量。 请注意, 要获得准确的测量结果, 你需要在另一个进程中执行这个操作。你可以在 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.00027">the Pile paper</a>的表一中找到每个子集解压后的大小。</p>
<p>如果你熟悉 Pandas, 这个结果可能会让人感到很意外。因为 Wes Kinney 的著名的<a target="_blank" rel="noopener" href="https://wesmckinney.com/blog/apache-arrow-pandas-internals/">经验法则</a> 是你需要的RAM应该是数据集的大小的5倍到10倍。 那么 🤗 Datasets 是如何解决这个内存管理问题的呢? 🤗 <font color="red">Datasets 将每一个数据集看作一个<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Memory-mapped_file">内存映射文件</a>, 它提供了RAM和文件系统存储之间的映射, 该映射允许库访问和操作数据集的元素, 而且无需将其完全加载到内存中。</font></p>
</blockquote>
<p>内存映射文件也一个在多个进程之间共享, 这使得像 <code>Dataset.map()</code>之类的方法可以并行化, 并且无需移动或者赋值数据集。在底层, 这些功能都是由<a target="_blank" rel="noopener" href="https://arrow.apache.org/">Apache Arrow</a>内存格式和<a target="_blank" rel="noopener" href="https://arrow.apache.org/docs/python/index.html"><code>pyarrow</code></a>库提供的支持, 使得数据加载和处理速度快如闪电。 (更多有关Apache Arrow的详细信息以及与Pandas的比较, 请查看<a target="_blank" rel="noopener" href="https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a">Dejan Simic’s blog post</a>.) 为了更清晰地看到这个过程, 让我们通过迭代PubMed Abstracts数据集中的所有元素来运行一个速度测试小程序:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line">code_snippet = <span class="string">&quot;&quot;&quot;batch_size = 1000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">for idx in range(0, len(pubmed_dataset), batch_size):</span></span><br><span class="line"><span class="string">    _ = pubmed_dataset[idx:idx + batch_size]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">time = timeit.timeit(stmt=code_snippet, number=<span class="number">1</span>, <span class="built_in">globals</span>=<span class="built_in">globals</span>())</span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">    <span class="string">f&quot;Iterated over <span class="subst">&#123;<span class="built_in">len</span>(pubmed_dataset)&#125;</span> examples (about <span class="subst">&#123;size_gb:<span class="number">.1</span>f&#125;</span> GB) in &quot;</span></span><br><span class="line">    <span class="string">f&quot;<span class="subst">&#123;time:<span class="number">.1</span>f&#125;</span>s, i.e. <span class="subst">&#123;size_gb/time:<span class="number">.3</span>f&#125;</span> GB/s&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="string">&#x27;Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s&#x27;</span></span><br></pre></td></tr></table></figure>
<p>这里我们使用了 Python的 <code>timeit</code> 模块来测量执行 <code>code_snippet</code>所耗的时间。 你通常能以十分之几GB/s到几GB/s的速度迭代数据集。通过上述的方法就已经能够解决大多数大数据集加载的限制, 但是有时候你不得不使用一个很大的数据集, 它甚至都不能存储在笔记本电脑的硬盘上。例如, 如果我们尝试下载整个 Pile, 我们需要825GB的可用磁盘空间! 为了处理这种情况, 🤗 Datasets 提供了一个流式功能, 这个功能允许我们动态下载和访问元素, 并且不需要下载整个数据集。让我们来看看这个功能是如何工作的。</p>
<h3 id="3流数据集"><a class="markdownIt-Anchor" href="#3流数据集"></a> 3.流数据集</h3>
<p>要使用数据集流, 你只需要将 <code>streaming=True</code> 参数传递给 <code>load_dataset()</code> 函数。接下来, 让我们再次加载 PubMed Abstracts 数据集, 但是采用流模式:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pubmed_dataset_streamed = load_dataset(</span><br><span class="line">    <span class="string">&quot;json&quot;</span>, data_files=data_files, split=<span class="string">&quot;train&quot;</span>, streaming=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><font color="red">与我们在本章其他地方遇到的熟悉的 <code>Dataset</code> 不同, <code>streaming=True</code> 返回的对象是一个 <code>IterableDataset</code>。 顾名思义, 要访问 <code>IterableDataset</code> , 我们需要迭代它。</font>我们可以按照如下方式访问流式数据集的第一个元素:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(pubmed_dataset_streamed))</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409574</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p><font size="5">IterableDataset.map()</font></p>
<p>如果您需要在训练期间标记流式数据集中的元素可以使用 <code>IterableDataset.map()</code>进行动态处理。该过程与我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第三章</a>中标记数据集的过程完全相同, 唯一的区别是输出是逐个返回的:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;distilbert-base-uncased&quot;</span>)</span><br><span class="line">tokenized_dataset = pubmed_dataset_streamed.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: tokenizer(x[<span class="string">&quot;text&quot;</span>]))</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(tokenized_dataset))</span><br><span class="line">&#123;<span class="string">&#x27;input_ids&#x27;</span>: [<span class="number">101</span>, <span class="number">4958</span>, <span class="number">5178</span>, <span class="number">4328</span>, <span class="number">6779</span>, ...], <span class="string">&#x27;attention_mask&#x27;</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, ...]&#125;</span><br></pre></td></tr></table></figure>
<p><font size="5">IterableDataset.shuffle()</font></p>
<p>💡 你可以传递 <code>batched=True</code> 来通过流式加速标记化, 如同我们在上一节看到的那样。它将逐批处理示例; 默认的批量大小为 1,000, 可以使用 <code>batch_size</code> 参数指定批量大小。</p>
<p>你还可以使用 <code>IterableDataset.shuffle()</code> 打乱流式数据集, 但与 <code>Dataset.shuffle()</code> 不同的是这只会打乱预定义 <code>buffer_size</code> 中的元素:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=<span class="number">10_000</span>, seed=<span class="number">42</span>)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(shuffled_dataset))</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11410799</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p><font size="5">IterableDataset.take() 和 IterableDataset.skip()</font></p>
<p>在这个示例中, 我们从缓冲区的前 10,000 个示例中随机选择了一个示例。一旦访问了一个示例, 它在缓冲区中的位置就会被语料库中的下一个示例填充 (即, 上述案例中的第 10,001个示例)。你还可以使用 <code>IterableDataset.take()</code> 和 <code>IterableDataset.skip()</code> 函数从流式数据集中选择元素, 它的作用类似于 <code>Dataset.select()</code>。例如, 要选择 PubMed Abstracts 数据集的前5个示例, 我们可以执行以下操作:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dataset_head = pubmed_dataset_streamed.take(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">list</span>(dataset_head)</span><br><span class="line">[&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409574</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409575</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...&#x27;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409576</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&quot;Hypoxaemia in children with severe pneumonia in Papua New Guinea ...&quot;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409577</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Oxygen concentrators and cylinders ...&#x27;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409578</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Oxygen supply in rural africa: a personal experience ...&#x27;</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>同样, 你可以使用 <code>IterableDataset.skip()</code> 函数将打乱的数据集拆分为训练集和验证集, 如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skip the first 1,000 examples and include the rest in the training set</span></span><br><span class="line">train_dataset = shuffled_dataset.skip(<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># Take the first 1,000 examples for the validation set</span></span><br><span class="line">validation_dataset = shuffled_dataset.take(<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">interleave_datasets()</font></p>
<p>让我们用一个常见的任务来进行我们对数据集流的最后探索: 将多个数据集组合在一起创建一个心得语料库。 🤗 Datasets 提供了一个 <code>interleave_datasets()</code> 函数, 它将一个 <code>IterableDataset</code> 对象列表组合为单个的 <code>IterableDataset</code>, 其中新数据集的元素是通过在列表中的对象交替获得的<font color="red">(轮换各选一个）</font>。当你试图组合大型数据集时, 这个函数特别有用, 让我们通过下面这个例子来试着组合 Pile的自由法律数据集,它是来自美国法院的51 GB的法律意见数据集:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">law_dataset_streamed = load_dataset(</span><br><span class="line">    <span class="string">&quot;json&quot;</span>,</span><br><span class="line">    data_files=<span class="string">&quot;https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst&quot;</span>,</span><br><span class="line">    split=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">    streaming=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(law_dataset_streamed))</span><br><span class="line">&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;case_ID&#x27;</span>: <span class="string">&#x27;110921.json&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;case_jurisdiction&#x27;</span>: <span class="string">&#x27;scotus.tar.gz&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;date_created&#x27;</span>: <span class="string">&#x27;2010-04-28T17:12:49Z&#x27;</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>这个数据集足够大, 可以对大多数笔记本电脑的RAM有足够的压力, 但是我们已经能够毫不费力地加载和访问它! 现在我们使用 <code>interleave_datasets()</code> 函数加载来自 FreeLaw 和 PubMed Abstracts 的数据集:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> islice</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> interleave_datasets</span><br><span class="line"></span><br><span class="line">combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])</span><br><span class="line"><span class="built_in">list</span>(islice(combined_dataset, <span class="number">2</span>))</span><br><span class="line">[&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pmid&#x27;</span>: <span class="number">11409574</span>, <span class="string">&#x27;language&#x27;</span>: <span class="string">&#x27;eng&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;case_ID&#x27;</span>: <span class="string">&#x27;110921.json&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;case_jurisdiction&#x27;</span>: <span class="string">&#x27;scotus.tar.gz&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;date_created&#x27;</span>: <span class="string">&#x27;2010-04-28T17:12:49Z&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>这里我们使用了来自Python的 <code>itertools</code> 模块的 <code>islice()</code> 函数从合并的数据集中选择前两个示例, 并且我们可以看到它们实际上就是两个源数据集中的前两个示例拼在一起形成的:</p>
<p><font size="5">传输整个Pile</font></p>
<p>最后, 如果你想流式传输整个825GB的 Pile, 你可以按照如下方式获取所有准备好的文件:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">base_url = <span class="string">&quot;https://the-eye.eu/public/AI/pile/&quot;</span></span><br><span class="line">data_files = &#123;</span><br><span class="line">    <span class="string">&quot;train&quot;</span>: [base_url + <span class="string">&quot;train/&quot;</span> + <span class="string">f&quot;<span class="subst">&#123;idx:02d&#125;</span>.jsonl.zst&quot;</span> <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">30</span>)],</span><br><span class="line">    <span class="string">&quot;validation&quot;</span>: base_url + <span class="string">&quot;val.jsonl.zst&quot;</span>,</span><br><span class="line">    <span class="string">&quot;test&quot;</span>: base_url + <span class="string">&quot;test.jsonl.zst&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line">pile_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=data_files, streaming=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(pile_dataset[<span class="string">&quot;train&quot;</span>]))</span><br><span class="line">&#123;<span class="string">&#x27;meta&#x27;</span>: &#123;<span class="string">&#x27;pile_set_name&#x27;</span>: <span class="string">&#x27;Pile-CC&#x27;</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web...&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>✏️ <strong>试试看!</strong> 使用像<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mc4"><code>mc4</code></a> 或者 <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/oscar"><code>oscar</code></a>这样的大型 Common Crawl 语料库来创建一个流式多语言数据集, 该数据集代表你选择的国家/地区语言的口语比例。例如, 瑞士的四种民族语言分别是德语、法语、意大利语和罗曼什语, 因此你可以尝试根据根据口语比例对Oscar子集进行采用来创建瑞士语料库。</p>
<h2 id="创建自己的数据集"><a class="markdownIt-Anchor" href="#创建自己的数据集"></a> 创建自己的数据集</h2>
<p>在本节中，我们将向您展示如何创建一个<a target="_blank" rel="noopener" href="https://github.com/features/issues/">GitHub issues</a>的语料库，GitHub issues通常用于跟踪 GitHub 存储库中的错误或功能。该语料库可用于各种目的，包括：</p>
<ul>
<li>探索关闭未解决的issue或拉取请求需要多长时间</li>
<li>训练一个<em>多标签分类器</em>可以根据issue的描述（例如，“错误”、“增强”或“issue”）用元数据标记issue</li>
<li>创建语义搜索引擎以查找与用户查询匹配的issue</li>
</ul>
<p><font color="red">GitHub issue可以理解为讨论区中的问题以及解答</font></p>
<h3 id="1获取数据集"><a class="markdownIt-Anchor" href="#1获取数据集"></a> 1.获取数据集</h3>
<p>您可以浏览 🤗 Datasets 中的所有issue<a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/issues">Issues tab</a>.如以下屏幕截图所示</p>
<p><img src="datasets-issues.png" alt></p>
<p>一个issue，它包含一个标题、一个描述和一组表征该issue的标签。下面的屏幕截图显示了一个示例.</p>
<p><img src="datasets-issues-single.png" alt></p>
<p>**方法1：**要下载所有存储库的issue，我们将使用<a target="_blank" rel="noopener" href="https://docs.github.com/en/rest">GitHub REST API</a>投票<a target="_blank" rel="noopener" href="https://docs.github.com/en/rest/reference/issues#list-repository-issues">Issues endpoint</a>.此节点返回一个 JSON 对象列表，每个对象包含大量字段，其中包括标题和描述以及有关issue状态的元数据等。</p>
<p>**方法2：**下载issue的一种便捷方式是通过 <strong>requests</strong> 库，这是用 Python 中发出 HTTP 请求的标准方式。您可以通过运行以下的代码来安装库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install requests</span><br></pre></td></tr></table></figure>
<p>安装库后，您通过调用 <strong>requests.get()</strong> 功能来获取<strong>Issues</strong>节点。例如，您可以运行以下命令来获取第一页上的第一个Issues：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://api.github.com/repos/huggingface/datasets/issues?page=1&amp;per_page=1&quot;</span></span><br><span class="line">response = requests.get(url)</span><br></pre></td></tr></table></figure>
<p>这 <strong>response</strong> 对象包含很多关于请求的有用信息，包括 HTTP 状态码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">response.status_code</span><br><span class="line"></span><br><span class="line"><span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>其中一个状态码 <strong>200</strong> 表示请求成功（您可以<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">在这里</a>找到可能的 HTTP 状态代码列表）。然而，我们真正感兴趣的是有效的信息，由于我们知道我们的issues是 JSON 格式，让我们按如下方式查看所有的信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">response.json()</span><br><span class="line">[&#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/issues/2792&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;repository_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;labels_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/issues/2792/labels&#123;/name&#125;&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;comments_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/issues/2792/comments&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;events_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/issues/2792/events&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/2792&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;id&#x27;</span>: <span class="number">968650274</span>,</span><br><span class="line">  <span class="string">&#x27;node_id&#x27;</span>: <span class="string">&#x27;MDExOlB1bGxSZXF1ZXN0NzEwNzUyMjc0&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;number&#x27;</span>: <span class="number">2792</span>,</span><br><span class="line">  <span class="string">&#x27;title&#x27;</span>: <span class="string">&#x27;Update GooAQ&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;user&#x27;</span>: &#123;<span class="string">&#x27;login&#x27;</span>: <span class="string">&#x27;bhavitvyamalik&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;id&#x27;</span>: <span class="number">19718818</span>,</span><br><span class="line">   <span class="string">&#x27;node_id&#x27;</span>: <span class="string">&#x27;MDQ6VXNlcjE5NzE4ODE4&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;avatar_url&#x27;</span>: <span class="string">&#x27;https://avatars.githubusercontent.com/u/19718818?v=4&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;gravatar_id&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/bhavitvyamalik&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;followers_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/followers&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;following_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/following&#123;/other_user&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;gists_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/gists&#123;/gist_id&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;starred_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/starred&#123;/owner&#125;&#123;/repo&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;subscriptions_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/subscriptions&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;organizations_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/orgs&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;repos_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/repos&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;events_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/events&#123;/privacy&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;received_events_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/received_events&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;User&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;site_admin&#x27;</span>: <span class="literal">False</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;labels&#x27;</span>: [],</span><br><span class="line">  <span class="string">&#x27;state&#x27;</span>: <span class="string">&#x27;open&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;locked&#x27;</span>: <span class="literal">False</span>,</span><br><span class="line">  <span class="string">&#x27;assignee&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">  <span class="string">&#x27;assignees&#x27;</span>: [],</span><br><span class="line">  <span class="string">&#x27;milestone&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">  <span class="string">&#x27;comments&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="string">&#x27;created_at&#x27;</span>: <span class="string">&#x27;2021-08-12T11:40:18Z&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;updated_at&#x27;</span>: <span class="string">&#x27;2021-08-12T12:31:17Z&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;closed_at&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">  <span class="string">&#x27;author_association&#x27;</span>: <span class="string">&#x27;CONTRIBUTOR&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;active_lock_reason&#x27;</span>: <span class="literal">None</span>,</span><br><span class="line">  <span class="string">&#x27;pull_request&#x27;</span>: &#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/pulls/2792&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/2792&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;diff_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/2792.diff&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;patch_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/2792.patch&#x27;</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;body&#x27;</span>: <span class="string">&#x27;[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;performed_via_github_app&#x27;</span>: <span class="literal">None</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>哇，这是很多信息！我们可以看到有用的字段，例如 <strong>标题</strong> , <strong>内容</strong> ， <strong>参与的成员</strong>， <strong>issue的描述信息</strong>，以及打开issue的GitHub 用户的信息。</p>
<blockquote>
<p>如 GitHub<a target="_blank" rel="noopener" href="https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting">文档</a> 中所述，未经身份验证的请求限制为每小时 60 个请求。虽然你可以增加 <strong>per_page</strong> 查询参数以减少您发出的请求数量，您仍然会遭到任何超过几千个issue的存储库的速率限制。因此，您应该关注 GitHub 的<a target="_blank" rel="noopener" href="https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token">创建个人身份令牌</a>，创建一个个人访问令牌这样您就可以将速率限制提高到每小时 5,000 个请求。获得令牌后，您可以将其包含在请求标头中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">GITHUB_TOKEN = xxx  <span class="comment"># Copy your GitHub token here</span></span><br><span class="line"></span><br><span class="line">headers = &#123;<span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;token <span class="subst">&#123;GITHUB_TOKEN&#125;</span>&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>⚠️ 不要与陌生人共享存在GITHUB令牌的笔记本。我们建议您在使用完后将GITHUB令牌删除，以避免意外泄漏此信息。一个更好的做法是，将令牌存储在.env文件中，并使用 <a target="_blank" rel="noopener" href="https://github.com/theskumar/python-dotenv"><code>python-dotenv</code> library</a> 为您自动将其作为环境变量加载。0</p>
</blockquote>
<p><font size="5">下载所有issue</font></p>
<p>现在我们有了访问令牌，让我们创建一个可以从 GitHub 存储库下载所有issue的函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tqdm.notebook <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fetch_issues</span>(<span class="params"></span></span><br><span class="line"><span class="params">    owner=<span class="string">&quot;huggingface&quot;</span>,</span></span><br><span class="line"><span class="params">    repo=<span class="string">&quot;datasets&quot;</span>,</span></span><br><span class="line"><span class="params">    num_issues=<span class="number">10_000</span>,</span></span><br><span class="line"><span class="params">    rate_limit=<span class="number">5_000</span>,</span></span><br><span class="line"><span class="params">    issues_path=Path(<span class="params"><span class="string">&quot;.&quot;</span></span>),</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> issues_path.is_dir():</span><br><span class="line">        issues_path.mkdir(exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    batch = []</span><br><span class="line">    all_issues = []</span><br><span class="line">    per_page = <span class="number">100</span>  <span class="comment"># Number of issues to return per page</span></span><br><span class="line">    num_pages = math.ceil(num_issues / per_page)</span><br><span class="line">    base_url = <span class="string">&quot;https://api.github.com/repos&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(num_pages)):</span><br><span class="line">        <span class="comment"># Query with state=all to get both open and closed issues</span></span><br><span class="line">        query = <span class="string">f&quot;issues?page=<span class="subst">&#123;page&#125;</span>&amp;per_page=<span class="subst">&#123;per_page&#125;</span>&amp;state=all&quot;</span></span><br><span class="line">        issues = requests.get(<span class="string">f&quot;<span class="subst">&#123;base_url&#125;</span>/<span class="subst">&#123;owner&#125;</span>/<span class="subst">&#123;repo&#125;</span>/<span class="subst">&#123;query&#125;</span>&quot;</span>, headers=headers)</span><br><span class="line">        batch.extend(issues.json())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(batch) &gt; rate_limit <span class="keyword">and</span> <span class="built_in">len</span>(all_issues) &lt; num_issues:</span><br><span class="line">            all_issues.extend(batch)</span><br><span class="line">            batch = []  <span class="comment"># Flush batch for next time period</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Reached GitHub rate limit. Sleeping for one hour ...&quot;</span>)</span><br><span class="line">            time.sleep(<span class="number">60</span> * <span class="number">60</span> + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    all_issues.extend(batch)</span><br><span class="line">    df = pd.DataFrame.from_records(all_issues)</span><br><span class="line">    df.to_json(<span class="string">f&quot;<span class="subst">&#123;issues_path&#125;</span>/<span class="subst">&#123;repo&#125;</span>-issues.jsonl&quot;</span>, orient=<span class="string">&quot;records&quot;</span>, lines=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;Downloaded all the issues for <span class="subst">&#123;repo&#125;</span>! Dataset stored at <span class="subst">&#123;issues_path&#125;</span>/<span class="subst">&#123;repo&#125;</span>-issues.jsonl&quot;</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>现在我们可以调用 <strong>fetch_issues()</strong> 批量下载所有issue，避免超过GitHub每小时的请求数限制；<font color="red">结果将存储在repository_name-issues.jsonl文件，其中每一行都是一个 JSON 对象，代表一个issue。</font>让我们使用这个函数从 🤗 Datasets中抓取所有issue：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Depending on your internet connection, this can take several minutes to run...</span></span><br><span class="line">fetch_issues()</span><br></pre></td></tr></table></figure>
<p>下载issue后，我们可以使用我们 <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter5/2">section 2</a>新学会的方法在本地加载它们:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">issues_dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=<span class="string">&quot;datasets-issues.jsonl&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">issues_dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;repository_url&#x27;</span>, <span class="string">&#x27;labels_url&#x27;</span>, <span class="string">&#x27;comments_url&#x27;</span>, <span class="string">&#x27;events_url&#x27;</span>, <span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;node_id&#x27;</span>, <span class="string">&#x27;number&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;locked&#x27;</span>, <span class="string">&#x27;assignee&#x27;</span>, <span class="string">&#x27;assignees&#x27;</span>, <span class="string">&#x27;milestone&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;created_at&#x27;</span>, <span class="string">&#x27;updated_at&#x27;</span>, <span class="string">&#x27;closed_at&#x27;</span>, <span class="string">&#x27;author_association&#x27;</span>, <span class="string">&#x27;active_lock_reason&#x27;</span>, <span class="string">&#x27;pull_request&#x27;</span>, <span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;timeline_url&#x27;</span>, <span class="string">&#x27;performed_via_github_app&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">3019</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>太好了，我们已经从头开始创建了我们的第一个数据集！但是为什么会有几千个issue，而🤗 Datasets存储库中的<a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/issues">Issues 选项卡</a>总共却只显示了大约 1,000 个issue🤔？如 GitHub <a target="_blank" rel="noopener" href="https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user">文档</a>中所述，那是因为我们也下载了所有的拉取请求：</p>
<blockquote>
<p>Git Hub的REST API v3认为每个pull请求都是一个issue，但并不是每个issue都是一个pull请求。因此，“Issues”节点可能在响应中同时返回issue和拉取请求。你可以通过pull_request 的 key来辨别pull请求。请注意，从“Issues”节点返回的pull请求的id将是一个issue id。</p>
</blockquote>
<p>由于issue和pull request的内容有很大的不同，我们先做一些小的预处理，让我们能够区分它们。</p>
<h3 id="2清洗数据集"><a class="markdownIt-Anchor" href="#2清洗数据集"></a> 2.清洗数据集</h3>
<p><strong>pull_request</strong> 列可用于区分issue和拉取请求。让我们随机挑选一些样本，看看有什么不同。我们将使用在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter5/3">第三节</a>, 学习的方法，使用 <strong>Dataset.shuffle()</strong> 和 <strong>Dataset.select()</strong> 抽取一个随机样本，然后将 <strong>html_url</strong> 和 <strong>pull_request</strong> 列使用zip函数打包，以便我们可以比较各种 URL：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sample = issues_dataset.shuffle(seed=<span class="number">666</span>).select(<span class="built_in">range</span>(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the URL and pull request entries</span></span><br><span class="line"><span class="keyword">for</span> url, pr <span class="keyword">in</span> <span class="built_in">zip</span>(sample[<span class="string">&quot;html_url&quot;</span>], sample[<span class="string">&quot;pull_request&quot;</span>]):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;&gt;&gt; URL: <span class="subst">&#123;url&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;&gt;&gt; Pull request: <span class="subst">&#123;pr&#125;</span>\n&quot;</span>)</span><br><span class="line">&gt;&gt; URL: https://github.com/huggingface/datasets/pull/<span class="number">850</span></span><br><span class="line">&gt;&gt; Pull request: &#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/pulls/850&#x27;</span>, <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/850&#x27;</span>, <span class="string">&#x27;diff_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/850.diff&#x27;</span>, <span class="string">&#x27;patch_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/850.patch&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt; URL: https://github.com/huggingface/datasets/issues/<span class="number">2773</span></span><br><span class="line">&gt;&gt; Pull request: <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; URL: https://github.com/huggingface/datasets/pull/<span class="number">783</span></span><br><span class="line">&gt;&gt; Pull request: &#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/pulls/783&#x27;</span>, <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/783&#x27;</span>, <span class="string">&#x27;diff_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/783.diff&#x27;</span>, <span class="string">&#x27;patch_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/783.patch&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>这里我们可以看到，每个pull请求都与各种url相关联，而普通issue只有一个None条目。我们可以使用这一点不同来创建一个新的is_pull_request列通过检查pull_request字段是否为None来区分它们:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">issues_dataset = issues_dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;is_pull_request&quot;</span>: <span class="literal">False</span> <span class="keyword">if</span> x[<span class="string">&quot;pull_request&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="literal">True</span>&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><font color="red">尽管我们可以通过删除或重命名某些列来进一步清理数据集，但在此阶段尽可能保持数据集“原始”状态通常是一个很好的做法，以便它可以在多个应用程序中轻松使用。</font>在我们将数据集推送到 Hugging Face Hub 之前，让我们再添加一些缺少的数据：与每个issue和拉取请求相关的评论。我们接下来将添加它们——你猜对了——我们将依然使用GitHub REST API！</p>
<h3 id="3扩充数据集"><a class="markdownIt-Anchor" href="#3扩充数据集"></a> 3.扩充数据集</h3>
<p><font size="5"> 拉取评论</font></p>
<p>GitHub REST API 提供了一个 <a target="_blank" rel="noopener" href="https://docs.github.com/en/rest/reference/issues#list-issue-comments">评论节点</a> 返回与issue编号相关的所有评论。让我们测试节点以查看它返回的内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">issue_number = <span class="number">2792</span></span><br><span class="line">url = <span class="string">f&quot;https://api.github.com/repos/huggingface/datasets/issues/<span class="subst">&#123;issue_number&#125;</span>/comments&quot;</span></span><br><span class="line">response = requests.get(url, headers=headers)</span><br><span class="line">response.json()</span><br><span class="line">[&#123;<span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/issues/comments/897594128&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;issue_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/repos/huggingface/datasets/issues/2792&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;id&#x27;</span>: <span class="number">897594128</span>,</span><br><span class="line">  <span class="string">&#x27;node_id&#x27;</span>: <span class="string">&#x27;IC_kwDODunzps41gDMQ&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;user&#x27;</span>: &#123;<span class="string">&#x27;login&#x27;</span>: <span class="string">&#x27;bhavitvyamalik&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;id&#x27;</span>: <span class="number">19718818</span>,</span><br><span class="line">   <span class="string">&#x27;node_id&#x27;</span>: <span class="string">&#x27;MDQ6VXNlcjE5NzE4ODE4&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;avatar_url&#x27;</span>: <span class="string">&#x27;https://avatars.githubusercontent.com/u/19718818?v=4&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;gravatar_id&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;html_url&#x27;</span>: <span class="string">&#x27;https://github.com/bhavitvyamalik&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;followers_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/followers&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;following_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/following&#123;/other_user&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;gists_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/gists&#123;/gist_id&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;starred_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/starred&#123;/owner&#125;&#123;/repo&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;subscriptions_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/subscriptions&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;organizations_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/orgs&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;repos_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/repos&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;events_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/events&#123;/privacy&#125;&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;received_events_url&#x27;</span>: <span class="string">&#x27;https://api.github.com/users/bhavitvyamalik/received_events&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;User&#x27;</span>,</span><br><span class="line">   <span class="string">&#x27;site_admin&#x27;</span>: <span class="literal">False</span>&#125;,</span><br><span class="line">  <span class="string">&#x27;created_at&#x27;</span>: <span class="string">&#x27;2021-08-12T12:21:52Z&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;updated_at&#x27;</span>: <span class="string">&#x27;2021-08-12T12:31:17Z&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;author_association&#x27;</span>: <span class="string">&#x27;CONTRIBUTOR&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;body&#x27;</span>: <span class="string">&quot;@albertvillanova my tests are failing here:\r\n```\r\ndataset_name = &#x27;gooaq&#x27;\r\n\r\n    def test_load_dataset(self, dataset_name):\r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\r\n&gt;       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\r\n\r\ntests/test_dataset_common.py:234: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntests/test_dataset_common.py:187: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) &gt; 0)\r\nE   AssertionError: False is not true\r\n```\r\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?&quot;</span>,</span><br><span class="line">  <span class="string">&#x27;performed_via_github_app&#x27;</span>: <span class="literal">None</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>我们可以看到注释存储在body字段中，所以让我们编写一个简单的函数，通过在response.json()中为每个元素挑选body内容来返回与某个issue相关的所有评论：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_comments</span>(<span class="params">issue_number</span>):</span><br><span class="line">    url = <span class="string">f&quot;https://api.github.com/repos/huggingface/datasets/issues/<span class="subst">&#123;issue_number&#125;</span>/comments&quot;</span></span><br><span class="line">    response = requests.get(url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> [r[<span class="string">&quot;body&quot;</span>] <span class="keyword">for</span> r <span class="keyword">in</span> response.json()]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Test our function works as expected</span></span><br><span class="line">get_comments(<span class="number">2792</span>)</span><br><span class="line">[<span class="string">&quot;@albertvillanova my tests are failing here:\r\n```\r\ndataset_name = &#x27;gooaq&#x27;\r\n\r\n    def test_load_dataset(self, dataset_name):\r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\r\n&gt;       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\r\n\r\ntests/test_dataset_common.py:234: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntests/test_dataset_common.py:187: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) &gt; 0)\r\nE   AssertionError: False is not true\r\n```\r\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>这看起来不错，所以让我们使用 <strong>Dataset.map()</strong> 方法在我们数据集中每个issue的添加一个<strong>comments</strong>列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Depending on your internet connection, this can take a few minutes...</span></span><br><span class="line">issues_with_comments_dataset = issues_dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;comments&quot;</span>: get_comments(x[<span class="string">&quot;number&quot;</span>])&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="4上传数据集"><a class="markdownIt-Anchor" href="#4上传数据集"></a> 4.上传数据集</h3>
<p>现在我们有了增强的数据集，是时候将它推送到 Hub 以便我们可以与社区共享它了！ 上传数据集非常简单：就像 🤗 Transformers 中的模型和分词器一样，我们可以使用 <code>push_to_hub()</code> 方法来推送数据集。 为此，我们需要一个身份验证令牌，它可以通过首先使用 <code>notebook_login()</code> 函数登录到 Hugging Face Hub 来获得：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> notebook_login</span><br><span class="line"></span><br><span class="line">notebook_login()</span><br></pre></td></tr></table></figure>
<p>这将创建一个小部件，您可以在其中输入您的用户名和密码， API 令牌将保存在~/.huggingface/令牌.如果您在终端中运行代码，则可以改为通过 CLI 登录：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure>
<p>完成此操作后，我们可以通过运行下面的代码上传我们的数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">issues_with_comments_dataset.push_to_hub(<span class="string">&quot;github-issues&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>之后，任何人都可以通过便捷地提供带有存储库 ID 作为 path 参数的 load_dataset() 来下载数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">remote_dataset = load_dataset(<span class="string">&quot;lewtun/github-issues&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">remote_dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;repository_url&#x27;</span>, <span class="string">&#x27;labels_url&#x27;</span>, <span class="string">&#x27;comments_url&#x27;</span>, <span class="string">&#x27;events_url&#x27;</span>, <span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;node_id&#x27;</span>, <span class="string">&#x27;number&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;locked&#x27;</span>, <span class="string">&#x27;assignee&#x27;</span>, <span class="string">&#x27;assignees&#x27;</span>, <span class="string">&#x27;milestone&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;created_at&#x27;</span>, <span class="string">&#x27;updated_at&#x27;</span>, <span class="string">&#x27;closed_at&#x27;</span>, <span class="string">&#x27;author_association&#x27;</span>, <span class="string">&#x27;active_lock_reason&#x27;</span>, <span class="string">&#x27;pull_request&#x27;</span>, <span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;performed_via_github_app&#x27;</span>, <span class="string">&#x27;is_pull_request&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">2855</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>您还可以使用一些 Git 魔法直接从终端将数据集上传到 Hugging Face Hub。有关如何执行此操作的详细信息，请参阅 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/share#share-a-dataset-using-the-cli">🤗 Datasets guide</a> 指南。</p>
</blockquote>
<h3 id="5创建数据集卡片"><a class="markdownIt-Anchor" href="#5创建数据集卡片"></a> 5.创建数据集卡片</h3>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/chapter5/5?fw=pt#%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8D%A1%E7%89%87">创建自己的数据集 (huggingface.co)</a></p>
<h2 id="使用-faiss-进行语义搜索"><a class="markdownIt-Anchor" href="#使用-faiss-进行语义搜索"></a> 使用 FAISS 进行语义搜索</h2>
<h3 id="1使用embedding进行语义搜索"><a class="markdownIt-Anchor" href="#1使用embedding进行语义搜索"></a> 1.使用embedding进行语义搜索</h3>
<p>基于 Transformer 的语言模型会将文本中的每个标记转换为嵌入向量.<font color="red">事实证明，可以“汇集”各个嵌入向量来创建整个句子、段落或文档（在某些情况下）的向量表示。然后，通过计算每个嵌入之间的点积相似度（或其他一些相似度度量）并返回相似度最大的文档，这些嵌入可用于在语料库中找到相似的文档。</font>在本节中，我们将使用嵌入来开发语义搜索引擎。与基于将查询中的关键字的传统方法相比，这些搜索引擎具有多种优势。</p>
<h3 id="2加载和准备数据集"><a class="markdownIt-Anchor" href="#2加载和准备数据集"></a> 2.加载和准备数据集</h3>
<p>我们需要做的第一件事是下载我们的 GitHub issues 数据集，所以让我们像往常那样使用 <code>load_dataset()</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">issues_dataset = load_dataset(<span class="string">&quot;lewtun/github-issues&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line">issues_dataset</span><br><span class="line"></span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;repository_url&#x27;</span>, <span class="string">&#x27;labels_url&#x27;</span>, <span class="string">&#x27;comments_url&#x27;</span>, <span class="string">&#x27;events_url&#x27;</span>, <span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;node_id&#x27;</span>, <span class="string">&#x27;number&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;locked&#x27;</span>, <span class="string">&#x27;assignee&#x27;</span>, <span class="string">&#x27;assignees&#x27;</span>, <span class="string">&#x27;milestone&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;created_at&#x27;</span>, <span class="string">&#x27;updated_at&#x27;</span>, <span class="string">&#x27;closed_at&#x27;</span>, <span class="string">&#x27;author_association&#x27;</span>, <span class="string">&#x27;active_lock_reason&#x27;</span>, <span class="string">&#x27;pull_request&#x27;</span>, <span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;performed_via_github_app&#x27;</span>, <span class="string">&#x27;is_pull_request&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">2855</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>这里我们在load_dataset()中使用了默认的训练集分割，所以它返回一个数据集而不是数据集字典。</p>
<p><font size="5">过滤掉pull请求</font></p>
<p><font color="red">第一项任务是过滤掉pull请求，因为这些请求很少用于回答用户提出的问题，而且会给我们的搜索引擎带来噪声。</font>现在应该很熟悉了，我们可以使用<code>dataset.filter()</code>函数来排除数据集中的这些行。同时，让我们也过滤掉没有注释的行，因为这些行不会是用户提问的答案:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">issues_dataset = issues_dataset.<span class="built_in">filter</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: (x[<span class="string">&quot;is_pull_request&quot;</span>] == <span class="literal">False</span> <span class="keyword">and</span> <span class="built_in">len</span>(x[<span class="string">&quot;comments&quot;</span>]) &gt; <span class="number">0</span>)</span><br><span class="line">)</span><br><span class="line">issues_dataset</span><br><span class="line"></span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;repository_url&#x27;</span>, <span class="string">&#x27;labels_url&#x27;</span>, <span class="string">&#x27;comments_url&#x27;</span>, <span class="string">&#x27;events_url&#x27;</span>, <span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;node_id&#x27;</span>, <span class="string">&#x27;number&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;user&#x27;</span>, <span class="string">&#x27;labels&#x27;</span>, <span class="string">&#x27;state&#x27;</span>, <span class="string">&#x27;locked&#x27;</span>, <span class="string">&#x27;assignee&#x27;</span>, <span class="string">&#x27;assignees&#x27;</span>, <span class="string">&#x27;milestone&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;created_at&#x27;</span>, <span class="string">&#x27;updated_at&#x27;</span>, <span class="string">&#x27;closed_at&#x27;</span>, <span class="string">&#x27;author_association&#x27;</span>, <span class="string">&#x27;active_lock_reason&#x27;</span>, <span class="string">&#x27;pull_request&#x27;</span>, <span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;performed_via_github_app&#x27;</span>, <span class="string">&#x27;is_pull_request&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">771</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><font size="5">删除其余部分</font></p>
<p>我们可以看到我们的数据集中有很多列，其中大部分我们不需要构建我们的搜索引擎。从搜索的角度来看，信息量最大的列是 <strong>title</strong> , <strong>body</strong> ， 和 <strong>comments</strong> ，而 <strong>html_url</strong> 为我们提供了一个回到源问题的链接。让我们使用 <strong>Dataset.remove_columns()</strong> 删除其余部分的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">columns = issues_dataset.column_names</span><br><span class="line">columns_to_keep = [<span class="string">&quot;title&quot;</span>, <span class="string">&quot;body&quot;</span>, <span class="string">&quot;html_url&quot;</span>, <span class="string">&quot;comments&quot;</span>]</span><br><span class="line">columns_to_remove = <span class="built_in">set</span>(columns_to_keep).symmetric_difference(columns)</span><br><span class="line">issues_dataset = issues_dataset.remove_columns(columns_to_remove)</span><br><span class="line">issues_dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;body&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">771</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><font size="5">用问题的标题和正文来扩充每条评论</font></p>
<p>为了创建我们的嵌入，我们将用问题的标题和正文来扩充每条评论，因为这些字段通常包含有用的上下文信息。因为我们的 <strong>comments</strong> 列当前是每个问题的评论列表，我们需要“重新组合”列，以便每一条评论都包含一个 <strong>(html_url, title, body, comment)</strong> 元组。在 Pandas 中，我们可以使用 <a target="_blank" rel="noopener" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html">DataFrame.explode() 函数</a>, 它为类似列表的列中的每个元素创建一个新行，同时复制所有其他列值。为了看到它的实际效果，让我们首先切换到 Pandas的<strong>DataFrame</strong> 格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">issues_dataset.set_format(<span class="string">&quot;pandas&quot;</span>)</span><br><span class="line">df = issues_dataset[:]</span><br></pre></td></tr></table></figure>
<p>如果我们检查这里的第一行 <strong>DataFrame</strong> 我们可以看到有四个评论与这个问题相关：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&quot;comments&quot;</span>][<span class="number">0</span>].tolist()</span><br><span class="line">[<span class="string">&#x27;the bug code locate in ：\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset(&quot;glue&quot;, data_args.task_name, cache_dir=model_args.cache_dir)&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;cannot connect，even by Web browser，please check that  there is some  problems。&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>我们希望这些评论中的每一条都得到一行。让我们检查是否是这种情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">comments_df = df.explode(<span class="string">&quot;comments&quot;</span>, ignore_index=<span class="literal">True</span>)</span><br><span class="line">comments_df.head(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th></th>
<th>html_url</th>
<th>title</th>
<th>comments</th>
<th>body</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td><a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/issues/2787">https://github.com/huggingface/datasets/issues/2787</a></td>
<td>ConnectionError: Couldn’t reach <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com">https://raw.githubusercontent.com</a></td>
<td>the bug code locate in ：\r\n if data_args.task_name is not None…</td>
<td>Hello,\r\nI am trying to run run_glue.py and it gives me this error…</td>
</tr>
<tr>
<td>1</td>
<td><a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/issues/2787">https://github.com/huggingface/datasets/issues/2787</a></td>
<td>ConnectionError: Couldn’t reach <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com">https://raw.githubusercontent.com</a></td>
<td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of <code>ConnectionError</code> coming from the <a target="_blank" rel="noopener" href="http://github.com">github.com</a> website: <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com">https://raw.githubusercontent.com</a>…</td>
<td>Hello,\r\nI am trying to run run_glue.py and it gives me this error…</td>
</tr>
<tr>
<td>2</td>
<td><a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/issues/2787">https://github.com/huggingface/datasets/issues/2787</a></td>
<td>ConnectionError: Couldn’t reach <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com">https://raw.githubusercontent.com</a></td>
<td>cannot connect，even by Web browser，please check that there is some problems。</td>
<td>Hello,\r\nI am trying to run run_glue.py and it gives me this error…</td>
</tr>
<tr>
<td>3</td>
<td><a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets/issues/2787">https://github.com/huggingface/datasets/issues/2787</a></td>
<td>ConnectionError: Couldn’t reach <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com">https://raw.githubusercontent.com</a></td>
<td>I can access <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py">https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py</a> without problem…</td>
<td>Hello,\r\nI am trying to run run_glue.py and it gives me this error…</td>
</tr>
</tbody>
</table>
<p>太好了，我们可以看到评论成功被扩充， <strong>comments</strong> 是包含个人评论的列！现在我们已经完成了 Pandas要完成的部分功能，我们可以快速切换回 <strong>Dataset</strong> 通过加载 <strong>DataFrame</strong> 在内存中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line">comments_dataset = Dataset.from_pandas(comments_df)</span><br><span class="line">comments_dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;body&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">2842</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>太好了，我们获取到了几千条的评论！</p>
<p><strong>✏️ Try it out! 看看能不能不用pandas就可以完成列的扩充； 这有点棘手； 你可能会发现 🤗 Datasets 文档的 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/about_map_batch#batch-mapping">“Batch mapping”</a> 对这个任务很有用。</strong></p>
<p><font size="5">创建一个新的<strong>comments_length</strong>列来存放每条评论的字数。</font></p>
<p>现在我们每行有一个评论，让我们创建一个新的 <strong>comments_length</strong> 列来存放每条评论的字数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">comments_dataset = comments_dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;comment_length&quot;</span>: <span class="built_in">len</span>(x[<span class="string">&quot;comments&quot;</span>].split())&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><font size="5">过滤掉简短的评论</font></p>
<p>我们可以使用这个新列来过滤掉简短的评论，其中通常包括“cc @lewtun”或“谢谢！”之类与我们的搜索引擎无关的内容。虽然无法为过滤器选择的精确数字，但大约大于15 个单词似乎是一个不错的选择：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">comments_dataset = comments_dataset.<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x[<span class="string">&quot;comment_length&quot;</span>] &gt; <span class="number">15</span>)</span><br><span class="line">comments_dataset</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;html_url&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;comments&#x27;</span>, <span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;comment_length&#x27;</span>],</span><br><span class="line">    num_rows: <span class="number">2098</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><font size="5">将问题标题、描述和评论连接到一个新的 text 列</font></p>
<p>稍微清理了我们的数据集后，让我们将问题标题、描述和评论连接到一个新的 <strong>text</strong> 列。像往常一样，我们可以编写一个简单的函数，并将其传递给 **Dataset.map()**来做到这些 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">concatenate_text</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;text&quot;</span>: examples[<span class="string">&quot;title&quot;</span>]</span><br><span class="line">        + <span class="string">&quot; \n &quot;</span></span><br><span class="line">        + examples[<span class="string">&quot;body&quot;</span>]</span><br><span class="line">        + <span class="string">&quot; \n &quot;</span></span><br><span class="line">        + examples[<span class="string">&quot;comments&quot;</span>]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">comments_dataset = comments_dataset.<span class="built_in">map</span>(concatenate_text)</span><br></pre></td></tr></table></figure>
<p>我们终于准备好创建一些嵌入了！让我们来看看。</p>
<h3 id="3创建文本嵌入"><a class="markdownIt-Anchor" href="#3创建文本嵌入"></a> 3.创建文本嵌入</h3>
<p>我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2">第二章</a>学过，我们可以通过使用 <strong>AutoModel</strong> 类来完成词嵌入。我们需要做的就是选择一个合适的检查点来加载模型。幸运的是，有一个名为 <strong>sentence-transformers</strong> 专门用于创建词嵌入。如库中<a target="_blank" rel="noopener" href="https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search">文档</a>所述的，我们这次要实现的是非对称语义搜索，因为我们有一个简短的查询，我们希望在比如问题评论等更长的文档中找到其答案。通过查看<a target="_blank" rel="noopener" href="https://www.sbert.net/docs/pretrained_models.html#model-overview">模型概述表</a>我们可以发现 <strong>multi-qa-mpnet-base-dot-v1</strong> 检查点在语义搜索方面具有最佳性能，因此我们将在我们的应用程序中使用它。我们还将使用相同的检查点加载标记器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"></span><br><span class="line">model_ckpt = <span class="string">&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_ckpt)</span><br><span class="line">model = AutoModel.from_pretrained(model_ckpt)</span><br></pre></td></tr></table></figure>
<p>为了加快嵌入过程，将模型和输入放在 GPU 设备上，所以现在让我们这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>正如我们之前提到的，<font color="red">我们希望将 GitHub 问题语料库中的每个条目表示为单个向量，因此我们需要以某种方式“池化”或平均化我们的标记嵌入。一种流行的方法是在我们模型的输出上执行CLS 池化，我们只获取[CLS]令牌的最后一个隐藏状态。</font>以下函数为我们提供了这样的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cls_pooling</span>(<span class="params">model_output</span>):</span><br><span class="line">    <span class="keyword">return</span> model_output.last_hidden_state[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>接下来，我们将创建一个辅助函数，该函数将标记文档列表，将tensor放在 GPU 上，然后提供给模型，最后对输出使用CLS 池化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_embeddings</span>(<span class="params">text_list</span>):</span><br><span class="line">    encoded_input = tokenizer(</span><br><span class="line">        text_list, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span></span><br><span class="line">    )</span><br><span class="line">    encoded_input = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> encoded_input.items()&#125;</span><br><span class="line">    model_output = model(**encoded_input)</span><br><span class="line">    <span class="keyword">return</span> cls_pooling(model_output)</span><br></pre></td></tr></table></figure>
<p>我们可以通过在我们的语料库中输入第一个文本条目并检查输出维度来测试该函数是否有效：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embedding = get_embeddings(comments_dataset[<span class="string">&quot;text&quot;</span>][<span class="number">0</span>])</span><br><span class="line">embedding.shape</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">768</span>])</span><br></pre></td></tr></table></figure>
<p>太好了，我们已经将语料库中的第一个条目转换为 768 维向量！我们可以用 <strong>Dataset.map()</strong> 应用我们的 <strong>get_embeddings()</strong> 函数到我们语料库中的每一行，所以让我们创建一个新的 <strong>embeddings</strong> 列如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">embeddings_dataset = comments_dataset.<span class="built_in">map</span>(</span><br><span class="line">    <span class="keyword">lambda</span> x: &#123;<span class="string">&quot;embeddings&quot;</span>: get_embeddings(x[<span class="string">&quot;text&quot;</span>]).detach().cpu().numpy()[<span class="number">0</span>]&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>请注意，我们已经将嵌入转换为 NumPy 数组——这是因为当我们尝试使用 FAISS 索引它们时，🤗 Datasets需要这种格式，我们接下来会这样做。</p>
<h3 id="4使用-faiss-进行高效的相似性搜索"><a class="markdownIt-Anchor" href="#4使用-faiss-进行高效的相似性搜索"></a> 4.使用 FAISS 进行高效的相似性搜索</h3>
<p><font size="5">指定我们要索引的数据集的哪一列</font></p>
<p>现在我们有了一个词嵌入数据集，我们需要一些方法来搜索它们。为此，我们将在 🤗 Datasets中使用一种特殊的数据结构，称为 FAISS指数.<a target="_blank" rel="noopener" href="https://faiss.ai/">FAISS</a> (short for Facebook AI Similarity Search) （Facebook AI Similarity Search 的缩写）是一个提供高效算法来快速搜索和聚类嵌入向量的库。<font color="red">FAISS 背后的基本思想是创建一个特殊的数据结构，称为指数。</font>这允许人们找到哪些嵌入词与输入的词嵌入相似。在 🤗 Datasets中创建一个 FAISS 索引很简单——我们使用 <strong>Dataset.add_faiss_index()</strong> 函数并指定我们要索引的数据集的哪一列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">embeddings_dataset.add_faiss_index(column=<span class="string">&quot;embeddings&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">问题embedding</font></p>
<p>现在，我们可以使用**Dataset.get_nearest_examples()**函数进行最近邻居查找。让我们通过首先嵌入一个问题来测试这一点，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">question = <span class="string">&quot;How can I load a dataset offline?&quot;</span></span><br><span class="line">question_embedding = get_embeddings([question]).cpu().detach().numpy()</span><br><span class="line">question_embedding.shape</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">768</span>])</span><br></pre></td></tr></table></figure>
<p><font size="5">找到最相似的嵌入</font></p>
<p>就像文档一样，我们现在有一个 768 维向量表示查询，我们可以将其与整个语料库进行比较以找到最相似的嵌入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores, samples = embeddings_dataset.get_nearest_examples(</span><br><span class="line">    <span class="string">&quot;embeddings&quot;</span>, question_embedding, k=<span class="number">5</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>Dataset.get_nearest_examples()</strong> 函数返回一个分数元组，对查询和文档之间的相似度进行排序，以及一组最佳匹配的结果（这里是 5 个）。</p>
<p><font size="5">收集到pandas.DataFrame</font></p>
<p>让我们把这些收集到一个 <strong>pandas.DataFrame</strong> 以便我们可以轻松地对它们进行排序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">samples_df = pd.DataFrame.from_dict(samples)</span><br><span class="line">samples_df[<span class="string">&quot;scores&quot;</span>] = scores</span><br><span class="line">samples_df.sort_values(<span class="string">&quot;scores&quot;</span>, ascending=<span class="literal">False</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">查看查询与评论的匹配程度</font></p>
<p>现在我们可以遍历前几行来查看我们的查询与评论的匹配程度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _, row <span class="keyword">in</span> samples_df.iterrows():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;COMMENT: <span class="subst">&#123;row.comments&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;SCORE: <span class="subst">&#123;row.scores&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;TITLE: <span class="subst">&#123;row.title&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;URL: <span class="subst">&#123;row.html_url&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it&#x27;d be great if offline mode is added similar to how `transformers` loads models offline fine.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?</span></span><br><span class="line"><span class="string">SCORE: 25.505046844482422</span></span><br><span class="line"><span class="string">TITLE: Discussion using datasets in offline mode</span></span><br><span class="line"><span class="string">URL: https://github.com/huggingface/datasets/issues/824</span></span><br><span class="line"><span class="string">==================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)</span></span><br><span class="line"><span class="string">You can now use them offline</span></span><br><span class="line"><span class="string">\`\`\`python</span></span><br><span class="line"><span class="string">datasets = load_dataset(&quot;text&quot;, data_files=data_files)</span></span><br><span class="line"><span class="string">\`\`\`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">We&#x27;ll do a new release soon</span></span><br><span class="line"><span class="string">SCORE: 24.555509567260742</span></span><br><span class="line"><span class="string">TITLE: Discussion using datasets in offline mode</span></span><br><span class="line"><span class="string">URL: https://github.com/huggingface/datasets/issues/824</span></span><br><span class="line"><span class="string">==================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there&#x27;s no internet.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Let me know if you know other ways that can make the offline mode experience better. I&#x27;d be happy to add them :)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">I already note the &quot;freeze&quot; modules option, to prevent local modules updates. It would be a cool feature.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&gt; @mandubian&#x27;s second bullet point suggests that there&#x27;s a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.</span></span><br><span class="line"><span class="string">For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do</span></span><br><span class="line"><span class="string">\`\`\`python</span></span><br><span class="line"><span class="string">load_dataset(&quot;./my_dataset&quot;)</span></span><br><span class="line"><span class="string">\`\`\`</span></span><br><span class="line"><span class="string">and the dataset script will generate your dataset once and for all.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">----------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">About I&#x27;m looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.</span></span><br><span class="line"><span class="string">cf #1724</span></span><br><span class="line"><span class="string">SCORE: 24.14896583557129</span></span><br><span class="line"><span class="string">TITLE: Discussion using datasets in offline mode</span></span><br><span class="line"><span class="string">URL: https://github.com/huggingface/datasets/issues/824</span></span><br><span class="line"><span class="string">==================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">COMMENT: &gt; here is my way to load a dataset offline, but it **requires** an online machine</span></span><br><span class="line"><span class="string">&gt;</span></span><br><span class="line"><span class="string">&gt; 1. (online machine)</span></span><br><span class="line"><span class="string">&gt;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>import datasets</p>
<p>data = datasets.load_dataset(…)</p>
<p>data.save_to_disk(/YOUR/DATASET/DIR)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. copy the dir from online to the offline machine</span><br><span class="line"></span><br><span class="line">3. (offline machine)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>import datasets</p>
<p>data = datasets.load_from_disk(/SAVED/DATA/DIR)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">HTH.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SCORE: 22.893993377685547</span><br><span class="line">TITLE: Discussion using datasets in offline mode</span><br><span class="line">URL: https://github.com/huggingface/datasets/issues/824</span><br><span class="line">==================================================</span><br><span class="line"></span><br><span class="line">COMMENT: here is my way to load a dataset offline, but it **requires** an online machine</span><br><span class="line">1. (online machine)</span><br><span class="line">\`\`\`</span><br><span class="line">import datasets</span><br><span class="line">data = datasets.load_dataset(...)</span><br><span class="line">data.save_to_disk(/YOUR/DATASET/DIR)</span><br><span class="line">\`\`\`</span><br><span class="line">2. copy the dir from online to the offline machine</span><br><span class="line">3. (offline machine)</span><br><span class="line">\`\`\`</span><br><span class="line">import datasets</span><br><span class="line">data = datasets.load_from_disk(/SAVED/DATA/DIR)</span><br><span class="line">\`\`\`</span><br><span class="line"></span><br><span class="line">HTH.</span><br><span class="line">SCORE: 22.406635284423828</span><br><span class="line">TITLE: Discussion using datasets in offline mode</span><br><span class="line">URL: https://github.com/huggingface/datasets/issues/824</span><br><span class="line">==================================================</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>我们的第二个搜索结果似乎与查询相符。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">HUI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/09/20/huggingface_course/NLP_Course(5)/">http://example.com/2024/09/20/huggingface_course/NLP_Course(5)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">HUI</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/2021060111433875004.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/20/huggingface_course/NLP_Course(6.1)/" title="NLP课程（六-上）- Tokenizer库"><img class="cover" src="/img/2021060111433875004.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NLP课程（六-上）- Tokenizer库</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/20/huggingface_course/NLP_Course(3)/" title="NLP课程（三）- 微调预训练模型"><img class="cover" src="/img/2021060111433875004.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP课程（三）- 微调预训练模型</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/87788970_p0_master1200.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HUI</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">40</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp%E8%AF%BE%E7%A8%8B%E4%BA%94-datasets%E5%BA%93"><span class="toc-text"> NLP课程（五）- Datasets库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8D%E5%9C%A8-hub"><span class="toc-text"> 数据集不在 Hub</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 1.加载本地数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%8A%A0%E8%BD%BD%E8%BF%9C%E7%A8%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 2.加载远程数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%87%E7%89%87%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-text"> 切片（数据清洗）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E5%88%87%E7%89%87%E4%B8%8E%E5%88%87%E5%88%86%E6%95%B0%E6%8D%AE"><span class="toc-text"> 1.切片与切分数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%88%9B%E5%BB%BA%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%97"><span class="toc-text"> 2.创建新的数据列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%8A%A0%E9%80%9Fmap%E6%96%B9%E6%B3%95"><span class="toc-text"> 3.加速map方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4datasets%E5%92%8Cdataframes%E7%9A%84%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-text"> 4.Datasets和DataFrames的相互转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E5%88%9B%E5%BB%BA%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-text"> 5.创建验证集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 6.保存数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#datasets%E7%94%A8%E4%BA%8E%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-text"> Datasets用于大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1pile"><span class="toc-text"> 1.Pile</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84"><span class="toc-text"> 2.内存映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E6%B5%81%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 3.流数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 创建自己的数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 1.获取数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E6%B8%85%E6%B4%97%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 2.清洗数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E6%89%A9%E5%85%85%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 3.扩充数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E4%B8%8A%E4%BC%A0%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 4.上传数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8D%A1%E7%89%87"><span class="toc-text"> 5.创建数据集卡片</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-faiss-%E8%BF%9B%E8%A1%8C%E8%AF%AD%E4%B9%89%E6%90%9C%E7%B4%A2"><span class="toc-text"> 使用 FAISS 进行语义搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E4%BD%BF%E7%94%A8embedding%E8%BF%9B%E8%A1%8C%E8%AF%AD%E4%B9%89%E6%90%9C%E7%B4%A2"><span class="toc-text"> 1.使用embedding进行语义搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> 2.加载和准备数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%88%9B%E5%BB%BA%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5"><span class="toc-text"> 3.创建文本嵌入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E4%BD%BF%E7%94%A8-faiss-%E8%BF%9B%E8%A1%8C%E9%AB%98%E6%95%88%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%90%9C%E7%B4%A2"><span class="toc-text"> 4.使用 FAISS 进行高效的相似性搜索</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/10/15/paper/SAM_introduce/" title="语义分割与SAM精读"><img src="/img/204405684.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="语义分割与SAM精读"/></a><div class="content"><a class="title" href="/2024/10/15/paper/SAM_introduce/" title="语义分割与SAM精读">语义分割与SAM精读</a><time datetime="2024-10-15T03:51:38.000Z" title="发表于 2024-10-15 11:51:38">2024-10-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/12/paper/U-Net/" title="U-Net精读"><img src="/img/pob5vjumok.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="U-Net精读"/></a><div class="content"><a class="title" href="/2024/10/12/paper/U-Net/" title="U-Net精读">U-Net精读</a><time datetime="2024-10-12T06:05:38.000Z" title="发表于 2024-10-12 14:05:38">2024-10-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/exercitation/bagu_CV(1)/" title="八股文-计算机视觉(一)">八股文-计算机视觉(一)</a><time datetime="2024-10-10T12:55:33.000Z" title="发表于 2024-10-10 20:55:33">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/exercitation/bagu_DeepLearning(1)/" title="八股文-深度学习(一)">八股文-深度学习(一)</a><time datetime="2024-10-10T12:55:33.000Z" title="发表于 2024-10-10 20:55:33">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/exercitation/bagu_LLM(1)/" title="八股文-大模型(一)">八股文-大模型(一)</a><time datetime="2024-10-10T12:55:33.000Z" title="发表于 2024-10-10 20:55:33">2024-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/2021060111433875004.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By HUI</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2024/09/20/huggingface_course/NLP_Course(5)/'
    this.page.identifier = '/2024/09/20/huggingface_course/NLP_Course(5)/'
    this.page.title = 'NLP课程（五）- Datasets库'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>