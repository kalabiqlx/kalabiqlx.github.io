<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLP课程（七/一）- Token分类 | HUI</title><meta name="author" content="HUI"><meta name="copyright" content="HUI"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（7.1）- Token分类 我们将探索的第一个应用程序是令牌分类。这个通用任务包含任何可以表述为“为句子中的每个标记分配标签”的问题，例如：  命名实体识别（NER） ：查找句子中的实体（例如人、位置或组织）。这可以表述为通过">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP课程（七&#x2F;一）- Token分类">
<meta property="og:url" content="http://example.com/2024/09/20/huggingface_course/NLP_Course(7.1)/index.html">
<meta property="og:site_name" content="HUI">
<meta property="og:description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（7.1）- Token分类 我们将探索的第一个应用程序是令牌分类。这个通用任务包含任何可以表述为“为句子中的每个标记分配标签”的问题，例如：  命名实体识别（NER） ：查找句子中的实体（例如人、位置或组织）。这可以表述为通过">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/2021060111433875004.jpg">
<meta property="article:published_time" content="2024-09-20T14:40:33.000Z">
<meta property="article:modified_time" content="2024-10-08T14:26:01.343Z">
<meta property="article:author" content="HUI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/2021060111433875004.jpg"><link rel="shortcut icon" href="/img/122061154_p0_master1200.jpg"><link rel="canonical" href="http://example.com/2024/09/20/huggingface_course/NLP_Course(7.1)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP课程（七/一）- Token分类',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-08 22:26:01'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/bronya.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/87788970_p0_master1200.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/2021060111433875004.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="HUI"><img class="site-icon" src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"/><span class="site-name">HUI</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP课程（七/一）- Token分类</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-20T14:40:33.000Z" title="发表于 2024-09-20 22:40:33">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-08T14:26:01.343Z" title="更新于 2024-10-08 22:26:01">2024-10-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/">HuggingFace</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP课程（七/一）- Token分类"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/09/20/huggingface_course/NLP_Course(7.1)/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/09/20/huggingface_course/NLP_Course(7.1)/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>转载自：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/">https://huggingface.co/learn/nlp-course/zh-CN/</a></p>
<p><strong>原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。</strong></p>
<h1 id="nlp课程71-token分类"><a class="markdownIt-Anchor" href="#nlp课程71-token分类"></a> NLP课程（7.1）- Token分类</h1>
<p>我们将探索的第一个应用程序是令牌分类。这个通用任务包含任何可以表述为“为句子中的每个标记分配标签”的问题，例如：</p>
<ul>
<li><strong>命名实体识别（NER）</strong> ：查找句子中的实体（例如人、位置或组织）。这可以表述为通过每个实体一个类和“无实体”一个类来为每个标记分配一个标签。</li>
<li><strong>词性标注（POS）</strong> ：将句子中的每个单词标记为对应于特定的词性（例如名词、动词、形容词等）。</li>
<li><strong>分块</strong>：查找属于同一实体的标记。此任务（可以与 POS 或 NER 组合）可以表述为将一个标签（通常为<code>B-</code> ）分配给位于块开头的任何标记，将另一个标签（通常为<code>I-</code> ）分配给块内的标记，第三个标签（通常是<code>O</code> ）表示不属于任何块的标记。</li>
</ul>
<p>在本节中，我们将在 NER 任务上微调模型 (BERT)</p>
<h2 id="数据准备"><a class="markdownIt-Anchor" href="#数据准备"></a> 数据准备</h2>
<p>首先，我们需要一个适合标记分类的数据集。在本节中，我们将使用<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/conll2003">CoNLL-2003 数据集</a>，其中包含来自路透社的新闻报道。</p>
<blockquote>
<p><strong>只要您的数据集由分成单词及其相应标签的文本组成，您就可以将此处描述的数据处理过程调整为您自己的数据集。如果您需要回顾一下如何在<code>Dataset</code>集中加载您自己的自定义数据，请参阅<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter5">第 5 章</a>。</strong></p>
</blockquote>
<h3 id="the-conll-2003-dataset-conll-2003-数据集"><a class="markdownIt-Anchor" href="#the-conll-2003-dataset-conll-2003-数据集"></a> The CoNLL-2003 dataset CoNLL-2003 数据集</h3>
<p>为了加载 CoNLL-2003 数据集，我们使用 🤗 Datasets 库中的<code>load_dataset()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;conll2003&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将下载并缓存数据集，就像我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第 3 章</a>中看到的 GLUE MRPC 数据集一样。检查这个对象向我们展示了存在的列以及训练集、验证集和测试集之间的划分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">raw_datasets</span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">14041</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">3250</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;chunk_tags&#x27;</span>, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;ner_tags&#x27;</span>, <span class="string">&#x27;pos_tags&#x27;</span>, <span class="string">&#x27;tokens&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">3453</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>特别是，我们可以看到数据集包含我们之前提到的三个任务的标签：<strong>NER</strong>、<strong>POS</strong> 和<strong>分块</strong>。与其他数据集的一个很大的区别是，输入文本不是以句子或文档的形式呈现，而是以单词列表的形式呈现（最后一列称为<code>tokens</code> ，但它包含单词，因为这些是预标记化的输入，仍然需要进行处理）通过分词器进行子词分词）。</p>
<p>让我们看一下训练集的第一个元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;tokens&quot;</span>]</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;EU&#x27;</span>, <span class="string">&#x27;rejects&#x27;</span>, <span class="string">&#x27;German&#x27;</span>, <span class="string">&#x27;call&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;boycott&#x27;</span>, <span class="string">&#x27;British&#x27;</span>, <span class="string">&#x27;lamb&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>由于我们想要执行命名实体识别，因此我们将查看 NER 标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;ner_tags&quot;</span>]</span><br><span class="line"></span><br><span class="line">[<span class="number">3</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>这些标签是准备训练的整数，但当我们想要检查数据时它们不一定有用。与文本分类一样，我们可以通过查看数据集的<code>features</code>属性来访问这些整数和标签名称之间的对应关系：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ner_feature = raw_datasets[<span class="string">&quot;train&quot;</span>].features[<span class="string">&quot;ner_tags&quot;</span>]</span><br><span class="line"></span><br><span class="line">ner_feature</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Sequence</span>(feature=ClassLabel(num_classes=<span class="number">9</span>, names=[<span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-PER&#x27;</span>, <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;B-ORG&#x27;</span>, <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;B-LOC&#x27;</span>, <span class="string">&#x27;I-LOC&#x27;</span>, <span class="string">&#x27;B-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>], names_file=<span class="literal">None</span>, <span class="built_in">id</span>=<span class="literal">None</span>), length=-<span class="number">1</span>, <span class="built_in">id</span>=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>因此，该列包含属于<code>ClassLabel</code>序列的元素。序列元素的类型位于<code>ner_feature</code>的<code>feature</code>属性中，我们可以通过查看该<code>feature</code>的<code>names</code>属性来访问名称列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">label_names = ner_feature.feature.names</span><br><span class="line">label_names</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-PER&#x27;</span>, <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;B-ORG&#x27;</span>, <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;B-LOC&#x27;</span>, <span class="string">&#x27;I-LOC&#x27;</span>, <span class="string">&#x27;B-MISC&#x27;</span>, <span class="string">&#x27;I-MISC&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/3">第 6 章</a>深入<code>token-classification</code>管道时已经看到了这些标签，但为了快速回顾一下：</p>
<ul>
<li><code>O</code>表示该词不对应于任何实体。</li>
<li><code>B-PER</code> / <code>I-PER</code>表示该词对应于<strong>人实体的开头/位于人实体内部</strong>。</li>
<li><code>B-ORG</code> / <code>I-ORG</code>表示该词对应于<strong>组织实体的开头/位于组织实体内部</strong>。</li>
<li><code>B-LOC</code> / <code>I-LOC</code>表示该单词对应于<strong>位置实体的开头/位于位置实体内</strong>。</li>
<li><code>B-MISC</code> / <code>I-MISC</code>表示该词对应于<strong>杂项实体的开头/位于杂项实体内部</strong>。</li>
</ul>
<p>现在解码我们之前看到的标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">words = raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;tokens&quot;</span>]</span><br><span class="line">labels = raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;ner_tags&quot;</span>]</span><br><span class="line">line1 = <span class="string">&quot;&quot;</span></span><br><span class="line">line2 = <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="keyword">for</span> word, label <span class="keyword">in</span> <span class="built_in">zip</span>(words, labels):</span><br><span class="line">    full_label = label_names[label]</span><br><span class="line">    max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(word), <span class="built_in">len</span>(full_label))</span><br><span class="line">    line1 += word + <span class="string">&quot; &quot;</span> * (max_length - <span class="built_in">len</span>(word) + <span class="number">1</span>) <span class="comment"># 用于输出的表示每个词能对齐</span></span><br><span class="line">    line2 += full_label + <span class="string">&quot; &quot;</span> * (max_length - <span class="built_in">len</span>(full_label) + <span class="number">1</span>) <span class="comment"># 用于输出的表示每个词能对齐</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(line1)</span><br><span class="line"><span class="built_in">print</span>(line2)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;EU    rejects German call to boycott British lamb .&#x27;</span></span><br><span class="line"><span class="string">&#x27;B-ORG O       B-MISC O    O  O       B-MISC  O    O&#x27;</span></span><br></pre></td></tr></table></figure>
<p>对于混合<code>B-</code>和<code>I-</code>标签的示例，以下是相同代码在索引 4 处的训练集元素上给出的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;Germany \&#x27;s representative to the European Union \&#x27;s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .&#x27;</span></span><br><span class="line"><span class="string">&#x27;B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O&#x27;</span></span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，跨越两个单词的实体，例如“European Union”和“Werner Zwingmann”，第一个单词被赋予<code>B-</code>标签，第二个单词被赋予<code>I-</code>标签。</p>
<h3 id="处理数据"><a class="markdownIt-Anchor" href="#处理数据"></a> 处理数据</h3>
<p>与往常一样，我们的文本需要先转换为token ID，然后模型才能理解它们。正如我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/">第 6 章</a>中看到的，token分类任务的一个很大的区别是我们有Pre-Tokenizer的输入。幸运的是，分词器 API 可以很轻松地处理这个问题；我们只需要用一个特殊的标志来警告<code>tokenizer</code> 。</p>
<p>首先，让我们创建我们的<code>tokenizer</code>对象。正如我们之前所说，我们将使用 BERT 预训练模型，因此我们首先下载并缓存关联的分词器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_checkpoint = <span class="string">&quot;bert-base-cased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>您可以将<code>model_checkpoint</code>替换为<a target="_blank" rel="noopener" href="https://huggingface.co/models">Hub</a>中您喜欢的任何其他模型，或者替换为您保存预训练模型和分词器的本地文件夹。唯一的限制是 tokenizer 需要由 🤗 Tokenizers 库支持，因此有一个“快速”版本可用。您可以在<a target="_blank" rel="noopener" href="https://huggingface.co/transformers/#supported-frameworks">这个大表</a>中看到带有快速版本的所有架构，并检查您正在使用的<code>tokenizer</code>对象是否确实由 🤗 Tokenizers 支持，您可以查看其<code>is_fast</code>属性：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.is_fast</span><br><span class="line"></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>要对预标记化的输入进行标记，我们可以像往常一样使用<code>tokenizer</code> ，只需添加<code>is_split_into_words=True</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;tokens&quot;</span>], is_split_into_words=<span class="literal">True</span>)</span><br><span class="line">inputs.tokens()</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;EU&#x27;</span>, <span class="string">&#x27;rejects&#x27;</span>, <span class="string">&#x27;German&#x27;</span>, <span class="string">&#x27;call&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;boycott&#x27;</span>, <span class="string">&#x27;British&#x27;</span>, <span class="string">&#x27;la&#x27;</span>, <span class="string">&#x27;##mb&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>is_split_into_words</strong> ( <code>bool</code> ,<em>可选</em>, 默认为<code>False</code> ) — 输入是否已预先标记化（例如，拆分为单词）。如果设置为<code>True</code> ，分词器假定输入已拆分为单词（例如，通过在空格上拆分），并将对其进行分词。这对于 NER 或 token 分类很有用。</p>
<p>若删除<code>is_split_into_words=True</code>，输出为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;EU&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>正如我们所看到的，标记生成器添加了模型使用的特殊标记（开头的<code>[CLS]</code>和结尾的<code>[SEP]</code> ），并且大部分单词保持不变。然而， <code>lamb</code>一词被标记为两个子词<code>la</code>和<code>##mb</code> 。这导致我们的输入和标签之间不匹配：标签列表只有 9 个元素，而我们的输入现在有 12 个标记。计算特殊标记很容易（我们知道它们位于开头和结尾），但我们还需要确保将所有标签与正确的单词对齐。</p>
<p>幸运的是，因为我们使用的是快速分词器，所以我们可以使用 🤗 分词器的超能力，这意味着我们可以轻松地将每个分词映射到其相应的单词（如<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/3">第 6 章</a>所示）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs.word_ids()</span><br><span class="line"></span><br><span class="line">[<span class="literal">None</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="literal">None</span>]</span><br></pre></td></tr></table></figure>
<p><font color="red">通过一点点工作，我们就可以扩展标签列表以匹配token。我们要应用的第一条规则是特殊标记的标签为<code>-100</code> 。这是因为默认情况下<code>-100</code>是一个在我们将使用的损失函数（交叉熵）中被忽略的索引。然后，每个token都会获得与其内部单词开头的token相同的标签，因为它们是同一实体的一部分。对于单词内部但不在开头的标记，我们将<code>B-</code>替换为<code>I-</code> （因为标记不是实体的开头）：</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">align_labels_with_tokens</span>(<span class="params">labels, word_ids</span>):</span><br><span class="line">    new_labels = []</span><br><span class="line">    current_word = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> word_id <span class="keyword">in</span> word_ids:</span><br><span class="line">        <span class="keyword">if</span> word_id != current_word:</span><br><span class="line">            <span class="comment"># Start of a new word!</span></span><br><span class="line">            current_word = word_id</span><br><span class="line">            label = -<span class="number">100</span> <span class="keyword">if</span> word_id <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> labels[word_id]</span><br><span class="line">            new_labels.append(label)</span><br><span class="line">        <span class="keyword">elif</span> word_id <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Special token</span></span><br><span class="line">            new_labels.append(-<span class="number">100</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># Same word as previous token</span></span><br><span class="line">            label = labels[word_id]</span><br><span class="line">            <span class="comment"># If the label is B-XXX we change it to I-XXX,对于单词内部但不在开头的标记，我们将`B-`替换为`I-</span></span><br><span class="line">            <span class="keyword">if</span> label % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">                label += <span class="number">1</span></span><br><span class="line">            new_labels.append(label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_labels</span><br></pre></td></tr></table></figure>
<p>让我们尝试一下我们的第一句话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">labels = raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;ner_tags&quot;</span>]</span><br><span class="line">word_ids = inputs.word_ids()</span><br><span class="line"><span class="built_in">print</span>(labels)</span><br><span class="line"><span class="built_in">print</span>(align_labels_with_tokens(labels, word_ids))</span><br><span class="line"></span><br><span class="line">[<span class="number">3</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">[-<span class="number">100</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, -<span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，我们的函数为开头和结尾的两个特殊标记添加了<code>-100</code> ，并为被拆分为两个标记的单词添加了新的<code>0</code> 。</p>
<blockquote>
<p>✏️**轮到你了！**一些研究人员更喜欢为每个单词只分配一个标签，并将<code>-100</code>分配给给定单词中的其他子标记。这是为了避免长单词分裂成大量子标记，从而严重造成损失。更改之前的函数，按照此规则将标签与输入 ID 对齐。</p>
</blockquote>
<p>为了预处理整个数据集，我们需要对所有输入进行标记，并对所有标签应用<code>align_labels_with_tokens()</code> 。为了利用快速分词器的速度，最好同时对大量文本进行分词，因此我们将编写一个处理示例列表的函数，并使用<code>Dataset.map()</code>方法和选项<code>batched=True</code> 。与我们之前的示例唯一不同的是，当分词器的输入是文本列表（或者在我们的例子中是列表列表）时， <code>word_ids()</code>函数需要获取我们想要的单词 ID 的示例的索引的单词），所以我们也添加：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_and_align_labels</span>(<span class="params">examples</span>):</span><br><span class="line">    tokenized_inputs = tokenizer(</span><br><span class="line">        examples[<span class="string">&quot;tokens&quot;</span>], truncation=<span class="literal">True</span>, is_split_into_words=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    all_labels = examples[<span class="string">&quot;ner_tags&quot;</span>]</span><br><span class="line">    new_labels = []</span><br><span class="line">    <span class="keyword">for</span> i, labels <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_labels): <span class="comment"># 这里i是样本索引 ，labels是对应样本的&quot;ner_tags&quot;</span></span><br><span class="line">        word_ids = tokenized_inputs.word_ids(i)</span><br><span class="line">        new_labels.append(align_labels_with_tokens(labels, word_ids))</span><br><span class="line"></span><br><span class="line">    tokenized_inputs[<span class="string">&quot;labels&quot;</span>] = new_labels</span><br><span class="line">    <span class="keyword">return</span> tokenized_inputs</span><br></pre></td></tr></table></figure>
<p>请注意，我们还没有填充我们的输入；我们稍后会在使用数据整理器创建批次时执行此操作。</p>
<p>现在，我们可以将所有预处理一次性应用于数据集的其他部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(</span><br><span class="line">    tokenize_and_align_labels,</span><br><span class="line">    batched=<span class="literal">True</span>,</span><br><span class="line">    remove_columns=raw_datasets[<span class="string">&quot;train&quot;</span>].column_names,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>我们已经完成了最困难的部分！现在数据已经经过预处理，实际的训练将与我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第 3 章</a>中所做的非常相似。</p>
<h2 id="使用-trainer-api-微调模型"><a class="markdownIt-Anchor" href="#使用-trainer-api-微调模型"></a> 使用 Trainer API 微调模型</h2>
<p>使用<code>Trainer</code>实际代码将与以前相同；唯一的变化是数据整理成批的方式和度量计算函数。</p>
<h3 id="数据整理"><a class="markdownIt-Anchor" href="#数据整理"></a> 数据整理</h3>
<p>我们不能像<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第 3 章</a>那样只使用<code>DataCollatorWithPadding</code> ，因为它只会填充输入（输入 ID、注意掩码和令牌类型 ID）。这里我们的标签应该以与输入完全相同的方式填充，以便它们保持相同的大小，使用<code>-100</code>作为值，以便在损失计算中忽略相应的预测。</p>
<p>这一切都是由一个 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification"><code>DataCollatorForTokenClassification</code></a>完成.与<code>DataCollatorWithPadding</code>一样，它使用用于预处理输入的<code>tokenizer</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorForTokenClassification</span><br><span class="line"></span><br><span class="line">data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>
<p>为了在几个样本上测试这一点，我们可以在训练集中的示例列表上调用它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch = data_collator([tokenized_datasets[<span class="string">&quot;train&quot;</span>][i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">batch[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">tensor([[-<span class="number">100</span>,    <span class="number">3</span>,    <span class="number">0</span>,    <span class="number">7</span>,    <span class="number">0</span>,    <span class="number">0</span>,    <span class="number">0</span>,    <span class="number">7</span>,    <span class="number">0</span>,    <span class="number">0</span>,    <span class="number">0</span>, -<span class="number">100</span>],</span><br><span class="line">        [-<span class="number">100</span>,    <span class="number">1</span>,    <span class="number">2</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>]])</span><br></pre></td></tr></table></figure>
<p>让我们将其与数据集中第一个和第二个元素的标签进行比较：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokenized_datasets[<span class="string">&quot;train&quot;</span>][i][<span class="string">&quot;labels&quot;</span>])</span><br><span class="line">[-<span class="number">100</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, -<span class="number">100</span>]</span><br><span class="line">[-<span class="number">100</span>, <span class="number">1</span>, <span class="number">2</span>, -<span class="number">100</span>]</span><br></pre></td></tr></table></figure>
<p><font color="red">正如我们所看到的，第二组标签的长度已经使用 <code>-100</code> 填充到与第一组标签相同。</font></p>
<h3 id="评估指标"><a class="markdownIt-Anchor" href="#评估指标"></a> 评估指标</h3>
<p>为了让 <code>Trainer</code> 在每个epoch计算一个度量，我们需要定义一个 <code>compute_metrics()</code> 函数，该函数接受预测和标签数组，并返回一个包含度量名称和值的字典</p>
<p>用于评估Token分类预测的传统框架是 <a target="_blank" rel="noopener" href="https://github.com/chakki-works/seqeval"><em>seqeval</em></a>. 要使用此指标，我们首先需要安装seqeval库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install seqeval</span><br></pre></td></tr></table></figure>
<p>然后我们可以通过加载它 <code>evaluate.load()</code> 函数就像我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第三章</a>做的那样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;seqeval&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font color="red">这个评估方式与标准精度不同:它实际上将标签列表作为字符串，而不是整数，因此在将预测和标签传递给它之前，我们需要完全解码它们。</font>让我们看看它是如何工作的。首先，我们将获得第一个训练示例的标签:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">labels = raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">0</span>][<span class="string">&quot;ner_tags&quot;</span>]</span><br><span class="line">labels = [label_names[i] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line">labels</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;B-ORG&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-MISC&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;B-MISC&#x27;</span>, <span class="string">&#x27;O&#x27;</span>, <span class="string">&#x27;O&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>然后我们可以通过更改索引 2 处的值来为那些创建假的预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predictions = labels.copy()</span><br><span class="line">predictions[<span class="number">2</span>] = <span class="string">&quot;O&quot;</span></span><br><span class="line">metric.compute(predictions=[predictions], references=[labels])</span><br></pre></td></tr></table></figure>
<p>请注意，该指标的输入是预测列表（不仅仅是一个）和标签列表。这是输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&#x27;MISC&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">0.5</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">0.67</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">2</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;ORG&#x27;</span>: &#123;<span class="string">&#x27;precision&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;recall&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;f1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;number&#x27;</span>: <span class="number">1</span>&#125;,</span><br><span class="line"> <span class="string">&#x27;overall_precision&#x27;</span>: <span class="number">1.0</span>,</span><br><span class="line"> <span class="string">&#x27;overall_recall&#x27;</span>: <span class="number">0.67</span>,</span><br><span class="line"> <span class="string">&#x27;overall_f1&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line"> <span class="string">&#x27;overall_accuracy&#x27;</span>: <span class="number">0.89</span>&#125;</span><br></pre></td></tr></table></figure>
<p>它返回很多信息！我们获得每个单独实体以及整体的准确率、召回率和 F1 分数。对于我们的度量计算，我们将只保留总分，但可以随意调整 <code>compute_metrics()</code> 函数返回您想要查看的所有指标。</p>
<p>这<code>compute_metrics()</code> 函数首先采用 logits 的 argmax 将它们转换为预测（像往常一样，logits 和概率的顺序相同，因此我们不需要应用 softmax）。然后我们必须将标签和预测从整数转换为字符串。我们删除标签为 <code>-100</code> 所有值 ，然后将结果传递给 <code>metric.compute()</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_preds</span>):</span><br><span class="line">    logits, labels = eval_preds</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove ignored index (special tokens) and convert to labels</span></span><br><span class="line">    true_labels = [[label_names[l] <span class="keyword">for</span> l <span class="keyword">in</span> label <span class="keyword">if</span> l != -<span class="number">100</span>] <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line">    true_predictions = [</span><br><span class="line">        [label_names[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line">    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;precision&quot;</span>: all_metrics[<span class="string">&quot;overall_precision&quot;</span>],</span><br><span class="line">        <span class="string">&quot;recall&quot;</span>: all_metrics[<span class="string">&quot;overall_recall&quot;</span>],</span><br><span class="line">        <span class="string">&quot;f1&quot;</span>: all_metrics[<span class="string">&quot;overall_f1&quot;</span>],</span><br><span class="line">        <span class="string">&quot;accuracy&quot;</span>: all_metrics[<span class="string">&quot;overall_accuracy&quot;</span>],</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>现在已经完成了，我们几乎准备好定义我们的 <code>Trainer</code> .我们只需要一个 <code>model</code> 微调！</p>
<h3 id="定义模型"><a class="markdownIt-Anchor" href="#定义模型"></a> 定义模型</h3>
<p>由于我们正在研究Token分类问题，因此我们将使用 <code>AutoModelForTokenClassification</code> 类。定义这个模型时要记住的主要事情是传递一些关于我们的标签数量的信息。执行此操作的最简单方法是将该数字传递给 <code>num_labels</code> 参数，但是如果我们想要一个很好的推理小部件，就像我们在本节开头看到的那样，最好设置正确的标签对应关系。</p>
<p>它们应该由两个字典设置， <code>id2label</code> 和 <code>label2id</code> ，其中包含从 ID 到标签的映射，反之亦然：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">id2label = &#123;<span class="built_in">str</span>(i): label <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_names)&#125;</span><br><span class="line">label2id = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> id2label.items()&#125;</span><br></pre></td></tr></table></figure>
<p>现在我们可以将它们传递给 <code>AutoModelForTokenClassification.from_pretrained()</code> 方法，它们将在模型的配置中设置，然后保存并上传到Hub：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForTokenClassification</span><br><span class="line"></span><br><span class="line">model = AutoModelForTokenClassification.from_pretrained(</span><br><span class="line">    model_checkpoint,</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>就像我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第三章</a>,定义我们的 <code>AutoModelForSequenceClassification</code> ，创建模型会发出警告，提示一些权重未被使用（来自预训练头的权重）和一些其他权重被随机初始化（来自新Token分类头的权重），我们将要训练这个模型。我们将在一分钟内完成，但首先让我们仔细检查我们的模型是否具有正确数量的标签：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.config.num_labels</span><br><span class="line"><span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>⚠️ 如果模型的标签数量错误，稍后调用Trainer.train()方法时会出现一个模糊的错误（类似于“CUDA error: device-side assert triggered”）。这是用户报告此类错误的第一个原因，因此请确保进行这样的检查以确认您拥有预期数量的标签。</p>
<h3 id="微调模型"><a class="markdownIt-Anchor" href="#微调模型"></a> 微调模型</h3>
<p>我们现在准备好训练我们的模型了！在定义我们的 <code>Trainer</code>之前，我们只需要做最后两件事：登录 Hugging Face 并定义我们的训练参数。如果您在notebook上工作，有一个方便的功能可以帮助您：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> notebook_login</span><br><span class="line"></span><br><span class="line">notebook_login()</span><br></pre></td></tr></table></figure>
<p>这将显示一个小部件，您可以在其中输入您的 Hugging Face 账号和密码。如果您不是在notebook上工作，只需在终端中输入以下行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure>
<p>Once this is done, we can define our <code>TrainingArguments</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"></span><br><span class="line">args = TrainingArguments(</span><br><span class="line">    <span class="string">&quot;bert-finetuned-ner&quot;</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    save_strategy=<span class="string">&quot;epoch&quot;</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,</span><br><span class="line">    push_to_hub=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>您之前已经看过其中的大部分内容：我们设置了一些超参数（例如学习率、要训练的 epoch 数和权重衰减），然后我们指定 <code>push_to_hub=True</code> 表明我们想要保存模型并在每个时期结束时对其进行评估，并且我们想要将我们的结果上传到模型中心。请注意，可以使用hub_model_id参数指定要推送到的存储库的名称(特别是，必须使用这个参数来推送到一个组织)。例如，当我们将模型推送到<a target="_blank" rel="noopener" href="https://huggingface.co/huggingface-course"><code>huggingface-course</code> organization</a>, 我们添加了 <code>hub_model_id=huggingface-course/bert-finetuned-ner</code> 到 <code>TrainingArguments</code> 。默认情况下，使用的存储库将在您的命名空间中并以您设置的输出目录命名，因此在我们的例子中它将是 <code>sgugger/bert-finetuned-ner</code>。</p>
<p>💡 如果您正在使用的输出目录已经存在，那么输出目录必须是从同一个存储库clone下来的。如果不是，您将在声明 <code>Trainer</code> 时遇到错误，并且需要设置一个新名称。</p>
<p>最后，我们只是将所有内容传递给 <code>Trainer</code> 并启动训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    compute_metrics=compute_metrics,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">)</span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>
<p>请注意，当训练发生时，每次保存模型时（这里是每个epooch），它都会在后台上传到 Hub。这样，如有必要，您将能够在另一台机器上继续您的训练。</p>
<p>训练完成后，我们使用 <code>push_to_hub()</code> 确保我们上传模型的最新版本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.push_to_hub(commit_message=<span class="string">&quot;Training complete&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>This command returns the URL of the commit it just did, if you want to inspect it:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed&#x27;</span></span><br></pre></td></tr></table></figure>
<p>这 <code>Trainer</code> 还创建了一张包含所有评估结果的模型卡并上传。在此阶段，您可以使用模型中心上的推理小部件来测试您的模型并与您的朋友分享。您已成功在Token分类任务上微调模型 - 恭喜！</p>
<p>如果您想更深入地了解训练循环，我们现在将向您展示如何使用 🤗 Accelerate 做同样的事情。</p>
<h2 id="自定义训练循环"><a class="markdownIt-Anchor" href="#自定义训练循环"></a> 自定义训练循环</h2>
<p>现在让我们看一下完整的训练循环，这样您可以轻松定义所需的部分。它看起来很像我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3/4">第三章</a>, 所做的，对评估进行了一些更改。</p>
<h3 id="做好训练前的准备"><a class="markdownIt-Anchor" href="#做好训练前的准备"></a> 做好训练前的准备</h3>
<p>首先我们需要为我们的数据集构建 <code>DataLoader</code> 。我们将重用我们的 <code>data_collator</code> 作为一个 <code>collate_fn</code> 并打乱训练集，但不打乱验证集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    collate_fn=data_collator,</span><br><span class="line">    batch_size=<span class="number">8</span>,</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=<span class="number">8</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>接下来，我们重新实例化我们的模型，以确保我们不会从之前的训练继续训练，而是再次从 BERT 预训练模型开始：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = AutoModelForTokenClassification.from_pretrained(</span><br><span class="line">    model_checkpoint,</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>然后我们将需要一个优化器。我们将使用经典 <code>AdamW</code> ，这就像 <code>Adam</code> ，但在应用权重衰减的方式上进行了改进：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br></pre></td></tr></table></figure>
<p>Once we have all those objects, we can send them to the <code>accelerator.prepare()</code> method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line">model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(</span><br><span class="line">    model, optimizer, train_dataloader, eval_dataloader</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>🚨 如果您在 TPU 上进行训练，则需要将以上单元格中的所有代码移动到专用的训练函数中。有关详细信息，请参阅 <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第3章</a>。</p>
<p>现在我们已经发送了我们的 <code>train_dataloader</code> 到 <code>accelerator.prepare()</code> ，我们可以使用它的长度来计算训练步骤的数量。请记住，我们应该始终在准备好dataloader后执行此操作，因为该方法会改变其长度。我们使用经典线性学习率调度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"></span><br><span class="line">num_train_epochs = <span class="number">3</span></span><br><span class="line">num_update_steps_per_epoch = <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">num_training_steps = num_train_epochs * num_update_steps_per_epoch</span><br><span class="line"></span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>最后，要将我们的模型推送到 Hub，我们需要创建一个 <code>Repository</code> 工作文件夹中的对象。如果您尚未登录，请先登录 Hugging Face。我们将从我们想要为模型提供的模型 ID 中确定存储库名称（您可以自由地用自己的选择替换 <code>repo_name</code> ；它只需要包含您的用户名，可以使用<code>get_full_repo_name()</code>函数的查看目前的repo_name）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> Repository, get_full_repo_name</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;bert-finetuned-ner-accelerate&quot;</span></span><br><span class="line">repo_name = get_full_repo_name(model_name)</span><br><span class="line">repo_name</span><br><span class="line"><span class="string">&#x27;sgugger/bert-finetuned-ner-accelerate&#x27;</span></span><br></pre></td></tr></table></figure>
<p>然后我们可以将该存储库克隆到本地文件夹中。 如果它已经存在，这个本地文件夹应该是我们正在使用的存储库的现有克隆：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output_dir = <span class="string">&quot;bert-finetuned-ner-accelerate&quot;</span></span><br><span class="line">repo = Repository(output_dir, clone_from=repo_name)</span><br></pre></td></tr></table></figure>
<p>我们现在可以通过调用 <code>repo.push_to_hub()</code> 方法上传保存在 <code>output_dir</code> 中的任何内容。 这将帮助我们在每个训练周期结束时上传中间模型。</p>
<h3 id="训练循环"><a class="markdownIt-Anchor" href="#训练循环"></a> 训练循环</h3>
<p>我们现在准备编写完整的训练循环。为了简化它的评估部分，我们定义了这个 <code>postprocess()</code> 接受预测和标签并将它们转换为字符串列表的函数，也就是 <code>metric</code>对象需要的输入格式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postprocess</span>(<span class="params">predictions, labels</span>):</span><br><span class="line">    predictions = predictions.detach().cpu().clone().numpy()</span><br><span class="line">    labels = labels.detach().cpu().clone().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove ignored index (special tokens) and convert to labels</span></span><br><span class="line">    true_labels = [[label_names[l] <span class="keyword">for</span> l <span class="keyword">in</span> label <span class="keyword">if</span> l != -<span class="number">100</span>] <span class="keyword">for</span> label <span class="keyword">in</span> labels]</span><br><span class="line">    true_predictions = [</span><br><span class="line">        [label_names[p] <span class="keyword">for</span> (p, l) <span class="keyword">in</span> <span class="built_in">zip</span>(prediction, label) <span class="keyword">if</span> l != -<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">for</span> prediction, label <span class="keyword">in</span> <span class="built_in">zip</span>(predictions, labels)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> true_labels, true_predictions</span><br></pre></td></tr></table></figure>
<p>然后我们可以编写训练循环。在定义一个进度条来跟踪训练的进行后，循环分为三个部分：</p>
<ul>
<li>训练本身，这是对<code>train_dataloader</code>的经典迭代，向前传递模型，然后反向传递和优化参数</li>
<li>评估,在获得我们模型的输出后:因为两个进程可能将输入和标签填充成不同的形状,在调用<code>gather()</code>方法前我们需要使用<code>accelerator.pad_across_processes()</code>来让预测和标签形状相同。如果我们不这样做，评估要么出错，要么永远不会得到结果。然后，我们将结果发送给<code>metric.add_batch()</code>，并在计算循环结束后调用<code>metric.compute()</code>。</li>
<li>保存和上传，首先保存模型和标记器，然后调用<code>repo.push_to_hub()</code>。注意，我们使用参数<code>blocking=False</code>告诉🤗 hub 库用在异步进程中推送。这样，训练将正常继续，并且该（长）指令将在后台执行。</li>
</ul>
<p>这是训练循环的完整代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_train_epochs):</span><br><span class="line">    <span class="comment"># Training</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Evaluation</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">        predictions = outputs.logits.argmax(dim=-<span class="number">1</span>)</span><br><span class="line">        labels = batch[<span class="string">&quot;labels&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Necessary to pad predictions and labels for being gathered</span></span><br><span class="line">        predictions = accelerator.pad_across_processes(predictions, dim=<span class="number">1</span>, pad_index=-<span class="number">100</span>)</span><br><span class="line">        labels = accelerator.pad_across_processes(labels, dim=<span class="number">1</span>, pad_index=-<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">        predictions_gathered = accelerator.gather(predictions)</span><br><span class="line">        labels_gathered = accelerator.gather(labels)</span><br><span class="line"></span><br><span class="line">        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)</span><br><span class="line">        metric.add_batch(predictions=true_predictions, references=true_labels)</span><br><span class="line"></span><br><span class="line">    results = metric.compute()</span><br><span class="line">    <span class="built_in">print</span>(</span><br><span class="line">        <span class="string">f&quot;epoch <span class="subst">&#123;epoch&#125;</span>:&quot;</span>,</span><br><span class="line">        &#123;</span><br><span class="line">            key: results[<span class="string">f&quot;overall_<span class="subst">&#123;key&#125;</span>&quot;</span>]</span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> [<span class="string">&quot;precision&quot;</span>, <span class="string">&quot;recall&quot;</span>, <span class="string">&quot;f1&quot;</span>, <span class="string">&quot;accuracy&quot;</span>]</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save and upload</span></span><br><span class="line">    accelerator.wait_for_everyone()</span><br><span class="line">    unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)</span><br><span class="line">    <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">        tokenizer.save_pretrained(output_dir)</span><br><span class="line">        repo.push_to_hub(</span><br><span class="line">            commit_message=<span class="string">f&quot;Training in progress epoch <span class="subst">&#123;epoch&#125;</span>&quot;</span>, blocking=<span class="literal">False</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<p>果这是您第一次看到用 🤗 Accelerate 保存的模型，让我们花点时间检查一下它附带的三行代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">accelerator.wait_for_everyone()</span><br><span class="line">unwrapped_model = accelerator.unwrap_model(model)</span><br><span class="line">unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)</span><br></pre></td></tr></table></figure>
<p>第一行是不言自明的：它告诉所有进程等到都处于那个阶段再继续(阻塞)。这是为了确保在保存之前，我们在每个过程中都有相同的模型。然后获取<code>unwrapped_model</code>，它是我们定义的基本模型。 <code>accelerator.prepare()</code>方法将模型更改为在分布式训练中工作，所以它不再有<code>save_pretraining()</code>方法;<code>accelerator.unwrap_model()</code>方法将撤销该步骤。最后，我们调用<code>save_pretraining()</code>，但告诉该方法使用<code>accelerator.save()</code>而不是<code>torch.save()</code>。</p>
<p>当完成之后，你应该有一个模型，它产生的结果与<code>Trainer</code>的结果非常相似。你可以在<a target="_blank" rel="noopener" href="https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate">hugs face-course/bert-fine - tuning -ner-accelerate</a>中查看我们使用这个代码训练的模型。如果你想测试训练循环的任何调整，你可以直接通过编辑上面显示的代码来实现它们!</p>
<h2 id="使用微调模型"><a class="markdownIt-Anchor" href="#使用微调模型"></a> 使用微调模型</h2>
<p>我们已经向您展示了如何使用我们在模型中心微调的模型和推理小部件。在本地使用它 <code>pipeline</code> ，您只需要指定正确的模型标识符：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace this with your own checkpoint</span></span><br><span class="line">model_checkpoint = <span class="string">&quot;huggingface-course/bert-finetuned-ner&quot;</span></span><br><span class="line">token_classifier = pipeline(</span><br><span class="line">    <span class="string">&quot;token-classification&quot;</span>, model=model_checkpoint, aggregation_strategy=<span class="string">&quot;simple&quot;</span></span><br><span class="line">)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line">[&#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9988506</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Sylvain&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9647625</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hugging Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9986118</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>太棒了！我们的模型与此管道的默认模型一样有效！</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">HUI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/09/20/huggingface_course/NLP_Course(7.1)/">http://example.com/2024/09/20/huggingface_course/NLP_Course(7.1)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">HUI</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/2021060111433875004.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/21/hexo/hexo/" title="使用hexo+github搭建个人博客"><img class="cover" src="/img/favicon.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">使用hexo+github搭建个人博客</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/20/huggingface_course/NLP_Course(6.3)/" title="NLP课程（六-下）- 逐块构建分词器"><img class="cover" src="/img/2021060111433875004.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP课程（六-下）- 逐块构建分词器</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/87788970_p0_master1200.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HUI</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp%E8%AF%BE%E7%A8%8B71-token%E5%88%86%E7%B1%BB"><span class="toc-text"> NLP课程（7.1）- Token分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text"> 数据准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-conll-2003-dataset-conll-2003-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text"> The CoNLL-2003 dataset CoNLL-2003 数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-text"> 处理数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-trainer-api-%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 使用 Trainer API 微调模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%95%B4%E7%90%86"><span class="toc-text"> 数据整理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text"> 评估指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 微调模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-text"> 自定义训练循环</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%9A%E5%A5%BD%E8%AE%AD%E7%BB%83%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87"><span class="toc-text"> 做好训练前的准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-text"> 训练循环</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-text"> 使用微调模型</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/10/23/LLM/RLHF/" title="大模型系列(二)- RLHF:基于人类反馈的强化学习"><img src="/img/image-20241023152702443.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大模型系列(二)- RLHF:基于人类反馈的强化学习"/></a><div class="content"><a class="title" href="/2024/10/23/LLM/RLHF/" title="大模型系列(二)- RLHF:基于人类反馈的强化学习">大模型系列(二)- RLHF:基于人类反馈的强化学习</a><time datetime="2024-10-23T05:31:38.000Z" title="发表于 2024-10-23 13:31:38">2024-10-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/22/Multimodel/BEiTv3/" title="多模态系列(六)- BEiTv3"><img src="/img/image-20241022190456136.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多模态系列(六)- BEiTv3"/></a><div class="content"><a class="title" href="/2024/10/22/Multimodel/BEiTv3/" title="多模态系列(六)- BEiTv3">多模态系列(六)- BEiTv3</a><time datetime="2024-10-22T07:48:38.000Z" title="发表于 2024-10-22 15:48:38">2024-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/22/Multimodel/multimodel_summerize/" title="多模态系列(七)- 总结"><div style="background: /img/"></div></a><div class="content"><a class="title" href="/2024/10/22/Multimodel/multimodel_summerize/" title="多模态系列(七)- 总结">多模态系列(七)- 总结</a><time datetime="2024-10-22T07:48:38.000Z" title="发表于 2024-10-22 15:48:38">2024-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/22/Multimodel/COCA/" title="多模态系列(五)- COCA"><img src="/img/image-20241022153206496.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="多模态系列(五)- COCA"/></a><div class="content"><a class="title" href="/2024/10/22/Multimodel/COCA/" title="多模态系列(五)- COCA">多模态系列(五)- COCA</a><time datetime="2024-10-22T06:51:38.000Z" title="发表于 2024-10-22 14:51:38">2024-10-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/22/LLM/Lora/" title="大模型系列(一)- LoRA"><img src="/img/image-20241024195508119.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大模型系列(一)- LoRA"/></a><div class="content"><a class="title" href="/2024/10/22/LLM/Lora/" title="大模型系列(一)- LoRA">大模型系列(一)- LoRA</a><time datetime="2024-10-22T05:03:38.000Z" title="发表于 2024-10-22 13:03:38">2024-10-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/2021060111433875004.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By HUI</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2024/09/20/huggingface_course/NLP_Course(7.1)/'
    this.page.identifier = '/2024/09/20/huggingface_course/NLP_Course(7.1)/'
    this.page.title = 'NLP课程（七/一）- Token分类'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>