<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰ | HUI</title><meta name="author" content="HUI"><meta name="copyright" content="HUI"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="è½¬è½½è‡ªï¼šhttps:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; åŸä¸­æ–‡æ–‡æ¡£æœ‰å¾ˆå¤šåœ°æ–¹ç¿»è¯‘çš„å¤ªæ•·è¡äº†ï¼Œå› æ­¤æ‰æœ‰è¯ç³»åˆ—æ–‡ç« ã€‚  NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰  é€å—æ„å»ºåˆ†è¯å™¨ æ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢å‡ èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæ ‡è®°åŒ–åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼š  è§„èŒƒåŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰ é¢„æ ‡è®°åŒ–ï¼ˆå°†è¾“å…¥åˆ†å‰²ä¸ºå•è¯ï¼‰ é€šè¿‡æ¨¡å‹è¿è¡Œè¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ ‡è®°">
<meta property="og:type" content="article">
<meta property="og:title" content="NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰">
<meta property="og:url" content="http://example.com/2024/09/20/NLP_Course(6.3)/index.html">
<meta property="og:site_name" content="HUI">
<meta property="og:description" content="è½¬è½½è‡ªï¼šhttps:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; åŸä¸­æ–‡æ–‡æ¡£æœ‰å¾ˆå¤šåœ°æ–¹ç¿»è¯‘çš„å¤ªæ•·è¡äº†ï¼Œå› æ­¤æ‰æœ‰è¯ç³»åˆ—æ–‡ç« ã€‚  NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰  é€å—æ„å»ºåˆ†è¯å™¨ æ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢å‡ èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæ ‡è®°åŒ–åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼š  è§„èŒƒåŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰ é¢„æ ‡è®°åŒ–ï¼ˆå°†è¾“å…¥åˆ†å‰²ä¸ºå•è¯ï¼‰ é€šè¿‡æ¨¡å‹è¿è¡Œè¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ ‡è®°">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/122061154_p0_master1200.jpg">
<meta property="article:published_time" content="2024-09-20T14:39:33.000Z">
<meta property="article:modified_time" content="2024-09-20T14:43:29.052Z">
<meta property="article:author" content="HUI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/122061154_p0_master1200.jpg"><link rel="shortcut icon" href="/img/122172157_p0_master1200.jpg"><link rel="canonical" href="http://example.com/2024/09/20/NLP_Course(6.3)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"æ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}","hits_stats":"å…±æ‰¾åˆ° ${hits} ç¯‡æ–‡ç« "}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  dateSuffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'åŠ è½½æ›´å¤š'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-20 22:43:29'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/bronya.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">åŠ è½½ä¸­...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/122061154_p0_master1200.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é </span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ™‚é–“è»¸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ¨™ç±¤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†é¡</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> æ¸…å–®</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> éŸ³æ¨‚</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> ç…§ç‰‡</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> é›»å½±</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹éˆ</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> é—œæ–¼</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="HUI"><img class="site-icon" src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"/><span class="site-name">HUI</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> é¦–é </span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> æ™‚é–“è»¸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> æ¨™ç±¤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> åˆ†é¡</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> æ¸…å–®</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> éŸ³æ¨‚</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> ç…§ç‰‡</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> é›»å½±</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> å‹éˆ</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> é—œæ–¼</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-09-20T14:39:33.000Z" title="å‘è¡¨äº 2024-09-20 22:39:33">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2024-09-20T14:43:29.052Z" title="æ›´æ–°äº 2024-09-20 22:43:29">2024-09-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/">HuggingFace</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>19åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">è¯„è®ºæ•°:</span><a href="/2024/09/20/NLP_Course(6.3)/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/09/20/NLP_Course(6.3)/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>è½¬è½½è‡ªï¼š<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/">https://huggingface.co/learn/nlp-course/zh-CN/</a></p>
<p><strong>åŸä¸­æ–‡æ–‡æ¡£æœ‰å¾ˆå¤šåœ°æ–¹ç¿»è¯‘çš„å¤ªæ•·è¡äº†ï¼Œå› æ­¤æ‰æœ‰è¯ç³»åˆ—æ–‡ç« ã€‚</strong></p>
<h1 id="nlpè¯¾ç¨‹å…­-ä¸‹"><a class="markdownIt-Anchor" href="#nlpè¯¾ç¨‹å…­-ä¸‹"></a> NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰</h1>
<h2 id="é€å—æ„å»ºåˆ†è¯å™¨"><a class="markdownIt-Anchor" href="#é€å—æ„å»ºåˆ†è¯å™¨"></a> é€å—æ„å»ºåˆ†è¯å™¨</h2>
<p>æ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢å‡ èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæ ‡è®°åŒ–åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼š</p>
<ul>
<li>è§„èŒƒåŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰</li>
<li>é¢„æ ‡è®°åŒ–ï¼ˆå°†è¾“å…¥åˆ†å‰²ä¸ºå•è¯ï¼‰</li>
<li>é€šè¿‡æ¨¡å‹è¿è¡Œè¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ ‡è®°åŒ–çš„å•è¯æ¥ç”Ÿæˆæ ‡è®°åºåˆ—ï¼‰</li>
<li>åå¤„ç†ï¼ˆæ·»åŠ åˆ†è¯å™¨çš„ç‰¹æ®Šæ ‡è®°ï¼Œç”Ÿæˆæ³¨æ„æ©ç å’Œæ ‡è®°ç±»å‹ IDï¼‰</li>
</ul>
<p>ç¤ºä¾‹ï¼š</p>
<p><img src="en_chapter6_tokenization_pipeline.svg" alt></p>
<p>Tokenizers åº“çš„æ„å»ºæ˜¯ä¸ºäº†ä¸ºæ¯ä¸ªæ­¥éª¤æä¾›å¤šä¸ªé€‰é¡¹ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬æ··åˆå’ŒåŒ¹é…åœ¨ä¸€èµ·ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•ä»å¤´å¼€å§‹æ„å»ºåˆ†è¯å™¨ï¼Œè€Œä¸æ˜¯ä»æ—§åˆ†è¯å™¨è®­ç»ƒæ–°çš„åˆ†è¯å™¨ã€‚ç„¶åï¼Œæ‚¨å°†èƒ½å¤Ÿæ„å»ºæ‚¨èƒ½æƒ³åˆ°çš„ä»»ä½•ç±»å‹çš„æ ‡è®°å™¨ï¼</p>
<p>ç¡®åœ°è¯´ï¼Œè¯¥åº“æ˜¯å›´ç»•ä¸€ä¸ªä¸­å¿ƒ<code>Tokenizer</code>ç±»æ„å»ºçš„ï¼Œå…¶ä¸­çš„æ„å»ºå—åœ¨å­æ¨¡å—ä¸­é‡æ–°åˆ†ç»„ï¼š</p>
<ul>
<li><code>normalizers</code>åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¯èƒ½ç±»å‹çš„<code>Normalizer</code> ï¼ˆå®Œæ•´åˆ—è¡¨<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/normalizers">è¯·å‚è§æ­¤å¤„</a>ï¼‰ã€‚</li>
<li><code>pre_tokenizers</code>åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¯èƒ½çš„<code>PreTokenizer</code>ç±»å‹ï¼ˆå®Œæ•´åˆ—è¡¨<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/pre-tokenizers">è¯·å‚è§æ­¤å¤„</a>ï¼‰ã€‚</li>
<li><code>models</code>åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„å„ç§ç±»å‹çš„<code>Model</code> ï¼Œä¾‹å¦‚<code>BPE</code> ã€ <code>WordPiece</code>å’Œ<code>Unigram</code> ï¼ˆå®Œæ•´åˆ—è¡¨<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/models">è¯·å‚è§æ­¤å¤„</a>ï¼‰ã€‚</li>
<li><code>trainers</code>åŒ…å«å¯ç”¨äºåœ¨è¯­æ–™åº“ä¸Šè®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰ä¸åŒç±»å‹çš„<code>Trainer</code> ï¼ˆæ¯ç§ç±»å‹çš„æ¨¡å‹ä¸€ä¸ªï¼›å®Œæ•´åˆ—è¡¨<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/trainers">è¯·å‚è§æ­¤å¤„</a>ï¼‰ã€‚</li>
<li><code>post_processors</code>åŒ…å«æ‚¨å¯ä»¥ä½¿ç”¨çš„å„ç§ç±»å‹çš„<code>PostProcessor</code> ï¼ˆå®Œæ•´åˆ—è¡¨<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/post-processors">è¯·å‚è§æ­¤å¤„</a>ï¼‰ã€‚</li>
<li><code>decoders</code>åŒ…å«å¯ç”¨äºè§£ç æ ‡è®°åŒ–è¾“å‡ºçš„å„ç§ç±»å‹çš„<code>Decoder</code> ï¼ˆå®Œæ•´åˆ—è¡¨<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/components#decoders">è§æ­¤å¤„</a>ï¼‰ã€‚</li>
</ul>
<p>æ‚¨å¯ä»¥<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/components">åœ¨æ­¤å¤„</a>æ‰¾åˆ°æ„å»ºå—çš„å®Œæ•´åˆ—è¡¨ã€‚</p>
<h3 id="1è·å–è¯­æ–™åº“"><a class="markdownIt-Anchor" href="#1è·å–è¯­æ–™åº“"></a> 1.è·å–è¯­æ–™åº“</h3>
<p>ä¸ºäº†è®­ç»ƒæˆ‘ä»¬çš„æ–°åˆ†è¯å™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå°çš„æ–‡æœ¬è¯­æ–™åº“ï¼ˆå› æ­¤ç¤ºä¾‹è¿è¡Œå¾—å¾ˆå¿«ï¼‰ã€‚è·å–è¯­æ–™åº“çš„æ­¥éª¤ä¸æˆ‘ä»¬åœ¨<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter6/2">æœ¬ç« å¼€å¤´</a>æ‰€é‡‡å–çš„æ­¥éª¤ç±»ä¼¼ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬å°†ä½¿ç”¨<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wikitext">WikiText-2</a>æ•°æ®é›†ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;wikitext&quot;</span>, name=<span class="string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), <span class="number">1000</span>):</span><br><span class="line">        <span class="keyword">yield</span> dataset[i : i + <span class="number">1000</span>][<span class="string">&quot;text&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>å‡½æ•°<code>get_training_corpus()</code>æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œå®ƒå°†ç”Ÿæˆ åŒ…å«1,000 ä¸ªæ–‡æœ¬çš„æ‰¹æ¬¡ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥è®­ç»ƒåˆ†è¯å™¨ã€‚</p>
<blockquote>
<p>ğŸ¤— åˆ†è¯å™¨ä¹Ÿå¯ä»¥ç›´æ¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•ç”Ÿæˆä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°ä½¿ç”¨çš„ WikiText-2 ä¸­çš„æ‰€æœ‰æ–‡æœ¬/è¾“å…¥ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;wikitext-2.txt&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)):</span><br><span class="line">  f.write(dataset[i][<span class="string">&quot;text&quot;</span>] + <span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•é€å—æ„å»ºæ‚¨è‡ªå·±çš„ BERTã€GPT-2 å’Œ XLNet åˆ†è¯å™¨ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›ä¸‰ç§ä¸»è¦æ ‡è®°åŒ–ç®—æ³•çš„ç¤ºä¾‹ï¼šWordPieceã€BPE å’Œ Unigramã€‚è®©æˆ‘ä»¬ä» BERT å¼€å§‹å§ï¼</p>
<h3 id="2ä»å¤´æ„å»ºwordpieceåˆ†è¯å™¨"><a class="markdownIt-Anchor" href="#2ä»å¤´æ„å»ºwordpieceåˆ†è¯å™¨"></a> 2.ä»å¤´æ„å»ºWordPieceåˆ†è¯å™¨</h3>
<p>è¦ä½¿ç”¨ ğŸ¤— Tokenizers åº“æ„å»ºåˆ†è¯å™¨ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨<code>model</code>å®ä¾‹åŒ–<code>Tokenizer</code>å¯¹è±¡ï¼Œç„¶åå°†å…¶<code>normalizer</code> ã€ <code>pre_tokenizer</code> ã€ <code>post_processor</code>å’Œ<code>decoder</code>å±æ€§è®¾ç½®ä¸ºæˆ‘ä»¬æƒ³è¦çš„å€¼ã€‚</p>
<p>å¯¹äºæ­¤ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ WordPiece æ¨¡å‹åˆ›å»ºä¸€ä¸ª<code>Tokenizer</code> ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (</span><br><span class="line">    decoders,</span><br><span class="line">    models,</span><br><span class="line">    normalizers,</span><br><span class="line">    pre_tokenizers,</span><br><span class="line">    processors,</span><br><span class="line">    trainers,</span><br><span class="line">    Tokenizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¿…é¡»æŒ‡å®š<code>unk_token</code> ï¼Œä»¥ä¾¿æ¨¡å‹çŸ¥é“åœ¨é‡åˆ°ä»¥å‰ä»æœªè§è¿‡çš„å­—ç¬¦æ—¶è¿”å›ä»€ä¹ˆã€‚æˆ‘ä»¬å¯ä»¥åœ¨æ­¤å¤„è®¾ç½®çš„å…¶ä»–å‚æ•°åŒ…æ‹¬æ¨¡å‹çš„<code>vocab</code> ï¼ˆæˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤ä¸éœ€è¦è®¾ç½®å®ƒï¼‰å’Œ<code>max_input_chars_per_word</code> ï¼Œå®ƒæŒ‡å®šæ¯ä¸ªå•è¯çš„æœ€å¤§é•¿åº¦ï¼ˆæ¯”ä¼ é€’çš„å€¼é•¿çš„å•è¯ï¼‰å°†ä¼šè¢«åˆ†å‰²ï¼‰</p>
<h4 id="æ ‡å‡†åŒ–"><a class="markdownIt-Anchor" href="#æ ‡å‡†åŒ–"></a> æ ‡å‡†åŒ–</h4>
<blockquote>
<p>normalizersåº“è§<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/normalizers">https://huggingface.co/docs/tokenizers/api/normalizers</a></p>
</blockquote>
<p>æ ‡è®°åŒ–çš„ç¬¬ä¸€æ­¥æ˜¯æ ‡å‡†åŒ–ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä»æ ‡å‡†åŒ–å¼€å§‹ã€‚ç”±äº BERT è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå› æ­¤æœ‰ä¸€ä¸ª<code>BertNormalizer</code> ï¼Œå®ƒå…·æœ‰æˆ‘ä»¬å¯ä»¥ä¸º BERT è®¾ç½®çš„ç»å…¸é€‰é¡¹ï¼š <code>lowercase</code>ï¼ˆå°å†™ï¼‰å’Œ<code>strip_accents</code>ï¼ˆæ˜¯å¦å»é™¤æ‰€æœ‰é‡éŸ³ç¬¦å·ï¼‰ ï¼Œè¿™æ˜¯ä¸è¨€è‡ªæ˜çš„ï¼› <code>clean_text</code>åˆ é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦å¹¶å°†é‡å¤ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼ï¼›å’Œ<code>handle_chinese_chars</code> ï¼Œå®ƒåœ¨ä¸­æ–‡å­—ç¬¦å‘¨å›´æ”¾ç½®ç©ºæ ¼ã€‚è¦å¤åˆ¶<code>bert-base-uncased</code>åˆ†è¯å™¨ï¼Œæˆ‘ä»¬åªéœ€è®¾ç½®æ­¤æ ‡å‡†åŒ–å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.BertNormalizer(lowercase=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><font color="red">ç„¶è€Œï¼Œä¸€èˆ¬æ¥è¯´ï¼Œåœ¨æ„å»ºæ–°çš„åˆ†è¯å™¨æ—¶ï¼Œæ‚¨å°†æ— æ³•è®¿é—® ğŸ¤— Tokenizers åº“ä¸­å·²ç»å®ç°çš„æ–¹ä¾¿çš„æ ‡å‡†åŒ–å™¨</font> - æ‰€ä»¥è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ‰‹åŠ¨åˆ›å»º BERT æ ‡å‡†åŒ–å™¨ã€‚è¯¥åº“æä¾›äº†<code>Lowercase</code>è§„èŒƒåŒ–å™¨å’Œ<code>StripAccents</code>è§„èŒƒåŒ–å™¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨<code>Sequence</code>ç»„åˆå¤šä¸ªè§„èŒƒåŒ–å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><font color="red">æˆ‘ä»¬è¿˜ä½¿ç”¨<code>NFD</code> Unicode è§„èŒƒåŒ–å™¨ï¼Œå¦åˆ™<code>StripAccents</code>è§„èŒƒåŒ–å™¨å°†æ— æ³•æ­£ç¡®è¯†åˆ«é‡éŸ³å­—ç¬¦ï¼Œå› æ­¤ä¸ä¼šå°†å®ƒä»¬åˆ é™¤ã€‚</font></p>
<p>æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code>normalizer</code>çš„<code>normalize_str()</code>æ–¹æ³•æ¥æ£€æŸ¥å®ƒå¯¹ç»™å®šæ–‡æœ¬çš„å½±å“ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenizer.normalizer.normalize_str(<span class="string">&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;</span>))</span><br><span class="line"></span><br><span class="line">hello how are u?</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>æ›´è¿›ä¸€æ­¥</strong>å¦‚æœæ‚¨åœ¨åŒ…å« unicode å­—ç¬¦<code>u&quot;\u0085&quot;</code>å­—ç¬¦ä¸²ä¸Šæµ‹è¯•å…ˆå‰è§„èŒƒåŒ–å™¨çš„ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ‚¨è‚¯å®šä¼šæ³¨æ„åˆ°è¿™ä¸¤ä¸ªè§„èŒƒåŒ–å™¨å¹¶ä¸å®Œå…¨ç›¸åŒã€‚ä¸ºäº†é¿å…ä½¿ç”¨<code>normalizers.Sequence</code>ä½¿ç‰ˆæœ¬è¿‡äºå¤æ‚ï¼Œæˆ‘ä»¬æ²¡æœ‰åŒ…å«å½“<code>clean_text</code>å‚æ•°è®¾ç½®ä¸º<code>True</code>æ—¶<code>BertNormalizer</code>æ‰€éœ€çš„æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ - è¿™æ˜¯é»˜è®¤è¡Œä¸ºã€‚ä½†ä¸ç”¨æ‹…å¿ƒï¼šé€šè¿‡åœ¨è§„èŒƒåŒ–åºåˆ—ä¸­æ·»åŠ ä¸¤ä¸ª<code>normalizers.Replace</code> .Replace ï¼Œæ— éœ€ä½¿ç”¨æ–¹ä¾¿çš„<code>BertNormalizer</code>å³å¯è·å¾—å®Œå…¨ç›¸åŒçš„è§„èŒƒåŒ–ã€‚</p>
</blockquote>
<h4 id="é¢„æ ‡è®°åŒ–"><a class="markdownIt-Anchor" href="#é¢„æ ‡è®°åŒ–"></a> é¢„æ ‡è®°åŒ–</h4>
<p>pre_tokenizersåº“è§<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/pre-tokenizers">https://huggingface.co/docs/tokenizers/api/pre-tokenizers</a></p>
<p>æ¥ä¸‹æ¥æ˜¯é¢„æ ‡è®°åŒ–æ­¥éª¤ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªé¢„æ„å»ºçš„<code>BertPreTokenizer</code> ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()</span><br></pre></td></tr></table></figure>
<p>æˆ–è€…æˆ‘ä»¬å¯ä»¥ä»å¤´å¼€å§‹æ„å»ºå®ƒï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()</span><br></pre></td></tr></table></figure>
<p><font color="red">è¯·æ³¨æ„ï¼Œ Whitespaceé¢„åˆ†è¯å™¨ä¼šæ ¹æ®ç©ºæ ¼å’Œæ‰€æœ‰éå­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿å­—ç¬¦çš„å­—ç¬¦è¿›è¡Œæ‹†åˆ†ï¼Œå› æ­¤ä»æŠ€æœ¯ä¸Šè®²ï¼Œå®ƒä¼šæ ¹æ®ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œæ‹†åˆ†ï¼š</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;Let&#x27;</span>, (<span class="number">0</span>, <span class="number">3</span>)), (<span class="string">&quot;&#x27;&quot;</span>, (<span class="number">3</span>, <span class="number">4</span>)), (<span class="string">&#x27;s&#x27;</span>, (<span class="number">4</span>, <span class="number">5</span>)), (<span class="string">&#x27;test&#x27;</span>, (<span class="number">6</span>, <span class="number">10</span>)), (<span class="string">&#x27;my&#x27;</span>, (<span class="number">11</span>, <span class="number">13</span>)), (<span class="string">&#x27;pre&#x27;</span>, (<span class="number">14</span>, <span class="number">17</span>)),</span><br><span class="line"> (<span class="string">&#x27;-&#x27;</span>, (<span class="number">17</span>, <span class="number">18</span>)), (<span class="string">&#x27;tokenizer&#x27;</span>, (<span class="number">18</span>, <span class="number">27</span>)), (<span class="string">&#x27;.&#x27;</span>, (<span class="number">27</span>, <span class="number">28</span>))]</span><br></pre></td></tr></table></figure>
<p>å¦‚æœæ‚¨åªæƒ³æŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œåˆ™åº”è¯¥ä½¿ç”¨<code>WhitespaceSplit</code>é¢„åˆ†è¯å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pre_tokenizer = pre_tokenizers.WhitespaceSplit()</span><br><span class="line">pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"></span><br><span class="line">[(<span class="string">&quot;Let&#x27;s&quot;</span>, (<span class="number">0</span>, <span class="number">5</span>)), (<span class="string">&#x27;test&#x27;</span>, (<span class="number">6</span>, <span class="number">10</span>)), (<span class="string">&#x27;my&#x27;</span>, (<span class="number">11</span>, <span class="number">13</span>)), (<span class="string">&#x27;pre-tokenizer.&#x27;</span>, (<span class="number">14</span>, <span class="number">28</span>))]</span><br></pre></td></tr></table></figure>
<p>ä¸è§„èŒƒåŒ–å™¨ä¸€æ ·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨<code>Sequence</code>æ¥ç»„æˆå¤šä¸ªé¢„æ ‡è®°å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pre_tokenizer = pre_tokenizers.<span class="type">Sequence</span>(</span><br><span class="line">    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()] <span class="comment">#æŒ‰ç©ºæ ¼åˆ†å‰²,å°†æ ‡ç‚¹ç¬¦å·åˆ†å‰²ä¸ºå•ä¸ªå­—ç¬¦</span></span><br><span class="line">)</span><br><span class="line">pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test my pre-tokenizer.&quot;</span>)</span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;Let&#x27;</span>, (<span class="number">0</span>, <span class="number">3</span>)), (<span class="string">&quot;&#x27;&quot;</span>, (<span class="number">3</span>, <span class="number">4</span>)), (<span class="string">&#x27;s&#x27;</span>, (<span class="number">4</span>, <span class="number">5</span>)), (<span class="string">&#x27;test&#x27;</span>, (<span class="number">6</span>, <span class="number">10</span>)), (<span class="string">&#x27;my&#x27;</span>, (<span class="number">11</span>, <span class="number">13</span>)), (<span class="string">&#x27;pre&#x27;</span>, (<span class="number">14</span>, <span class="number">17</span>)),</span><br><span class="line"> (<span class="string">&#x27;-&#x27;</span>, (<span class="number">17</span>, <span class="number">18</span>)), (<span class="string">&#x27;tokenizer&#x27;</span>, (<span class="number">18</span>, <span class="number">27</span>)), (<span class="string">&#x27;.&#x27;</span>, (<span class="number">27</span>, <span class="number">28</span>))]</span><br></pre></td></tr></table></figure>
<h4 id="æ¨¡å‹"><a class="markdownIt-Anchor" href="#æ¨¡å‹"></a> æ¨¡å‹</h4>
<p>Modelsåº“è§<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/models">https://huggingface.co/docs/tokenizers/api/models</a></p>
<p>trainersåº“è§<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/trainers#trainers">https://huggingface.co/docs/tokenizers/api/trainers#trainers</a></p>
<p>æ ‡è®°åŒ–ç®¡é“çš„ä¸‹ä¸€æ­¥æ˜¯é€šè¿‡æ¨¡å‹è¿è¡Œè¾“å…¥ã€‚æˆ‘ä»¬å·²ç»åœ¨åˆå§‹åŒ–ä¸­æŒ‡å®šäº†æ¨¡å‹ï¼Œä½†æˆ‘ä»¬ä»ç„¶éœ€è¦è®­ç»ƒå®ƒï¼Œè¿™å°†éœ€è¦<code>WordPieceTrainer</code> ã€‚åœ¨ ğŸ¤— Tokenizers ä¸­å®ä¾‹åŒ–è®­ç»ƒå™¨æ—¶è¦è®°ä½çš„ä¸»è¦äº‹æƒ…æ˜¯ï¼Œæ‚¨éœ€è¦å°†æ‚¨æ‰“ç®—ä½¿ç”¨çš„æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ä¼ é€’ç»™å®ƒ - å¦åˆ™å®ƒä¸ä¼šå°†å®ƒä»¬æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œå› ä¸ºå®ƒä»¬ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>]</span><br><span class="line"><span class="comment"># æœ€ç»ˆè¯æ±‡è¡¨çš„å¤§å°25000ï¼ŒåŒ…æ‹¬æ‰€æœ‰æ ‡è®°å’Œå­—æ¯è¡¨ã€‚special_tokens:æ¨¡å‹åº”è¯¥çŸ¥é“çš„ç‰¹æ®Šæ ‡è®°çš„åˆ—è¡¨ã€‚</span></span><br><span class="line">trainer = trainers.WordPieceTrainer(vocab_size=<span class="number">25000</span>, special_tokens=special_tokens)</span><br></pre></td></tr></table></figure>
<p>é™¤äº†æŒ‡å®š<code>vocab_size</code>å’Œ<code>special_tokens</code>ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è®¾ç½®<code>min_frequency</code> ï¼ˆæ ‡è®°å¯¹åº”å…·æœ‰çš„æœ€å°é¢‘ç‡æ‰èƒ½åˆå¹¶ï¼‰æˆ–æ›´æ”¹<code>continuing_subword_prefix</code> ï¼ˆå¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨ä¸<code>##</code>ä¸åŒçš„å‰ç¼€ï¼‰ã€‚</p>
<p>è¦ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è¿­ä»£å™¨æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶æ¥è®­ç»ƒæˆ‘ä»¬çš„åˆ†è¯å™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼ˆæˆ‘ä»¬äº‹å…ˆä½¿ç”¨ç©º<code>WordPiece</code>é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.model = models.WordPiece(unk_token=<span class="string">&quot;[UNK]&quot;</span>)</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure>
<p>åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥é€šè¿‡è°ƒç”¨<code>encode()</code>æ–¹æ³•æ¥æµ‹è¯•æ–‡æœ¬ä¸Šçš„åˆ†è¯å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;let&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;tok&#x27;</span>, <span class="string">&#x27;##eni&#x27;</span>, <span class="string">&#x27;##zer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>è·å¾—çš„<code>encoding</code>æ˜¯<code>Encoding</code> ï¼Œå®ƒåœ¨å…¶å„ç§å±æ€§ä¸­åŒ…å«åˆ†è¯å™¨çš„æ‰€æœ‰å¿…è¦è¾“å‡ºï¼š <code>ids</code> ã€ <code>type_ids</code> ã€ <code>tokens</code> ã€ <code>offsets</code> ã€ <code>attention_mask</code> ã€ <code>special_tokens_mask</code>å’Œ<code>overflowing</code> ã€‚</p>
<h4 id="åå¤„ç†"><a class="markdownIt-Anchor" href="#åå¤„ç†"></a> åå¤„ç†</h4>
<p>Post-processorsåº“è§<a target="_blank" rel="noopener" href="https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors">https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors</a>.</p>
<p>æ ‡è®°åŒ–ç®¡é“çš„æœ€åä¸€æ­¥æ˜¯åå¤„ç†ã€‚æˆ‘ä»¬éœ€è¦åœ¨å¼€å¤´æ·»åŠ <code>[CLS]</code>æ ‡è®°ï¼Œåœ¨æœ«å°¾æ·»åŠ <code>[SEP]</code>æ ‡è®°ï¼ˆæˆ–è€…åœ¨æ¯ä¸ªå¥å­ä¹‹åï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€å¯¹å¥å­ï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code>TemplateProcessor</code> ï¼Œä½†é¦–å…ˆæˆ‘ä»¬éœ€è¦çŸ¥é“è¯æ±‡è¡¨ä¸­<code>[CLS]</code>å’Œ<code>[SEP]</code>æ ‡è®°çš„ IDï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"></span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>è¦ä¸º<code>TemplateProcessor</code>ç¼–å†™æ¨¡æ¿ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šå¦‚ä½•å¤„ç†å•ä¸ªå¥å­å’Œä¸€å¯¹å¥å­ã€‚å¯¹äºä¸¤è€…ï¼Œæˆ‘ä»¬ç¼–å†™æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°ï¼›ç¬¬ä¸€ä¸ªï¼ˆæˆ–å•ä¸ªï¼‰å¥å­ç”±<code>$A</code>è¡¨ç¤ºï¼Œè€Œç¬¬äºŒä¸ªå¥å­ï¼ˆå¦‚æœç¼–ç ä¸€å¯¹ï¼‰ç”±<code>$B</code>è¡¨ç¤ºã€‚å¯¹äºå…¶ä¸­çš„æ¯ä¸€ä¸ªï¼ˆç‰¹æ®Šæ ‡è®°å’Œå¥å­ï¼‰ï¼Œæˆ‘ä»¬è¿˜åœ¨å†’å·åæŒ‡å®šç›¸åº”çš„æ ‡è®°ç±»å‹ IDã€‚</p>
<p>ç»å…¸çš„ BERT æ¨¡æ¿å®šä¹‰å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0&quot;</span>,</span><br><span class="line">    pair=<span class="string">f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;[CLS]&quot;</span>, cls_token_id), (<span class="string">&quot;[SEP]&quot;</span>, sep_token_id)],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>è¯·æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ç‰¹æ®Šæ ‡è®°çš„ IDï¼Œä»¥ä¾¿æ ‡è®°ç”Ÿæˆå™¨å¯ä»¥æ­£ç¡®åœ°å°†å®ƒä»¬è½¬æ¢ä¸ºå…¶ IDã€‚</p>
<p>æ·»åŠ åï¼Œå›åˆ°æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹å°†ç»™å‡ºï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;let&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;tok&#x27;</span>, <span class="string">&#x27;##eni&#x27;</span>, <span class="string">&#x27;##zer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>äºä¸€å¯¹å¥å­ï¼Œæˆ‘ä»¬å¾—åˆ°æ­£ç¡®çš„ç»“æœï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="string">&quot;on a pair of sentences.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="built_in">print</span>(encoding.type_ids)</span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;let&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;tok&#x27;</span>, <span class="string">&#x27;##eni&#x27;</span>, <span class="string">&#x27;##zer&#x27;</span>, <span class="string">&#x27;...&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;pair&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;sentences&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å‡ ä¹å·²ç»ä»å¤´å¼€å§‹æ„å»ºäº†è¿™ä¸ªåˆ†è¯å™¨â€”â€”æœ€åä¸€æ­¥æ˜¯åŒ…å«ä¸€ä¸ªè§£ç å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.WordPiece(prefix=<span class="string">&quot;##&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>è®©æˆ‘ä»¬ç”¨ä¹‹å‰çš„<code>encoding</code>æ¥æµ‹è¯•ä¸€ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode(encoding.ids)</span><br><span class="line"><span class="string">&quot;let&#x27;s test this tokenizer... on a pair of sentences.&quot;</span></span><br></pre></td></tr></table></figure>
<p>ä¼Ÿå¤§çš„ï¼æˆ‘ä»¬å¯ä»¥å°†æ ‡è®°ç”Ÿæˆå™¨ä¿å­˜åœ¨å•ä¸ª JSON æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.save(<span class="string">&quot;tokenizer.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code>from_file()</code>æ–¹æ³•åœ¨<code>Tokenizer</code>å¯¹è±¡ä¸­é‡æ–°åŠ è½½è¯¥æ–‡ä»¶ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_tokenizer = Tokenizer.from_file(<span class="string">&quot;tokenizer.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>è¦åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨æ­¤åˆ†è¯å™¨ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶åŒ…è£…åœ¨<code>PreTrainedTokenizerFast</code>ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šç”¨ç±»ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬çš„åˆ†è¯å™¨å¯¹åº”äºç°æœ‰æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨è¯¥ç±»ï¼ˆæ­¤å¤„ä¸º<code>BertTokenizerFast</code> ï¼‰ã€‚å¦‚æœæ‚¨åº”ç”¨æœ¬è¯¾ç¨‹æ¥æ„å»ºå…¨æ–°çš„åˆ†è¯å™¨ï¼Œåˆ™å¿…é¡»ä½¿ç”¨ç¬¬ä¸€ä¸ªé€‰é¡¹ã€‚</p>
</blockquote>
<p>è¦å°†åˆ†è¯å™¨åŒ…è£…åœ¨<code>PreTrainedTokenizerFast</code>ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’æˆ‘ä»¬æ„å»ºä¸º<code>tokenizer_object</code>çš„åˆ†è¯å™¨ï¼Œæˆ–è€…ä¼ é€’æˆ‘ä»¬ä¿å­˜ä¸º<code>tokenizer_file</code>çš„åˆ†è¯å™¨æ–‡ä»¶ã€‚è¦è®°ä½çš„å…³é”®æ˜¯æˆ‘ä»¬å¿…é¡»æ‰‹åŠ¨è®¾ç½®æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œå› ä¸ºè¯¥ç±»æ— æ³•ä»<code>tokenizer</code>å¯¹è±¡æ¨æ–­å“ªä¸ªæ ‡è®°æ˜¯æ©ç æ ‡è®°ã€ <code>[CLS]</code>æ ‡è®°ç­‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    <span class="comment"># tokenizer_file=&quot;tokenizer.json&quot;, # You can load from the tokenizer file, alternatively</span></span><br><span class="line">    unk_token=<span class="string">&quot;[UNK]&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;[PAD]&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;[CLS]&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;[SEP]&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;[MASK]&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>å¦‚æœæ‚¨ä½¿ç”¨ç‰¹å®šçš„æ ‡è®°ç”Ÿæˆå™¨ç±»ï¼ˆä¾‹å¦‚<code>BertTokenizerFast</code> ï¼‰ï¼Œåˆ™åªéœ€æŒ‡å®šä¸é»˜è®¤æ ‡è®°ä¸åŒçš„ç‰¹æ®Šæ ‡è®°ï¼ˆæ­¤å¤„ä¸ºæ— ï¼‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<p>ç„¶åï¼Œæ‚¨å¯ä»¥åƒä»»ä½•å…¶ä»– ğŸ¤— Transformers åˆ†è¯å™¨ä¸€æ ·ä½¿ç”¨æ­¤åˆ†è¯å™¨ã€‚æ‚¨å¯ä»¥ä½¿ç”¨<code>save_pretrained()</code>æ–¹æ³•ä¿å­˜å®ƒï¼Œæˆ–ä½¿ç”¨<code>push_to_hub()</code>æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ° Hubã€‚</p>
<p>ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†å¦‚ä½•æ„å»º WordPiece åˆ†è¯å™¨ï¼Œè®©æˆ‘ä»¬å¯¹ BPE åˆ†è¯å™¨æ‰§è¡Œç›¸åŒçš„æ“ä½œã€‚å› ä¸ºæ‚¨çŸ¥é“æ‰€æœ‰æ­¥éª¤ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šèµ°å¾—æ›´å¿«ä¸€äº›ï¼Œå¹¶ä¸”åªçªå‡ºæ˜¾ç¤ºå·®å¼‚ã€‚</p>
<h3 id="3ä»å¤´æ„å»ºbpeåˆ†è¯å™¨"><a class="markdownIt-Anchor" href="#3ä»å¤´æ„å»ºbpeåˆ†è¯å™¨"></a> 3.ä»å¤´æ„å»ºBPEåˆ†è¯å™¨</h3>
<p>ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª GPT-2 åˆ†è¯å™¨ã€‚ä¸ BERT åˆ†è¯å™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ BPE æ¨¡å‹åˆå§‹åŒ–<code>Tokenizer</code> ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.BPE())</span><br></pre></td></tr></table></figure>
<p>ä¸ BERT ä¸€æ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¯æ±‡è¡¨çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¯æ±‡è¡¨æ¥åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬éœ€è¦ä¼ é€’<code>vocab</code>å¹¶<code>merges</code> ï¼‰ï¼Œä½†ç”±äºæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è¿™æ ·åšã€‚æˆ‘ä»¬ä¹Ÿä¸éœ€è¦æŒ‡å®š<code>unk_token</code>å› ä¸º GPT-2 ä½¿ç”¨å­—èŠ‚çº§ BPEï¼Œè¿™ä¸éœ€è¦å®ƒã€‚</p>
<h4 id="é¢„æ ‡è®°åŒ–-2"><a class="markdownIt-Anchor" href="#é¢„æ ‡è®°åŒ–-2"></a> é¢„æ ‡è®°åŒ–</h4>
<p>GPT-2 ä¸ä½¿ç”¨æ ‡å‡†åŒ–å™¨ï¼Œå› æ­¤æˆ‘ä»¬è·³è¿‡è¯¥æ­¥éª¤å¹¶ç›´æ¥è¿›å…¥é¢„æ ‡è®°åŒ–ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬åœ¨æ­¤å¤„æ·»åŠ åˆ°<code>ByteLevel</code>çš„é€‰é¡¹æ˜¯ä¸åœ¨å¥å­å¼€å¤´æ·»åŠ ç©ºæ ¼ï¼ˆå¦åˆ™ä¸ºé»˜è®¤å€¼ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„é¢„æ ‡è®°åŒ–ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test pre-tokenization!&quot;</span>)</span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;Let&#x27;</span>, (<span class="number">0</span>, <span class="number">3</span>)), (<span class="string">&quot;&#x27;s&quot;</span>, (<span class="number">3</span>, <span class="number">5</span>)), (<span class="string">&#x27;Ä test&#x27;</span>, (<span class="number">5</span>, <span class="number">10</span>)), (<span class="string">&#x27;Ä pre&#x27;</span>, (<span class="number">10</span>, <span class="number">14</span>)), (<span class="string">&#x27;-&#x27;</span>, (<span class="number">14</span>, <span class="number">15</span>)),(<span class="string">&#x27;tokenization&#x27;</span>, (<span class="number">15</span>, <span class="number">27</span>)), (<span class="string">&#x27;!&#x27;</span>, (<span class="number">27</span>, <span class="number">28</span>))]</span><br></pre></td></tr></table></figure>
<h4 id="æ¨¡å‹-2"><a class="markdownIt-Anchor" href="#æ¨¡å‹-2"></a> æ¨¡å‹</h4>
<p>æ¥ä¸‹æ¥æ˜¯æ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒã€‚å¯¹äº GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Šæ ‡è®°æ˜¯æ–‡æœ¬ç»“æŸæ ‡è®°ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = trainers.BpeTrainer(vocab_size=<span class="number">25000</span>, special_tokens=[<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>])</span><br><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<p>ä¸<code>WordPieceTrainer</code>ä»¥åŠ<code>vocab_size</code>å’Œ<code>special_tokens</code>ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦æŒ‡å®š<code>min_frequency</code> ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬æœ‰è¯å°¾åç¼€ï¼ˆå¦‚<code>&lt;/w&gt;</code> ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code>end_of_word_suffix</code>è®¾ç½®å®ƒã€‚</p>
<p>è¯¥åˆ†è¯å™¨è¿˜å¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.model = models.BPE()</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure>
<p>è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ ‡è®°åŒ–ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;L&#x27;</span>, <span class="string">&#x27;et&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;Ä test&#x27;</span>, <span class="string">&#x27;Ä this&#x27;</span>, <span class="string">&#x27;Ä to&#x27;</span>, <span class="string">&#x27;ken&#x27;</span>, <span class="string">&#x27;izer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h4 id="åå¤„ç†-2"><a class="markdownIt-Anchor" href="#åå¤„ç†-2"></a> åå¤„ç†</h4>
<p>æˆ‘ä»¬å¯¹ GPT-2 åˆ†è¯å™¨åº”ç”¨å­—èŠ‚çº§åå¤„ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.ByteLevel(trim_offsets=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p><code>trim_offsets = False</code>é€‰é¡¹å‘åå¤„ç†å™¨æŒ‡ç¤ºæˆ‘ä»¬åº”è¯¥ä¿ç•™ä»¥ â€˜Ä â€™ å¼€å¤´çš„æ ‡è®°çš„åç§»é‡ï¼š<font color="red">è¿™æ ·åç§»é‡çš„å¼€å§‹å°†æŒ‡å‘å•è¯ä¹‹å‰çš„ç©ºæ ¼ï¼Œè€Œä¸æ˜¯ç¬¬ä¸€ä¸ªå­—ç¬¦å•è¯ï¼ˆå› ä¸ºç©ºé—´åœ¨æŠ€æœ¯ä¸Šæ˜¯tokençš„ä¸€éƒ¨åˆ†ï¼‰ã€‚</font>è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬åˆšåˆšç¼–ç çš„æ–‡æœ¬çš„ç»“æœï¼Œå…¶ä¸­<code>'Ä test'</code>æ˜¯ç´¢å¼• 4 å¤„çš„æ ‡è®°ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span></span><br><span class="line">encoding = tokenizer.encode(sentence)</span><br><span class="line">start, end = encoding.offsets[<span class="number">4</span>]</span><br><span class="line">sentence[start:end]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27; test&#x27;</span></span><br></pre></td></tr></table></figure>
<p>æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªå­—èŠ‚çº§è§£ç å™¨ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.ByteLevel()</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯ä»¥ä»”ç»†æ£€æŸ¥å®ƒæ˜¯å¦æ­£å¸¸å·¥ä½œï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode(encoding.ids)</span><br><span class="line"><span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span></span><br></pre></td></tr></table></figure>
<p>ä¼Ÿå¤§çš„ï¼ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜ tokenizerï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨å®ƒï¼Œè¯·å°†å…¶åŒ…è£…åœ¨<code>PreTrainedTokenizerFast</code>æˆ–<code>GPT2TokenizerFast</code>ä¸­ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>or: æˆ–è€…ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2TokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<h3 id="4-ä»å¤´æ„å»ºunigramåˆ†è¯å™¨"><a class="markdownIt-Anchor" href="#4-ä»å¤´æ„å»ºunigramåˆ†è¯å™¨"></a> 4. ä»å¤´æ„å»ºUnigramåˆ†è¯å™¨</h3>
<p>ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª XLNet åˆ†è¯å™¨ã€‚ä¸ä¹‹å‰çš„åˆ†è¯å™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ Unigram æ¨¡å‹åˆå§‹åŒ–<code>Tokenizer</code> ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer(models.Unigram())</span><br></pre></td></tr></table></figure>
<p>åŒæ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¯æ±‡è¡¨çš„è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¯æ±‡è¡¨æ¥åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ã€‚</p>
<h4 id="æ ‡å‡†åŒ–-2"><a class="markdownIt-Anchor" href="#æ ‡å‡†åŒ–-2"></a> æ ‡å‡†åŒ–</h4>
<p>å¯¹äºæ ‡å‡†åŒ–ï¼ŒXLNet ä½¿ç”¨ä¸€äº›æ›¿æ¢ï¼ˆæ¥è‡ª SentencePieceï¼‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Regex</span><br><span class="line"></span><br><span class="line">tokenizer.normalizer = normalizers.<span class="type">Sequence</span>(</span><br><span class="line">    [</span><br><span class="line">        normalizers.Replace(<span class="string">&quot;``&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>),</span><br><span class="line">        normalizers.Replace(<span class="string">&quot;&#x27;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>),</span><br><span class="line">        normalizers.NFKD(),</span><br><span class="line">        normalizers.StripAccents(),</span><br><span class="line">        normalizers.Replace(Regex(<span class="string">&quot; &#123;2,&#125;&quot;</span>), <span class="string">&quot; &quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>è¿™ä¼šå°†<code>â€œ</code>å’Œ<code>â€</code>æ›¿æ¢ä¸º<code>â€</code> ï¼Œå¹¶å°†ä¸¤ä¸ªæˆ–å¤šä¸ªç©ºæ ¼çš„ä»»ä½•åºåˆ—æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼ï¼Œå¹¶åˆ é™¤è¦æ ‡è®°çš„æ–‡æœ¬ä¸­çš„é‡éŸ³ç¬¦å·ã€‚</p>
<h4 id="é¢„æ ‡è®°åŒ–-3"><a class="markdownIt-Anchor" href="#é¢„æ ‡è®°åŒ–-3"></a> é¢„æ ‡è®°åŒ–</h4>
<p>ç”¨äºä»»ä½• SentencePiece åˆ†è¯å™¨çš„é¢„åˆ†è¯å™¨æ˜¯<code>Metaspace</code> ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„é¢„æ ‡è®°åŒ–ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Let&#x27;s test the pre-tokenizer!&quot;</span>)</span><br><span class="line">[(<span class="string">&quot;â–Let&#x27;s&quot;</span>, (<span class="number">0</span>, <span class="number">5</span>)), (<span class="string">&#x27;â–test&#x27;</span>, (<span class="number">5</span>, <span class="number">10</span>)), (<span class="string">&#x27;â–the&#x27;</span>, (<span class="number">10</span>, <span class="number">14</span>)), (<span class="string">&#x27;â–pre-tokenizer!&#x27;</span>, (<span class="number">14</span>, <span class="number">29</span>))]</span><br></pre></td></tr></table></figure>
<h4 id="æ¨¡å‹-3"><a class="markdownIt-Anchor" href="#æ¨¡å‹-3"></a> æ¨¡å‹</h4>
<p>æ¥ä¸‹æ¥æ˜¯æ¨¡å‹ï¼Œéœ€è¦è®­ç»ƒã€‚ XLNet æœ‰ç›¸å½“å¤šçš„ç‰¹æ®Šä»¤ç‰Œï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">special_tokens = [<span class="string">&quot;&lt;cls&gt;&quot;</span>, <span class="string">&quot;&lt;sep&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>, <span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;mask&gt;&quot;</span>, <span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>]</span><br><span class="line">trainer = trainers.UnigramTrainer(</span><br><span class="line">    vocab_size=<span class="number">25000</span>, special_tokens=special_tokens, unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span></span><br><span class="line">)</span><br><span class="line">tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)</span><br></pre></td></tr></table></figure>
<p>å¯¹äº<code>UnigramTrainer</code>æ¥è¯´ï¼Œä¸è¦å¿˜è®°çš„ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°æ˜¯<code>unk_token</code> ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä¼ é€’ç‰¹å®šäº Unigram ç®—æ³•çš„å…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚åˆ é™¤æ ‡è®°çš„æ¯ä¸ªæ­¥éª¤çš„<code>shrinking_factor</code> ï¼ˆé»˜è®¤ä¸º 0.75ï¼‰æˆ–æŒ‡å®šç»™å®šæ ‡è®°çš„æœ€å¤§é•¿åº¦çš„<code>max_piece_length</code> ï¼ˆé»˜è®¤ä¸º 16ï¼‰ã€‚</p>
<p>è¯¥åˆ†è¯å™¨è¿˜å¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.model = models.Unigram()</span><br><span class="line">tokenizer.train([<span class="string">&quot;wikitext-2.txt&quot;</span>], trainer=trainer)</span><br></pre></td></tr></table></figure>
<p>è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„æ ‡è®°åŒ–ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer.&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line">[<span class="string">&#x27;â–Let&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;â–test&#x27;</span>, <span class="string">&#x27;â–this&#x27;</span>, <span class="string">&#x27;â–to&#x27;</span>, <span class="string">&#x27;ken&#x27;</span>, <span class="string">&#x27;izer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="åå¤„ç†-3"><a class="markdownIt-Anchor" href="#åå¤„ç†-3"></a> åå¤„ç†</h3>
<p>XLNet çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯å®ƒå°†<code>&lt;cls&gt;</code>æ ‡è®°æ”¾åœ¨å¥å­æœ«å°¾ï¼Œç±»å‹ ID ä¸º 2ï¼ˆä»¥å°†å…¶ä¸å…¶ä»–æ ‡è®°åŒºåˆ†å¼€ï¼‰ã€‚ç»“æœï¼Œå®ƒåœ¨å·¦ä¾§å¡«å……ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡æ¿å¤„ç†æ‰€æœ‰ç‰¹æ®Šä»¤ç‰Œå’Œä»¤ç‰Œç±»å‹ IDï¼Œå°±åƒ BERT ä¸€æ ·ï¼Œä½†é¦–å…ˆæˆ‘ä»¬å¿…é¡»è·å–<code>&lt;cls&gt;</code>å’Œ<code>&lt;sep&gt;</code>ä»¤ç‰Œçš„ IDï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cls_token_id = tokenizer.token_to_id(<span class="string">&quot;&lt;cls&gt;&quot;</span>)</span><br><span class="line">sep_token_id = tokenizer.token_to_id(<span class="string">&quot;&lt;sep&gt;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(cls_token_id, sep_token_id)</span><br><span class="line"></span><br><span class="line"><span class="number">0</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>æ¨¡æ¿å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.post_processor = processors.TemplateProcessing(</span><br><span class="line">    single=<span class="string">&quot;$A:0 &lt;sep&gt;:0 &lt;cls&gt;:2&quot;</span>,</span><br><span class="line">    pair=<span class="string">&quot;$A:0 &lt;sep&gt;:0 $B:1 &lt;sep&gt;:1 &lt;cls&gt;:2&quot;</span>,</span><br><span class="line">    special_tokens=[(<span class="string">&quot;&lt;sep&gt;&quot;</span>, sep_token_id), (<span class="string">&quot;&lt;cls&gt;&quot;</span>, cls_token_id)],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¼–ç ä¸€å¯¹å¥å­æ¥æµ‹è¯•å®ƒçš„å·¥ä½œåŸç†ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoding = tokenizer.encode(<span class="string">&quot;Let&#x27;s test this tokenizer...&quot;</span>, <span class="string">&quot;on a pair of sentences!&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(encoding.tokens)</span><br><span class="line"><span class="built_in">print</span>(encoding.type_ids)</span><br><span class="line">[<span class="string">&#x27;â–Let&#x27;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;â–test&#x27;</span>, <span class="string">&#x27;â–this&#x27;</span>, <span class="string">&#x27;â–to&#x27;</span>, <span class="string">&#x27;ken&#x27;</span>, <span class="string">&#x27;izer&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="string">&#x27;â–&#x27;</span>, <span class="string">&#x27;on&#x27;</span>, <span class="string">&#x27;â–&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;â–pair&#x27;</span>, </span><br><span class="line">  <span class="string">&#x27;â–of&#x27;</span>, <span class="string">&#x27;â–sentence&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;!&#x27;</span>, <span class="string">&#x27;&lt;sep&gt;&#x27;</span>, <span class="string">&#x27;&lt;cls&gt;&#x27;</span>]</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<p>æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ª<code>Metaspace</code>è§£ç å™¨ï¼šæˆ‘ä»¬å°±å®Œæˆäº†è¿™ä¸ªåˆ†è¯å™¨ï¼æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜ tokenizerï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformer ä¸­ä½¿ç”¨å®ƒï¼Œè¯·å°†å…¶åŒ…è£…åœ¨<code>PreTrainedTokenizerFast</code>æˆ–<code>XLNetTokenizerFast</code>ä¸­ã€‚ä½¿ç”¨<code>PreTrainedTokenizerFast</code>æ—¶éœ€è¦æ³¨æ„çš„ä¸€ä»¶äº‹æ˜¯ï¼Œåœ¨ç‰¹æ®Šæ ‡è®°ä¹‹ä¸Šï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰ ğŸ¤— Transformers åº“åœ¨å·¦ä¾§å¡«å……ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;s&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span><br><span class="line">    unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;&lt;pad&gt;&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;&lt;cls&gt;&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;&lt;sep&gt;&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;&lt;mask&gt;&quot;</span>,</span><br><span class="line">    padding_side=<span class="string">&quot;left&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Or alternatively: æˆ–è€…ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decoder = decoders.Metaspace()</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬å°±å®Œæˆäº†è¿™ä¸ªåˆ†è¯å™¨ï¼æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜ tokenizerï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformer ä¸­ä½¿ç”¨å®ƒï¼Œè¯·å°†å…¶åŒ…è£…åœ¨<code>PreTrainedTokenizerFast</code>æˆ–<code>XLNetTokenizerFast</code>ä¸­ã€‚ä½¿ç”¨<code>PreTrainedTokenizerFast</code>æ—¶éœ€è¦æ³¨æ„çš„ä¸€ä»¶äº‹æ˜¯ï¼Œåœ¨ç‰¹æ®Šæ ‡è®°ä¹‹ä¸Šï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰ ğŸ¤— Transformers åº“åœ¨å·¦ä¾§å¡«å……ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> PreTrainedTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = PreTrainedTokenizerFast(</span><br><span class="line">    tokenizer_object=tokenizer,</span><br><span class="line">    bos_token=<span class="string">&quot;&lt;s&gt;&quot;</span>,</span><br><span class="line">    eos_token=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span><br><span class="line">    unk_token=<span class="string">&quot;&lt;unk&gt;&quot;</span>,</span><br><span class="line">    pad_token=<span class="string">&quot;&lt;pad&gt;&quot;</span>,</span><br><span class="line">    cls_token=<span class="string">&quot;&lt;cls&gt;&quot;</span>,</span><br><span class="line">    sep_token=<span class="string">&quot;&lt;sep&gt;&quot;</span>,</span><br><span class="line">    mask_token=<span class="string">&quot;&lt;mask&gt;&quot;</span>,</span><br><span class="line">    padding_side=<span class="string">&quot;left&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>Or alternatively: æˆ–è€…ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizerFast</span><br><span class="line"></span><br><span class="line">wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="http://example.com">HUI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="http://example.com/2024/09/20/NLP_Course(6.3)/">http://example.com/2024/09/20/NLP_Course(6.3)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥è‡ª <a href="http://example.com" target="_blank">HUI</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/122061154_p0_master1200.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/20/NLP_Course(7.1)/" title="NLPè¯¾ç¨‹ï¼ˆä¸ƒ/ä¸€ï¼‰"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info">NLPè¯¾ç¨‹ï¼ˆä¸ƒ/ä¸€ï¼‰</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/20/NLP_Course(6.2)/" title="NLPè¯¾ç¨‹ï¼ˆå…­-ä¸­ï¼‰"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">NLPè¯¾ç¨‹ï¼ˆå…­-ä¸­ï¼‰</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> è¯„è®º</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/122061154_p0_master1200.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HUI</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp%E8%AF%BE%E7%A8%8B%E5%85%AD-%E4%B8%8B"><span class="toc-text"> NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%90%E5%9D%97%E6%9E%84%E5%BB%BA%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text"> é€å—æ„å»ºåˆ†è¯å™¨</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E8%8E%B7%E5%8F%96%E8%AF%AD%E6%96%99%E5%BA%93"><span class="toc-text"> 1.è·å–è¯­æ–™åº“</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BAwordpiece%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text"> 2.ä»å¤´æ„å»ºWordPieceåˆ†è¯å™¨</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-text"> æ ‡å‡†åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="toc-text"> é¢„æ ‡è®°åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-text"> æ¨¡å‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E5%A4%84%E7%90%86"><span class="toc-text"> åå¤„ç†</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BAbpe%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text"> 3.ä»å¤´æ„å»ºBPEåˆ†è¯å™¨</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96-2"><span class="toc-text"> é¢„æ ‡è®°åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-2"><span class="toc-text"> æ¨¡å‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E5%A4%84%E7%90%86-2"><span class="toc-text"> åå¤„ç†</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E4%BB%8E%E5%A4%B4%E6%9E%84%E5%BB%BAunigram%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-text"> 4. ä»å¤´æ„å»ºUnigramåˆ†è¯å™¨</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96-2"><span class="toc-text"> æ ‡å‡†åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96-3"><span class="toc-text"> é¢„æ ‡è®°åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-3"><span class="toc-text"> æ¨¡å‹</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8E%E5%A4%84%E7%90%86-3"><span class="toc-text"> åå¤„ç†</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(7.1)/" title="NLPè¯¾ç¨‹ï¼ˆä¸ƒ/ä¸€ï¼‰">NLPè¯¾ç¨‹ï¼ˆä¸ƒ/ä¸€ï¼‰</a><time datetime="2024-09-20T14:40:33.000Z" title="å‘è¡¨äº 2024-09-20 22:40:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(6.3)/" title="NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰">NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰</a><time datetime="2024-09-20T14:39:33.000Z" title="å‘è¡¨äº 2024-09-20 22:39:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(6.2)/" title="NLPè¯¾ç¨‹ï¼ˆå…­-ä¸­ï¼‰">NLPè¯¾ç¨‹ï¼ˆå…­-ä¸­ï¼‰</a><time datetime="2024-09-20T14:35:33.000Z" title="å‘è¡¨äº 2024-09-20 22:35:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(6.1)/" title="NLPè¯¾ç¨‹ï¼ˆå…­-ä¸Šï¼‰">NLPè¯¾ç¨‹ï¼ˆå…­-ä¸Šï¼‰</a><time datetime="2024-09-20T14:34:33.000Z" title="å‘è¡¨äº 2024-09-20 22:34:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(5)/" title="NLPè¯¾ç¨‹ï¼ˆäº”ï¼‰">NLPè¯¾ç¨‹ï¼ˆäº”ï¼‰</a><time datetime="2024-09-20T14:33:33.000Z" title="å‘è¡¨äº 2024-09-20 22:33:33">2024-09-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By HUI</div><div class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="èŠå¤©"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="ç›´è¾¾è¯„è®º"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2024/09/20/NLP_Course(6.3)/'
    this.page.identifier = '/2024/09/20/NLP_Course(6.3)/'
    this.page.title = 'NLPè¯¾ç¨‹ï¼ˆå…­-ä¸‹ï¼‰'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“åŠ è½½ä¸­</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>