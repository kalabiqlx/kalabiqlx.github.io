<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLP课程（六-上）- Tokenizer库 | HUI</title><meta name="author" content="HUI"><meta name="copyright" content="HUI"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（六-上）- Tokenizer库  根据已有的tokenizer训练新的tokenizer  1.准备语料库  ⚠️ 训练标记器与训练模型不同！模型训练使用随机梯度下降使每个batch的loss小一点。它本质上是随机的（这意味">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP课程（六-上）- Tokenizer库">
<meta property="og:url" content="http://example.com/2024/09/20/NLP_Course(6.1)/index.html">
<meta property="og:site_name" content="HUI">
<meta property="og:description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（六-上）- Tokenizer库  根据已有的tokenizer训练新的tokenizer  1.准备语料库  ⚠️ 训练标记器与训练模型不同！模型训练使用随机梯度下降使每个batch的loss小一点。它本质上是随机的（这意味">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/87788970_p0_master1200.jpg">
<meta property="article:published_time" content="2024-09-20T14:34:33.000Z">
<meta property="article:modified_time" content="2024-09-21T09:02:45.382Z">
<meta property="article:author" content="HUI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/87788970_p0_master1200.jpg"><link rel="shortcut icon" href="/img/122061154_p0_master1200.jpg"><link rel="canonical" href="http://example.com/2024/09/20/NLP_Course(6.1)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP课程（六-上）- Tokenizer库',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-21 17:02:45'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/bronya.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/87788970_p0_master1200.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="HUI"><img class="site-icon" src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"/><span class="site-name">HUI</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP课程（六-上）- Tokenizer库</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-20T14:34:33.000Z" title="发表于 2024-09-20 22:34:33">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-21T09:02:45.382Z" title="更新于 2024-09-21 17:02:45">2024-09-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/">HuggingFace</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>50分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP课程（六-上）- Tokenizer库"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/09/20/NLP_Course(6.1)/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/09/20/NLP_Course(6.1)/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>转载自：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/">https://huggingface.co/learn/nlp-course/zh-CN/</a></p>
<p><strong>原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。</strong></p>
<h1 id="nlp课程六-上-tokenizer库"><a class="markdownIt-Anchor" href="#nlp课程六-上-tokenizer库"></a> NLP课程（六-上）- Tokenizer库</h1>
<h2 id="根据已有的tokenizer训练新的tokenizer"><a class="markdownIt-Anchor" href="#根据已有的tokenizer训练新的tokenizer"></a> 根据已有的tokenizer训练新的tokenizer</h2>
<h3 id="1准备语料库"><a class="markdownIt-Anchor" href="#1准备语料库"></a> 1.准备语料库</h3>
<blockquote>
<p>⚠️ <font color="red">训练标记器与训练模型不同！模型训练使用随机梯度下降使每个batch的loss小一点。它本质上是随机的（这意味着在进行两次相同的训练时，您必须设置一些随机数种子才能获得相同的结果）。训练标记器是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，用于选择它们的确切规则取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，您总是会得到相同的结果。</font></p>
</blockquote>
<p>Transformers 中有一个非常简单的 API，你可以用它来训练一个新的标记器，使它与现有标记器相同的特征： <code>AutoTokenizer.train_new_from_iterator() </code></p>
<p>首要任务是在训练语料库中收集该语言的大量数据。为了提供每个人都能理解的示例，我们在这里不会使用俄语或中文之类的语言，而是使用在特定领域的英语语言：Python 代码。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/datasets">🤗 Datasets</a>库可以帮助我们组装一个 Python 源代码语料库。我们将使用**load_dataset()**功能下载和缓存<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/code_search_net">CodeSearchNet</a>数据集。该数据集是为<a target="_blank" rel="noopener" href="https://wandb.ai/github/CodeSearchNet/benchmark">CodeSearchNet 挑战</a>而创建的并包含来自 GitHub 上开源库的数百万种编程语言的函数。在这里，我们将加载此数据集的 Python 部分：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># This can take a few minutes to load, so grab a coffee or tea while you wait!</span></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;code_search_net&quot;</span>, <span class="string">&quot;python&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以查看训练集的部分，以查看我们数据集中有哪些列：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">Dataset(&#123;</span><br><span class="line">    features: [<span class="string">&#x27;repository_name&#x27;</span>, <span class="string">&#x27;func_path_in_repository&#x27;</span>, <span class="string">&#x27;func_name&#x27;</span>, <span class="string">&#x27;whole_func_string&#x27;</span>, <span class="string">&#x27;language&#x27;</span>, </span><br><span class="line">      <span class="string">&#x27;func_code_string&#x27;</span>, <span class="string">&#x27;func_code_tokens&#x27;</span>, <span class="string">&#x27;func_documentation_string&#x27;</span>, <span class="string">&#x27;func_documentation_tokens&#x27;</span>, <span class="string">&#x27;split_name&#x27;</span>, </span><br><span class="line">      <span class="string">&#x27;func_code_url&#x27;</span></span><br><span class="line">    ],</span><br><span class="line">    num_rows: <span class="number">412178</span></span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>我们可以看到数据集将文档字符串与代码分开，并且有他们各自的标记化后的结果。</p>
<p>这里。 我们将只使用 <code>whole_func_string</code> 列来训练我们的标记器。 我们可以通过指定到 <code>train</code> 中的一部分来查看这些函数的一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="number">123456</span>][<span class="string">&quot;whole_func_string&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>应该打印以下内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">handle_simple_responses</span>(<span class="params"></span></span><br><span class="line"><span class="params">      self, timeout_ms=<span class="literal">None</span>, info_cb=DEFAULT_MESSAGE_CALLBACK</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Accepts normal responses from the device.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      timeout_ms: Timeout in milliseconds to wait for each response.</span></span><br><span class="line"><span class="string">      info_cb: Optional callback for text sent from the bootloader.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      OKAY packet&#x27;s message.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="variable language_">self</span>._accept_responses(<span class="string">&#x27;OKAY&#x27;</span>, info_cb, timeout_ms=timeout_ms)</span><br></pre></td></tr></table></figure>
<p><font color="red">我们需要做的第一件事是将数据集转换为迭代器文本列表 - 例如，文本列表。</font>使用文本列表将使我们的标记器运行得更快（训练成批文本而不是一个接一个地处理单个文本），如果我们想避免一次将所有内容都放在内存中，它应该是一个迭代器。<font color="red">如果你的语料库很大，你会想要利用这样一个特性：Datasets不会将所有内容都加载到 RAM 中，而是将数据集的元素存储在磁盘上。</font></p>
<p><font size="5">创建一个包含1,000个文本的列表</font></p>
<p><font color="red">执行以下操作将创建一个包含 1,000 个文本的列表的列表，但会将所有内容加载到内存中：</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Don&#x27;t uncomment the following line unless your dataset is small!</span></span><br><span class="line">training_corpus = [raw_datasets[<span class="string">&quot;train&quot;</span>][i: i + <span class="number">1000</span>][<span class="string">&quot;whole_func_string&quot;</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(raw_datasets[<span class="string">&quot;train&quot;</span>]), <span class="number">1000</span>)]</span><br></pre></td></tr></table></figure>
<p><font color="red">使用 Python 生成器，我们可以避免 Python 将任何内容加载到内存中，直到真正需要为止。</font>要创建这样的生成器，您只需要将括号替换为圆括号：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">training_corpus = (</span><br><span class="line">    raw_datasets[<span class="string">&quot;train&quot;</span>][i : i + <span class="number">1000</span>][<span class="string">&quot;whole_func_string&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(raw_datasets[<span class="string">&quot;train&quot;</span>]), <span class="number">1000</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><font color="red">这行代码不会获取数据集的任何元素；它只是创建了一个可以在 Python 中使用的对象<strong>for</strong>环形。文本只会在您需要时加载（即，当您处于<strong>for</strong>需要它们的循环），并且一次只会加载 1,000 个文本。这样，即使您正在处理庞大的数据集，也不会耗尽所有内存。</font></p>
<p>生成器对象的问题在于它只能使用一次，每次访问它将给出下一个值。 下面是一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gen = (i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(gen))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(gen))</span><br></pre></td></tr></table></figure>
<p>我们第一次得到了这个列表，然后是一个空列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line">[]python</span><br></pre></td></tr></table></figure>
<p><strong>方法1：</strong></p>
<p>这就是我们定义一个返回生成器的函数的原因：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        raw_datasets[<span class="string">&quot;train&quot;</span>][i : i + <span class="number">1000</span>][<span class="string">&quot;whole_func_string&quot;</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(raw_datasets[<span class="string">&quot;train&quot;</span>]), <span class="number">1000</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_corpus = get_training_corpus()</span><br></pre></td></tr></table></figure>
<p><strong>方法2：</strong></p>
<p>您还可以在一个 <strong>for</strong> 循环内部使用 <strong>yield</strong> 关键字定义您的生成器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_training_corpus</span>():</span><br><span class="line">    dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), <span class="number">1000</span>):</span><br><span class="line">        samples = dataset[start_idx : start_idx + <span class="number">1000</span>]</span><br><span class="line">        <span class="keyword">yield</span> samples[<span class="string">&quot;whole_func_string&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>这将产生与以前完全相同的生成器，但允许您使用比列表生成式中更复杂的逻辑。</p>
<h3 id="2训练一个新的tokenizer"><a class="markdownIt-Anchor" href="#2训练一个新的tokenizer"></a> 2.训练一个新的Tokenizer</h3>
<blockquote>
<p><font color="red">这一节是在已有Tokenizer的基础上，利用新的语料库训练一个新的Tokenizer，这代表Tokenizer的分割算法并没有变。</font></p>
</blockquote>
<p>现在我们的语料库是文本批量迭代器的形式，我们准备训练一个新的标记器。为此，我们首先需要加载要与模型配对的标记器（此处为GPT-2）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">old_tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font color="red">即使我们要训练一个新的标记器，最好还是这样做以避免完全从头开始。这样，我们就不必指定任何关于标记化算法或我们想要使用的特殊标记；我们的新标记器将与 GPT-2 完全相同，唯一会改变的是输入的数据，这将取决于我们训练的语料。</font></p>
<p>首先让我们看看这个标记器将如何处理示例的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">example = <span class="string">&#x27;&#x27;&#x27;def add_numbers(a, b):</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;Add the two numbers `a` and `b`.&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    return a + b&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">tokens = old_tokenizer.tokenize(example)</span><br><span class="line">tokens</span><br><span class="line">[<span class="string">&#x27;def&#x27;</span>, <span class="string">&#x27;Ġadd&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;umbers&#x27;</span>, <span class="string">&#x27;(&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;Ġb&#x27;</span>, <span class="string">&#x27;):&#x27;</span>, <span class="string">&#x27;Ċ&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġ&quot;&quot;&quot;&#x27;</span>, <span class="string">&#x27;Add&#x27;</span>, <span class="string">&#x27;Ġthe&#x27;</span>, <span class="string">&#x27;Ġtwo&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Ġnumbers&#x27;</span>, <span class="string">&#x27;Ġ`&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;`&#x27;</span>, <span class="string">&#x27;Ġand&#x27;</span>, <span class="string">&#x27;Ġ`&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;`&#x27;</span>, <span class="string">&#x27;.&quot;&#x27;</span>, <span class="string">&#x27;&quot;&quot;&#x27;</span>, <span class="string">&#x27;Ċ&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġreturn&#x27;</span>, <span class="string">&#x27;Ġa&#x27;</span>, <span class="string">&#x27;Ġ+&#x27;</span>, <span class="string">&#x27;Ġb&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>这个标记器有一些特殊的符号，比如 <strong>Ċ</strong> 和 <strong>Ġ</strong> ，分别表示空格和换行符。正如我们所看到的，这不是太有效：标记器为每个空格返回单独的标记，当它可以将缩进级别组合在一起时（因为在代码中具有四个或八个空格的集合将非常普遍）。它也有点奇怪地拆分了函数名称，而习惯使用**_**的函数命名的方法。</p>
<p>让我们训练一个新的标记器，看看它是否能解决这些问题。为此，我们将使用 <strong>train_new_from_iterator()</strong> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, <span class="number">52000</span>)</span><br></pre></td></tr></table></figure>
<p>如果您的语料库非常大，此命令可能需要一些时间，但对于这个 1.6 GB 文本数据集，它的速度非常快（在具有 12 个内核的 AMD Ryzen 9 3900X CPU 上为 1 分 16 秒）。</p>
<blockquote>
<p><font color="red">注意**AutoTokenizer.train_new_from_iterator()**仅当您使用的标记器是“快速（fast）”标记器时才有效。</font>正如您将在下一节中看到的，🤗 Transformers 库包含两种类型的标记器：一些完全用 Python 编写，而另一些（快速的）由 🤗 Tokenizers 库支持，该库用<a target="_blank" rel="noopener" href="https://www.rust-lang.org/">Rust</a>编程语言编写。 Python 是最常用于数据科学和深度学习应用程序的语言，但是当需要并行化以提高速度时，必须用另一种语言编写。例如，模型计算核心的矩阵乘法是用 CUDA 编写的，CUDA 是一个针对 GPU 的优化 C 库。</p>
<p>用纯 Python 训练一个全新的标记器会非常缓慢，这就是我们开发 🤗 Tokenizers库的原因。请注意，正如您无需学习 CUDA 语言即可在 GPU 上执行您的模型一样，您也无需学习 Rust 即可使用快速标记器。 🤗 Tokenizers 库为许多内部调用 Rust 代码的方法提供 Python 绑定；例如，并行化新标记器的训练，或者，正如我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter3">第三章</a>中看到的，对一批输入进行标记化。</p>
</blockquote>
<p><font color="red">大多数 Transformer 模型都有可用的快速标记器（您可以</font><a target="_blank" rel="noopener" href="https://huggingface.co/transformers/#supported-frameworks">在这里</a><font color="red">检查一些例外情况)，如果 <strong>AutoTokenizer</strong> 可用，API 总是为您选择快速标记器。</font>在下一节中，我们将看看快速标记器具有的其他一些特殊功能，这些功能对于标记分类和问答等任务非常有用。然而，在深入研究之前，让我们在上一个示例中尝试我们全新的标记器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenizer.tokenize(example)</span><br><span class="line">tokens</span><br><span class="line">[<span class="string">&#x27;def&#x27;</span>, <span class="string">&#x27;Ġadd&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;numbers&#x27;</span>, <span class="string">&#x27;(&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;Ġb&#x27;</span>, <span class="string">&#x27;):&#x27;</span>, <span class="string">&#x27;ĊĠĠĠ&#x27;</span>, <span class="string">&#x27;Ġ&quot;&quot;&quot;&#x27;</span>, <span class="string">&#x27;Add&#x27;</span>, <span class="string">&#x27;Ġthe&#x27;</span>, <span class="string">&#x27;Ġtwo&#x27;</span>, <span class="string">&#x27;Ġnumbers&#x27;</span>, <span class="string">&#x27;Ġ`&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;`&#x27;</span>, <span class="string">&#x27;Ġand&#x27;</span>, <span class="string">&#x27;Ġ`&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;`.&quot;&quot;&quot;&#x27;</span>, <span class="string">&#x27;ĊĠĠĠ&#x27;</span>, <span class="string">&#x27;Ġreturn&#x27;</span>, <span class="string">&#x27;Ġa&#x27;</span>, <span class="string">&#x27;Ġ+&#x27;</span>, <span class="string">&#x27;Ġb&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p><font color="red">在这里我们再次看到特殊符号<strong>Ċ</strong>和<strong>Ġ</strong>表示空格和换行符，但我们也可以看到我们的标记器学习了一些高度特定于 Python 函数语料库的标记：例如，有一个<strong>ĊĠĠĠ</strong>表示缩进的标记，以及<strong>Ġ</strong>表示开始文档字符串的三个引号的标记</font>。标记器还正确使用**_**命名的规范将函数名称拆分为。这是一个非常紧凑的表示；相比之下，在同一个例子中使用简单的英语标记器会给我们一个更长的句子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(tokens))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(old_tokenizer.tokenize(example)))</span><br><span class="line"><span class="number">27</span></span><br><span class="line"><span class="number">36</span></span><br></pre></td></tr></table></figure>
<p>让我们再看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">example = <span class="string">&quot;&quot;&quot;class LinearLayer():</span></span><br><span class="line"><span class="string">    def __init__(self, input_size, output_size):</span></span><br><span class="line"><span class="string">        self.weight = torch.randn(input_size, output_size)</span></span><br><span class="line"><span class="string">        self.bias = torch.zeros(output_size)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __call__(self, x):</span></span><br><span class="line"><span class="string">        return x @ self.weights + self.bias</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">tokenizer.tokenize(example)</span><br><span class="line">[<span class="string">&#x27;class&#x27;</span>, <span class="string">&#x27;ĠLinear&#x27;</span>, <span class="string">&#x27;Layer&#x27;</span>, <span class="string">&#x27;():&#x27;</span>, <span class="string">&#x27;ĊĠĠĠ&#x27;</span>, <span class="string">&#x27;Ġdef&#x27;</span>, <span class="string">&#x27;Ġ__&#x27;</span>, <span class="string">&#x27;init&#x27;</span>, <span class="string">&#x27;__(&#x27;</span>, <span class="string">&#x27;self&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;Ġinput&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;,&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Ġoutput&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;):&#x27;</span>, <span class="string">&#x27;ĊĠĠĠĠĠĠĠ&#x27;</span>, <span class="string">&#x27;Ġself&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;Ġ=&#x27;</span>, <span class="string">&#x27;Ġtorch&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;randn&#x27;</span>, <span class="string">&#x27;(&#x27;</span>, <span class="string">&#x27;input&#x27;</span>, <span class="string">&#x27;_&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;Ġoutput&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;)&#x27;</span>, <span class="string">&#x27;ĊĠĠĠĠĠĠĠ&#x27;</span>, <span class="string">&#x27;Ġself&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;bias&#x27;</span>, <span class="string">&#x27;Ġ=&#x27;</span>, <span class="string">&#x27;Ġtorch&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;zeros&#x27;</span>, <span class="string">&#x27;(&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;output&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, <span class="string">&#x27;size&#x27;</span>, <span class="string">&#x27;)&#x27;</span>, <span class="string">&#x27;ĊĊĠĠĠ&#x27;</span>, <span class="string">&#x27;Ġdef&#x27;</span>, <span class="string">&#x27;Ġ__&#x27;</span>, <span class="string">&#x27;call&#x27;</span>, <span class="string">&#x27;__(&#x27;</span>, <span class="string">&#x27;self&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;Ġx&#x27;</span>, <span class="string">&#x27;):&#x27;</span>, <span class="string">&#x27;ĊĠĠĠĠĠĠĠ&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Ġreturn&#x27;</span>, <span class="string">&#x27;Ġx&#x27;</span>, <span class="string">&#x27;Ġ@&#x27;</span>, <span class="string">&#x27;Ġself&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;weights&#x27;</span>, <span class="string">&#x27;Ġ+&#x27;</span>, <span class="string">&#x27;Ġself&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;bias&#x27;</span>, <span class="string">&#x27;ĊĠĠĠĠ&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>除了一个缩进对应的token，这里我们还可以看到一个<strong>双缩进的token：</strong> <strong>ĊĠĠĠĠĠĠĠ</strong> .特殊的 Python 词如 <strong>class</strong> , <strong>init</strong> , <strong>call</strong> , <strong>self</strong> ， 和 <strong>return</strong> 每个都被标记为一个标记，我们可以看到，以及分裂 <strong>_</strong> 和 <strong>.</strong> 标记器甚至可以正确拆分驼峰式名称： <strong>LinearLayer</strong> 被标记为 <strong>[ĠLinear, Layer]</strong> .</p>
<h3 id="3保存tokenizer"><a class="markdownIt-Anchor" href="#3保存tokenizer"></a> 3.保存Tokenizer</h3>
<p>为了确保我们以后可以使用它，我们需要保存我们的新标记器。就像模型一样，是通过 <strong>save_pretrained()</strong> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.save_pretrained(<span class="string">&quot;code-search-net-tokenizer&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将创建一个名为的<em><strong>code-search-net-tokenizer</strong></em>的新文件夹，它将包含重新加载标记器所需要的所有文件。如果您想与您的同事和朋友分享这个标记器，您可以通过登录您的帐户将其上传到 Hub。如果您在notebook上工作，有一个方便的功能可以帮助您：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> notebook_login</span><br><span class="line"></span><br><span class="line">notebook_login()</span><br></pre></td></tr></table></figure>
<p>这将显示一个小部件，您可以在其中输入您的 Hugging Face 登录凭据。如果您不是在notebook上工作，只需在终端中输入以下行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure>
<p>登录后，您可以通过执行以下命令来推送您的标记器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.push_to_hub(<span class="string">&quot;code-search-net-tokenizer&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这将在您的命名空间中创建一个名为<strong>code-search-net-tokenizer</strong>的新存储库 ，包含标记器文件。然后，您可以使用以下命令从任何地方加载标记器的 <strong>from_pretrained()</strong> 方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace &quot;huggingface-course&quot; below with your actual namespace to use your own tokenizer</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;huggingface-course/code-search-net-tokenizer&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="快速标记器的特殊能力"><a class="markdownIt-Anchor" href="#快速标记器的特殊能力"></a> 快速标记器的特殊能力</h2>
<p>慢速分词器是在 🤗 Transformers 库中用 Python 编写的，而快速版本是由 🤗 分词器提供的，它们是用 Rust 编写的。</p>
<blockquote>
<p>⚠️ 对单个句子进行分词时，您不会总是看到相同分词器的慢速和快速版本之间的速度差异。事实上，快速版本实际上可能更慢！只有同时对大量文本进行标记时，您才能清楚地看到差异。</p>
</blockquote>
<h3 id="1批量编码"><a class="markdownIt-Anchor" href="#1批量编码"></a> 1.批量编码</h3>
<p>分词器的输出不是简单的 Python 字典；我们得到的实际上是一个特殊的 <strong>BatchEncoding</strong> object。它是字典的子类（这就是为什么我们之前能够毫无问题地索引到该结果中的原因），但具有主要由快速标记器使用的附加方法。</p>
<p>除了它们的并行化能力之外，快速标记器的关键功能是它们始终跟踪最终标记来自的原始文本范围——我们称之为<strong>偏移映射.</strong></p>
<p>我们看一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br><span class="line">example = <span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span></span><br><span class="line">encoding = tokenizer(example)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(encoding))</span><br></pre></td></tr></table></figure>
<p>如前所述，我们得到一个 <strong>BatchEncoding</strong> 标记器输出中的对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p>由于 <strong>AutoTokenizer</strong> 类默认选择快速标记器，我们可以使用附加方法 this <strong>BatchEncoding</strong> 对象提供。</p>
<p><font size="5">检查我们的分词器是快的还是慢的。</font></p>
<p>**方法一：**我们可以检查 <strong>is_fast</strong> 的属性 <strong>tokenizer</strong> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.is_fast</span><br><span class="line"></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>**方法二：**检查我们的相同属性 <strong>encoding</strong> ：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoding.is_fast</span><br><span class="line"></span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p><font size="5">快速标记器使我们能够做什么</font></p>
<p><strong>1.首先，我们可以访问令牌而无需将 ID 转换回令牌：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">encoding.tokens()</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;My&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;##yl&#x27;</span>, <span class="string">&#x27;##va&#x27;</span>, <span class="string">&#x27;##in&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;I&#x27;</span>, <span class="string">&#x27;work&#x27;</span>, <span class="string">&#x27;at&#x27;</span>, <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;##gging&#x27;</span>, <span class="string">&#x27;Face&#x27;</span>, <span class="string">&#x27;in&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>在这种情况下，索引 5 处的令牌是 <strong>##yl</strong> ，它是原始句子中“Sylvain”一词的一部分。</p>
<p><strong>2.我们也可以使用 word_ids() 获取每个标记来自的单词索引的方法：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoding.word_ids()</span><br><span class="line"></span><br><span class="line">[<span class="literal">None</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="literal">None</span>]</span><br></pre></td></tr></table></figure>
<p>我们可以看到分词器的特殊标记 <strong>[CLS]</strong> 和 <strong>[SEP]</strong> 被映射到 <strong>None</strong> ，然后每个标记都映射到它起源的单词。这对于确定一个标记是否在单词的开头或两个标记是否在同一个单词中特别有用。我们可以依靠 <strong>##</strong> 前缀，但它仅适用于类似 BERT 的分词器；<font color="red">这种方法适用于任何类型的标记器，只要它是快速的。</font></p>
<p><font color="red">一个词是什么的概念很复杂。例如，“I’ll”（“I will”的缩写）算一两个词吗？它实际上取决于分词器和它应用的预分词操作。一些标记器只是在空格上拆分，因此他们会将其视为一个词。其他人在空格顶部使用标点符号，因此将其视为两个词。</font></p>
<p>同样，有一个 <strong>sentence_ids()</strong> 我们可以用来将标记映射到它来自的句子的方法（尽管在这种情况下， <strong>token_type_ids</strong> 分词器返回的信息可以为我们提供相同的信息）。</p>
<p><strong>3.最后，我们可以将任何单词或标记映射到原始文本中的字符。</strong></p>
<p>反之亦然，通过 <strong>word_to_chars()</strong> 或者 <strong>token_to_chars()</strong> 和 <strong>char_to_word()</strong> 或者 <strong>char_to_token()</strong> 方法。例如， <strong>word_ids()</strong> 方法告诉我们 <strong>##yl</strong> 是索引 3 处单词的一部分，但它是句子中的哪个单词？我们可以这样发现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start, end = encoding.word_to_chars(<span class="number">3</span>)</span><br><span class="line">example[start:end]</span><br><span class="line"></span><br><span class="line">Sylvain</span><br></pre></td></tr></table></figure>
<p>正如我们之前提到的，这一切都是由快速分词器跟踪每个标记来自<em>偏移量</em>列表的文本范围这一事实提供的。为了说明它们的用途，接下来我们将向您展示如何手动复制<code>token-classification</code>管道的结果。</p>
<h3 id="2token分类管道内部"><a class="markdownIt-Anchor" href="#2token分类管道内部"></a> 2.token分类管道内部</h3>
<h4 id="通过管道获得基本结果"><a class="markdownIt-Anchor" href="#通过管道获得基本结果"></a> 通过管道获得基本结果</h4>
<p>首先，让我们获取一个token分类管道，以便我们可以手动比较一些结果。默认使用的模型是<a target="_blank" rel="noopener" href="https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english">dbmdz/bert-large-cased-finetuned-conll03-english</a>;它对句子执行 NER：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">token_classifier = pipeline(<span class="string">&quot;token-classification&quot;</span>)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line"></span><br><span class="line">[&#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9993828</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">12</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99815476</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##yl&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">14</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99590725</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##va&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">16</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9992327</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##in&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97389334</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">35</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.976115</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##gging&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">35</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">40</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.98879766</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">41</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99321055</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>该模型正确地将“Sylvain”生成的每个标记识别为<strong>一个人</strong>，将“Hugging Face”生成的每个标记识别为一个<strong>组织</strong>，将“Brooklyn”生成的标记识别为一个<strong>位置</strong>。</p>
<p><font size="5">将对应于同一实体的token组合在一起</font></p>
<p>我们还可以要求管道将对应于同一实体的token组合在一起：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">token_classifier = pipeline(<span class="string">&quot;token-classification&quot;</span>, aggregation_strategy=<span class="string">&quot;simple&quot;</span>)</span><br><span class="line">token_classifier(<span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)</span><br><span class="line">[&#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9981694</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Sylvain&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97960204</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hugging Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99321055</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>选择的<code>aggregation_strategy</code>将更改为每个分组实体计算的分数。对于<code>&quot;simple&quot;</code>分数只是给定实体中每个标记的分数的平均值：例如，“Sylvain”的分数是我们在前面的示例中看到的标记<code>S</code> 、 <code>##yl</code>分数的平均值、 <code>##va</code>和<code>##in</code> 。其他可用的策略有：</p>
<ul>
<li><code>&quot;first&quot;</code>, 其中每个实体的分数是该实体的第一个标记的分数（因此对于“Sylvain”，它将是 0.993828，标记的分数)</li>
<li><code>&quot;max&quot;</code>,其中每个实体的分数是该实体中标记的最大分数（因此对于“Hugging Face”，它将是 0.98879766，即“Face”的分数）</li>
<li><code>&quot;average&quot;</code>, 其中每个实体的分数是组成该实体的单词分数的平均值，其中每个实体的分数是组成该实体的单词分数的平均值（因此对于“Sylvain”，与<code>&quot;simple&quot;</code>策略没有区别，但“Hugging Face”的分数为0.9819，“拥抱”得分的平均值，0.975，“脸”得分的平均值，0.98879）</li>
</ul>
<p>现在让我们看看如何在不使用pipeline（）函数的情况下获得这些结果！</p>
<h4 id="从输入到预测"><a class="markdownIt-Anchor" href="#从输入到预测"></a> 从输入到预测</h4>
<p>首先，我们需要标记我们的输入并将其传递给模型。这是完全按照<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2">Chapter 2</a>;我们使用 <strong>AutoXxx</strong> 类，然后在我们的示例中使用它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForTokenClassification</span><br><span class="line"></span><br><span class="line">model_checkpoint = <span class="string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br><span class="line">model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)</span><br><span class="line"></span><br><span class="line">example = <span class="string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span></span><br><span class="line">inputs = tokenizer(example, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br></pre></td></tr></table></figure>
<p>由于我们正在使用 <strong>AutoModelForTokenClassification</strong> 在这里，我们为输入序列中的每个标记获得一组 logits：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;input_ids&quot;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(outputs.logits.shape)</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">19</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">19</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>
<p><font color="red">我们有一个包含 19 个标记的 1 个序列的批次，模型有 9 个不同的标签，因此模型的输出具有 1 x 19 x 9 的形状。</font>与文本分类管道一样，我们使用 softmax 函数来转换这些 logits到概率，我们采用 argmax 来获得预测（请注意，我们可以在 logits 上采用 argmax，因为 softmax 不会改变顺序）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">probabilities = torch.nn.functional.softmax(outputs.logits, dim=-<span class="number">1</span>)[<span class="number">0</span>].tolist() <span class="comment"># (19,9)</span></span><br><span class="line"></span><br><span class="line">predictions = outputs.logits.argmax(dim=-<span class="number">1</span>)[<span class="number">0</span>].tolist() <span class="comment">#(19,1)</span></span><br><span class="line"><span class="built_in">print</span>(predictions)</span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>model.config.id2label</strong> 属性包含索引到标签的映射，我们可以用它来理解预测：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model.config.id2label</span><br><span class="line">&#123;<span class="number">0</span>: <span class="string">&#x27;O&#x27;</span>,</span><br><span class="line"><span class="number">1</span>: <span class="string">&#x27;B-MISC&#x27;</span>,</span><br><span class="line"><span class="number">2</span>: <span class="string">&#x27;I-MISC&#x27;</span>,</span><br><span class="line"><span class="number">3</span>: <span class="string">&#x27;B-PER&#x27;</span>,</span><br><span class="line"><span class="number">4</span>: <span class="string">&#x27;I-PER&#x27;</span>,</span><br><span class="line"><span class="number">5</span>: <span class="string">&#x27;B-ORG&#x27;</span>,</span><br><span class="line"><span class="number">6</span>: <span class="string">&#x27;I-ORG&#x27;</span>,</span><br><span class="line"><span class="number">7</span>: <span class="string">&#x27;B-LOC&#x27;</span>,</span><br><span class="line"><span class="number">8</span>: <span class="string">&#x27;I-LOC&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>正如我们之前看到的，有 9 个标签： <strong>O</strong> 是不在任何命名实体中的标记的标签（它代表“外部”），然后我们为每种类型的实体（杂项、人员、组织和位置）提供两个标签。标签 <strong>B-XXX</strong> 表示令牌在实体的开头 <strong>XXX</strong> 和标签 <strong>I-XXX</strong> 表示令牌在实体内 <strong>XXX</strong> 。<font color="red">例如，在当前示例中，我们希望我们的模型对令牌进行分类 <strong>S</strong> 作为 <strong>B-PER</strong> （一个人实体的开始）和令牌 <strong>##yl</strong> , <strong>##va</strong> 和 <strong>##in</strong> 作为 <strong>I-PER</strong> （在个人实体内）</font></p>
<p><font color="red">在这种情况下，您可能认为模型是错误的，因为它给出了标签 <strong>I-PER</strong> 对所有这四个令牌，但这并不完全正确。实际上有两种格式 <strong>B-</strong> 和 <strong>I-</strong> 标签：<strong>IOB1</strong>和<strong>IOB2</strong>。<strong>IOB2</strong> 格式（下面粉红色）是我们介绍的格式，而在 <strong>IOB1</strong> 格式（蓝色）中，标签以 <strong>B-</strong> 仅用于分隔相同类型的两个相邻实体。我们使用的模型在使用该格式的数据集上进行了微调，这就是它分配标签的原因 <strong>I-PER</strong> 到 <strong>S</strong> 令牌。</font></p>
<p><img src="en_chapter6_IOB_versions.svg" alt></p>
<p>有了这张地图，我们已经准备好（几乎完全）重现第一个管道的结果——我们可以获取每个未被归类为的标记的分数和标签 <strong>O</strong> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">tokens = inputs.tokens()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(predictions):</span><br><span class="line">    label = model.config.id2label[pred]</span><br><span class="line">    <span class="keyword">if</span> label != <span class="string">&quot;O&quot;</span>:</span><br><span class="line">        results.append(</span><br><span class="line">            &#123;<span class="string">&quot;entity&quot;</span>: label, <span class="string">&quot;score&quot;</span>: probabilities[idx][pred], <span class="string">&quot;index&quot;</span>:idx, word<span class="string">&quot;: tokens[idx]&#125;</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print(results)</span></span><br><span class="line"><span class="string">[&#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9993828, &#x27;index&#x27;: 4, &#x27;word&#x27;: &#x27;S&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99815476, &#x27;index&#x27;: 5, &#x27;word&#x27;: &#x27;##yl&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.99590725, &#x27;index&#x27;: 6, &#x27;word&#x27;: &#x27;##va&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-PER&#x27;, &#x27;score&#x27;: 0.9992327, &#x27;index&#x27;: 7, &#x27;word&#x27;: &#x27;##in&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.97389334, &#x27;index&#x27;: 12, &#x27;word&#x27;: &#x27;Hu&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.976115, &#x27;index&#x27;: 13, &#x27;word&#x27;: &#x27;##gging&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-ORG&#x27;, &#x27;score&#x27;: 0.98879766, &#x27;index&#x27;: 14, &#x27;word&#x27;: &#x27;Face&#x27;&#125;,</span></span><br><span class="line"><span class="string"> &#123;&#x27;entity&#x27;: &#x27;I-LOC&#x27;, &#x27;score&#x27;: 0.99321055, &#x27;index&#x27;: 16, &#x27;word&#x27;: &#x27;Brooklyn&#x27;&#125;]</span></span><br></pre></td></tr></table></figure>
<p>这与我们之前的情况非常相似，只有一个例外：管道还为我们提供了有关 <strong>start</strong> 和 <strong>end</strong> 原始句子中的每个实体。这是我们的偏移映射将发挥作用的地方。要获得偏移量，我们只需要设置 <strong>return_offsets_mapping=True</strong> 当我们将分词器应用于我们的输入时：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">[(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">2</span>), (<span class="number">3</span>, <span class="number">7</span>), (<span class="number">8</span>, <span class="number">10</span>), (<span class="number">11</span>, <span class="number">12</span>), (<span class="number">12</span>, <span class="number">14</span>), (<span class="number">14</span>, <span class="number">16</span>), (<span class="number">16</span>, <span class="number">18</span>), (<span class="number">19</span>, <span class="number">22</span>), (<span class="number">23</span>, <span class="number">24</span>), (<span class="number">25</span>, <span class="number">29</span>), (<span class="number">30</span>, <span class="number">32</span>),</span><br><span class="line"> (<span class="number">33</span>, <span class="number">35</span>), (<span class="number">35</span>, <span class="number">40</span>), (<span class="number">41</span>, <span class="number">45</span>), (<span class="number">46</span>, <span class="number">48</span>), (<span class="number">49</span>, <span class="number">57</span>), (<span class="number">57</span>, <span class="number">58</span>), (<span class="number">0</span>, <span class="number">0</span>)]</span><br></pre></td></tr></table></figure>
<p>每个元组是对应于每个标记的文本跨度，其中 <strong>(0, 0)</strong> 保留用于特殊令牌。我们之前看到索引 5 处的令牌是 <strong>##yl</strong> ， 其中有 <strong>(12, 14)</strong> 作为这里的抵消。如果我们在示例中抓取相应的切片：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example[<span class="number">12</span>:<span class="number">14</span>]</span><br></pre></td></tr></table></figure>
<p>我们得到了正确的文本跨度，而没有 <strong>##</strong> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yl</span><br></pre></td></tr></table></figure>
<p>使用这个，我们现在可以完成之前的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">results = []</span><br><span class="line">inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">tokens = inputs_with_offsets.tokens()</span><br><span class="line">offsets = inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, pred <span class="keyword">in</span> <span class="built_in">enumerate</span>(predictions):</span><br><span class="line">    label = model.config.id2label[pred]</span><br><span class="line">    <span class="keyword">if</span> label != <span class="string">&quot;O&quot;</span>:</span><br><span class="line">        start, end = offsets[idx]</span><br><span class="line">        results.append(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;entity&quot;</span>: label,</span><br><span class="line">                <span class="string">&quot;score&quot;</span>: probabilities[idx][pred],</span><br><span class="line">                <span class="string">&quot;word&quot;</span>: tokens[idx],</span><br><span class="line">                <span class="string">&quot;start&quot;</span>: start,</span><br><span class="line">                <span class="string">&quot;end&quot;</span>: end,</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(results)</span><br><span class="line">[&#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9993828</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;S&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">12</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99815476</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##yl&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">14</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99590725</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##va&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">16</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9992327</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##in&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97389334</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">12</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">35</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.976115</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">13</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;##gging&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">35</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">40</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.98879766</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">14</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">41</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity&#x27;</span>: <span class="string">&#x27;I-LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99321055</span>, <span class="string">&#x27;index&#x27;</span>: <span class="number">16</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>这和我们从第一个管道中得到的一样！</p>
<h4 id="分组实体"><a class="markdownIt-Anchor" href="#分组实体"></a> 分组实体</h4>
<p><font color="red">使用偏移量来确定每个实体的开始和结束键很方便，但该信息并不是绝对必要的。然而，当我们想要将实体组合在一起时，偏移量将为我们节省大量混乱的代码。</font>如，如果我们想要将标记<code>Hu</code> 、 <code>##gging</code>和<code>Face</code>组合在一起，我们可以制定特殊规则，规定在删除<code>##</code>时应附加前两个标记，并且应在<code>Face</code>上添加一个空格，因为它不以<code>##</code>开头 — 但这仅适用于这种特定类型的分词器。我们必须为 SentencePiece 或字节对编码分词器编写另一组规则（本章稍后讨论）。</p>
<p>有了偏移量，所有自定义代码都会消失：我们只需获取原始文本中从第一个标记开始到最后一个标记结束的范围。因此，对于标记<code>Hu</code> 、 <code>##gging</code>和<code>Face</code> ，我们应该从字符 33 （ <code>Hu</code>的开头）开始，并在字符 45 （ <code>Face</code>的结尾）之前结束：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">example[<span class="number">33</span>:<span class="number">45</span>]</span><br><span class="line"></span><br><span class="line">Hugging Face</span><br></pre></td></tr></table></figure>
<p>为了编写在对实体进行分组时对预测进行后处理的代码，我们将把连续并标记为<code>I-XXX</code>的实体分组在一起，第一个实体除外，它可以标记为<code>B-XXX</code>或<code>I-XXX</code> （因此，当我们得到<code>O</code> （一种新类型的实体）或<code>B-XXX</code> （告诉我们相同类型的实体正在启动）时，我们停止对实体进行分组）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line">inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">tokens = inputs_with_offsets.tokens()</span><br><span class="line">offsets = inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> idx &lt; <span class="built_in">len</span>(predictions):</span><br><span class="line">    pred = predictions[idx]</span><br><span class="line">    label = model.config.id2label[pred]</span><br><span class="line">    <span class="keyword">if</span> label != <span class="string">&quot;O&quot;</span>:</span><br><span class="line">        <span class="comment"># Remove the B- or I-</span></span><br><span class="line">        label = label[<span class="number">2</span>:]</span><br><span class="line">        start, _ = offsets[idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Grab all the tokens labeled with I-label</span></span><br><span class="line">        all_scores = []</span><br><span class="line">        <span class="keyword">while</span> (</span><br><span class="line">            idx &lt; <span class="built_in">len</span>(predictions)</span><br><span class="line">            <span class="keyword">and</span> model.config.id2label[predictions[idx]] == <span class="string">f&quot;I-<span class="subst">&#123;label&#125;</span>&quot;</span></span><br><span class="line">        ):</span><br><span class="line">            all_scores.append(probabilities[idx][pred])</span><br><span class="line">            _, end = offsets[idx]</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># The score is the mean of all the scores of the tokens in that grouped entity</span></span><br><span class="line">        score = np.mean(all_scores).item()</span><br><span class="line">        word = example[start:end]</span><br><span class="line">        results.append(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;entity_group&quot;</span>: label,</span><br><span class="line">                <span class="string">&quot;score&quot;</span>: score,</span><br><span class="line">                <span class="string">&quot;word&quot;</span>: word,</span><br><span class="line">                <span class="string">&quot;start&quot;</span>: start,</span><br><span class="line">                <span class="string">&quot;end&quot;</span>: end,</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line">    idx += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(results)</span><br></pre></td></tr></table></figure>
<p>我们得到了与第二条管道相同的结果！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;PER&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9981694</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Sylvain&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">18</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;ORG&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97960204</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Hugging Face&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">33</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">45</span>&#125;,</span><br><span class="line"> &#123;<span class="string">&#x27;entity_group&#x27;</span>: <span class="string">&#x27;LOC&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.99321055</span>, <span class="string">&#x27;word&#x27;</span>: <span class="string">&#x27;Brooklyn&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">49</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">57</span>&#125;]</span><br></pre></td></tr></table></figure>
<p>这些偏移量非常有用的另一个任务示例是问答。深入研究这个管道，我们将在下一节中进行，也将使我们能够了解 🤗 Transformers 库中标记器的最后一个功能：<font color="red">当我们将输入截断为给定长度时处理溢出的标记。</font></p>
<h2 id="qa-管道中的快速标记器"><a class="markdownIt-Anchor" href="#qa-管道中的快速标记器"></a> QA 管道中的快速标记器</h2>
<h3 id="1-使用问答管道"><a class="markdownIt-Anchor" href="#1-使用问答管道"></a> 1. 使用问答管道</h3>
<p>正如我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1">第 1 章</a>中看到的，我们可以使用这样的<code>question-answering</code>管道来获取问题的答案：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">question_answerer = pipeline(<span class="string">&quot;question-answering&quot;</span>)</span><br><span class="line">context = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration</span></span><br><span class="line"><span class="string">between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">question = <span class="string">&quot;Which deep learning libraries back 🤗 Transformers?&quot;</span></span><br><span class="line">question_answerer(question=question, context=context)</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.97773</span>,</span><br><span class="line"> <span class="string">&#x27;start&#x27;</span>: <span class="number">78</span>,</span><br><span class="line"> <span class="string">&#x27;end&#x27;</span>: <span class="number">105</span>,</span><br><span class="line"> <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p><font color="red">与其他管道不同，其他管道无法截断和分割长于模型接受的最大长度的文本（因此可能会丢失文档末尾的信息），该管道可以处理非常长的上下文，并将返回回答问题，即使它在最后：</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">long_context = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">🤗 Transformers: State of the Art NLP</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,</span></span><br><span class="line"><span class="string">question answering, summarization, translation, text generation and more in over 100 languages.</span></span><br><span class="line"><span class="string">Its aim is to make cutting-edge NLP easier to use for everyone.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and</span></span><br><span class="line"><span class="string">then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and</span></span><br><span class="line"><span class="string">can be modified to enable quick research experiments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Why should I use transformers?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. Easy-to-use state-of-the-art models:</span></span><br><span class="line"><span class="string">  - High performance on NLU and NLG tasks.</span></span><br><span class="line"><span class="string">  - Low barrier to entry for educators and practitioners.</span></span><br><span class="line"><span class="string">  - Few user-facing abstractions with just three classes to learn.</span></span><br><span class="line"><span class="string">  - A unified API for using all our pretrained models.</span></span><br><span class="line"><span class="string">  - Lower compute costs, smaller carbon footprint:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. Researchers can share trained models instead of always retraining.</span></span><br><span class="line"><span class="string">  - Practitioners can reduce compute time and production costs.</span></span><br><span class="line"><span class="string">  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. Choose the right framework for every part of a model&#x27;s lifetime:</span></span><br><span class="line"><span class="string">  - Train state-of-the-art models in 3 lines of code.</span></span><br><span class="line"><span class="string">  - Move a single model between TF2.0/PyTorch frameworks at will.</span></span><br><span class="line"><span class="string">  - Seamlessly pick the right framework for training, evaluation and production.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">4. Easily customize a model or an example to your needs:</span></span><br><span class="line"><span class="string">  - We provide examples for each architecture to reproduce the results published by its original authors.</span></span><br><span class="line"><span class="string">  - Model internals are exposed as consistently as possible.</span></span><br><span class="line"><span class="string">  - Model files can be used independently of the library for quick experiments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration</span></span><br><span class="line"><span class="string">between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">question_answerer(question=question, context=long_context)</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.97149</span>,</span><br><span class="line"> <span class="string">&#x27;start&#x27;</span>: <span class="number">1892</span>,</span><br><span class="line"> <span class="string">&#x27;end&#x27;</span>: <span class="number">1919</span>,</span><br><span class="line"> <span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2使用模型进行问答"><a class="markdownIt-Anchor" href="#2使用模型进行问答"></a> 2.使用模型进行问答</h3>
<p>与任何其他管道一样，我们首先对输入进行标记，然后通过模型发送它。 <code>question-answering</code>管道默认使用的检查点是 <a target="_blank" rel="noopener" href="https://huggingface.co/distilbert-base-cased-distilled-squad">distilbert-base-cased-distilled-squad</a> （名称中的“squad”来自模型微调的数据集；我们将在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter7/7">第 7 章</a>中详细讨论 SQuAD 数据集）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForQuestionAnswering</span><br><span class="line"></span><br><span class="line">model_checkpoint = <span class="string">&quot;distilbert-base-cased-distilled-squad&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</span><br><span class="line">model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(question, context, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">outputs = model(**inputs)</span><br></pre></td></tr></table></figure>
<p><font size="5">屏蔽了与我们不想预测的位置相对应的 logits</font></p>
<p>请注意，我们将问题和上下文标记为一对，首先是问题。</p>
<p><img src="en_chapter6_question_tokens.svg" alt></p>
<p>问答模型的工作原理与我们迄今为止看到的模型略有不同。<font color="red">以上图为例，模型经过训练可以预测答案开始的token索引（此处为 21）和答案结束处的标记索引（此处为 24）。这就是为什么这些模型不返回一个 logits 张量，而是返回两个：一个用于对应于答案的开始标记的 logits，另一个用于对应于答案的结束标记的 logits。</font>由于在本例中我们只有一个包含 66 个标记的输入，因此我们得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_logits = outputs.start_logits</span><br><span class="line">end_logits = outputs.end_logits</span><br><span class="line"><span class="built_in">print</span>(start_logits.shape, end_logits.shape)</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">66</span>]) torch.Size([<span class="number">1</span>, <span class="number">66</span>])</span><br></pre></td></tr></table></figure>
<p><font color="red">为了将这些 logits 转换为概率，我们将应用一个 softmax 函数 - 但在此之前，我们需要确保屏蔽不属于上下文的索引。我们的输入是 <code>[CLS] question [SEP] context [SEP]</code> ，所以我们需要屏蔽问题的标记以及<code>[SEP]</code>标记。但是，我们将保留<code>[CLS]</code>标记，因为某些模型使用它来指示答案不在上下文中。</font></p>
<p><font color="red">由于我们随后将应用 softmax，因此我们只需要将要屏蔽的 logits 替换为一个大的负数即可。</font>在这里，我们使用<code>-10000</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">sequence_ids = inputs.sequence_ids() <span class="comment"># 表示与每个标记关联的输入序列的索引,[CLS]与[SEP]的索引是None</span></span><br><span class="line"><span class="comment"># Mask everything apart from the tokens of the context</span></span><br><span class="line">mask = [i != <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> sequence_ids]</span><br><span class="line"><span class="comment"># Unmask the [CLS] token</span></span><br><span class="line">mask[<span class="number">0</span>] = <span class="literal">False</span></span><br><span class="line">mask = torch.tensor(mask)[<span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">start_logits[mask] = -<span class="number">10000</span></span><br><span class="line">end_logits[mask] = -<span class="number">10000</span></span><br></pre></td></tr></table></figure>
<p>现在我们已经正确屏蔽了与我们不想预测的位置相对应的 logits，我们可以应用 softmax：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><font color="red">在这个阶段，我们可以采用开始和结束概率的 argmax，但我们最终可能会得到一个大于结束索引的开始索引，因此我们需要采取更多预防措施:</font>我们将计算每个可能的<code>start_index</code>和<code>end_index</code>的概率，其中<code>start_index &lt;= end_index</code> ，然后采用概率最高的元组<code>(start_index, end_index)</code> 。</p>
<blockquote>
<p>假设事件“答案从<code>start_index</code>开始”和“答案在<code>end_index</code>结束”是独立的，则答案从<code>start_index</code>开始并在<code>end_index</code>结束的概率为：</p>
<center>start_probabilities[start_index]×end_probabilities[end_index]</center>
<p>因此，要计算所有分数，我们只需要计算所有乘积**start_probabilities[start_index]×end_probabilities[end_index]**其中<code>start_index &lt;= end_index</code> 。</p>
</blockquote>
<p>首先让我们计算所有可能的产品：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = start_probabilities[:, <span class="literal">None</span>] * end_probabilities[<span class="literal">None</span>, :]</span><br></pre></td></tr></table></figure>
<p>然后我们将<code>start_index &gt; end_index</code>的值设置为<code>0</code> （其他概率均为正数）来屏蔽它们。 <code>torch.triu()</code>函数返回作为参数传递的 2D 张量的上三角部分，因此它将为我们进行掩蔽：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = torch.triu(scores)</span><br></pre></td></tr></table></figure>
<p>现在我们只需要获取最大值的索引即可。由于 PyTorch 将返回展平张量中的索引，因此我们需要使用向下除法<code>//</code>和模<code>%</code>运算来获取<code>start_index</code>和<code>end_index</code> ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">max_index = scores.argmax().item()</span><br><span class="line">start_index = max_index // scores.shape[<span class="number">1</span>]</span><br><span class="line">end_index = max_index % scores.shape[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(scores[start_index, end_index])</span><br></pre></td></tr></table></figure>
<p>我们还没有完全完成，但至少我们已经有了答案的正确分数（您可以通过将其与上一节中的第一个结果进行比较来检查这一点）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.97773</span></span><br></pre></td></tr></table></figure>
<p>我们有以标记表示的答案的<code>start_index</code>和<code>end_index</code> ，所以现在我们只需要转换为上下文中的字符索引。这就是偏移量非常有用的地方。我们可以像在令牌分类任务中那样获取并使用它们：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">offsets = inputs_with_offsets[<span class="string">&quot;offset_mapping&quot;</span>]</span><br><span class="line"></span><br><span class="line">start_char, _ = offsets[start_index]</span><br><span class="line">_, end_char = offsets[end_index]</span><br><span class="line">answer = context[start_char:end_char]</span><br></pre></td></tr></table></figure>
<p>现在我们只需格式化所有内容即可获得结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">result = &#123;</span><br><span class="line">    <span class="string">&quot;answer&quot;</span>: answer,</span><br><span class="line">    <span class="string">&quot;start&quot;</span>: start_char,</span><br><span class="line">    <span class="string">&quot;end&quot;</span>: end_char,</span><br><span class="line">    <span class="string">&quot;score&quot;</span>: scores[start_index, end_index],</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">&#123;<span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;start&#x27;</span>: <span class="number">78</span>,</span><br><span class="line"> <span class="string">&#x27;end&#x27;</span>: <span class="number">105</span>,</span><br><span class="line"> <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97773</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3处理长上下文"><a class="markdownIt-Anchor" href="#3处理长上下文"></a> 3.处理长上下文</h3>
<p><font size="5">以最大长度截断输入</font></p>
<p>如果我们尝试对之前用作示例的问题和长上下文进行标记，我们将获得比<code>question-answering</code>管道中使用的最大长度更高的标记数量（即 384）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(question, long_context)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(inputs[<span class="string">&quot;input_ids&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="number">461</span></span><br></pre></td></tr></table></figure>
<p>因此，我们需要以最大长度截断输入。我们可以通过多种方法来做到这一点，但我们不想截断问题，而只想截断上下文。由于上下文是第二个句子，我们将使用<code>&quot;only_second&quot;</code>截断策略。<font color="red">那么出现的问题是问题的答案可能不在被截断的上下文中。</font>例如，在这里，我们选择了一个问题，其答案位于上下文末尾，而当我们截断它时，答案不存在：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(question, long_context, max_length=<span class="number">384</span>, truncation=<span class="string">&quot;only_second&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(inputs[<span class="string">&quot;input_ids&quot;</span>]))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,</span></span><br><span class="line"><span class="string">question answering, summarization, translation, text generation and more in over 100 languages.</span></span><br><span class="line"><span class="string">Its aim is to make cutting-edge NLP easier to use for everyone.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and</span></span><br><span class="line"><span class="string">then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and</span></span><br><span class="line"><span class="string">can be modified to enable quick research experiments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Why should I use transformers?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. Easy-to-use state-of-the-art models:</span></span><br><span class="line"><span class="string">  - High performance on NLU and NLG tasks.</span></span><br><span class="line"><span class="string">  - Low barrier to entry for educators and practitioners.</span></span><br><span class="line"><span class="string">  - Few user-facing abstractions with just three classes to learn.</span></span><br><span class="line"><span class="string">  - A unified API for using all our pretrained models.</span></span><br><span class="line"><span class="string">  - Lower compute costs, smaller carbon footprint:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. Researchers can share trained models instead of always retraining.</span></span><br><span class="line"><span class="string">  - Practitioners can reduce compute time and production costs.</span></span><br><span class="line"><span class="string">  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. Choose the right framework for every part of a model&#x27;s lifetime:</span></span><br><span class="line"><span class="string">  - Train state-of-the-art models in 3 lines of code.</span></span><br><span class="line"><span class="string">  - Move a single model between TF2.0/PyTorch frameworks at will.</span></span><br><span class="line"><span class="string">  - Seamlessly pick the right framework for training, evaluation and production.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">4. Easily customize a model or an example to your needs:</span></span><br><span class="line"><span class="string">  - We provide examples for each architecture to reproduce the results published by its original authors.</span></span><br><span class="line"><span class="string">  - Model internal [SEP]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>这意味着模型将很难选择正确的答案。<font color="red"></font></p>
<p><font size="5">单个句子截断成多块</font></p>
<p><font color="red">为了解决这个问题， <code>question-answering</code>管道允许我们将上下文分割成更小的块，并指定最大长度。为了确保我们不会在错误的位置分割上下文以找到答案，它还包括块之间的一些重叠。</font></p>
<p>我们可以让分词器（快或慢）通过添加<code>return_overflowing_tokens=True</code> 来为我们做到这一点 ，我们可以用<code>stride</code>参数指定我们想要的重叠。这是一个使用较小句子的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sentence = <span class="string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span></span><br><span class="line">inputs = tokenizer(</span><br><span class="line">    sentence, truncation=<span class="literal">True</span>, return_overflowing_tokens=<span class="literal">True</span>, max_length=<span class="number">6</span>, stride=<span class="number">2</span> <span class="comment"># 最大长度为6，重叠的token为2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ids <span class="keyword">in</span> inputs[<span class="string">&quot;input_ids&quot;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(tokenizer.decode(ids))</span><br><span class="line"><span class="string">&#x27;[CLS] This sentence is not [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] is not too long [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] too long but we [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] but we are going [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] are going to split [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] to split it anyway [SEP]&#x27;</span></span><br><span class="line"><span class="string">&#x27;[CLS] it anyway. [SEP]&#x27;</span></span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，该句子已被分割成多个块，使得<code>inputs[&quot;input_ids&quot;]</code>中的每个条目最多有 6 个token（我们需要添加填充以使最后一个条目与其他条目的大小相同） ）并且每个条目之间有 2 个token重叠。</p>
<p>让我们仔细看看标记化的结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(inputs.keys())</span><br><span class="line"></span><br><span class="line">dict_keys([<span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;overflow_to_sample_mapping&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>正如预期的那样，我们得到了输入 ID 和注意力掩码。<font color="red">最后一个键， <code>overflow_to_sample_mapping</code> ，是一个映射，它告诉我们每个结果对应于哪个句子——这里我们有 7 个结果，它们都来自我们传递给分词器的（唯一的）句子：</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;overflow_to_sample_mapping&quot;</span>])</span><br><span class="line"></span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p><font size="5">多个句子截断成多块</font></p>
<p>当我们一起标记多个句子时，这更有用。例如，这个：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    <span class="string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This sentence is shorter but will still get split.&quot;</span>,</span><br><span class="line">]</span><br><span class="line">inputs = tokenizer(</span><br><span class="line">    sentences, truncation=<span class="literal">True</span>, return_overflowing_tokens=<span class="literal">True</span>, max_length=<span class="number">6</span>, stride=<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;overflow_to_sample_mapping&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>gets us: 让我们：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]</span><br></pre></td></tr></table></figure>
<p>这意味着第一个句子像以前一样被分成 7 个块，接下来的 4 个块来自第二个句子。</p>
<p><font size="5">综合:</font></p>
<p>现在让我们回到我们的长期背景。默认情况下<code>question-answering</code>管道使用的最大长度为 384，正如我们之前提到的，步长为 128，这对应于模型微调的方式 （您可以在调用时通过传递<code>max_seq_len</code>和<code>stride</code>参数来调整这些参数）管道）。因此，我们将在标记化时使用这些参数。我们还将添加填充（以具有相同长度的样本，以便我们可以构建张量）并询问偏移量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(</span><br><span class="line">    question,</span><br><span class="line">    long_context,</span><br><span class="line">    stride=<span class="number">128</span>,</span><br><span class="line">    max_length=<span class="number">384</span>,</span><br><span class="line">    padding=<span class="string">&quot;longest&quot;</span>,</span><br><span class="line">    truncation=<span class="string">&quot;only_second&quot;</span>,</span><br><span class="line">    return_overflowing_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_offsets_mapping=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这些<code>inputs</code>将包含模型期望的输入 ID 和注意力掩码，以及我们刚才讨论的偏移量和<code>overflow_to_sample_mapping</code> 。<font color="red">由于这两个不是模型使用的参数，因此在将其转换为张量之前，我们会将它们从<code>inputs</code>中弹出（并且我们不会存储映射，因为它在这里没有用：</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">_ = inputs.pop(<span class="string">&quot;overflow_to_sample_mapping&quot;</span>)</span><br><span class="line">offsets = inputs.pop(<span class="string">&quot;offset_mapping&quot;</span>)</span><br><span class="line"></span><br><span class="line">inputs = inputs.convert_to_tensors(<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(inputs[<span class="string">&quot;input_ids&quot;</span>].shape)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">384</span>])</span><br></pre></td></tr></table></figure>
<p>我们的长上下文被分成两部分，这意味着在它通过我们的模型之后，我们将有两组开始和结束逻辑：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(**inputs)</span><br><span class="line"></span><br><span class="line">start_logits = outputs.start_logits</span><br><span class="line">end_logits = outputs.end_logits</span><br><span class="line"><span class="built_in">print</span>(start_logits.shape, end_logits.shape)</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">384</span>]) torch.Size([<span class="number">2</span>, <span class="number">384</span>])</span><br></pre></td></tr></table></figure>
<p><font color="red">和之前一样，我们在采用 softmax 之前首先屏蔽掉不属于上下文的 token。我们还屏蔽所有填充标记（如注意力掩码所标记的）：</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sequence_ids = inputs.sequence_ids()</span><br><span class="line"><span class="comment"># Mask everything apart from the tokens of the context</span></span><br><span class="line">mask = [i != <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> sequence_ids]</span><br><span class="line"><span class="comment"># Unmask the [CLS] token</span></span><br><span class="line">mask[<span class="number">0</span>] = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Mask all the [PAD] tokens</span></span><br><span class="line">mask = torch.logical_or(torch.tensor(mask)[<span class="literal">None</span>], (inputs[<span class="string">&quot;attention_mask&quot;</span>] == <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">start_logits[mask] = -<span class="number">10000</span></span><br><span class="line">end_logits[mask] = -<span class="number">10000</span></span><br></pre></td></tr></table></figure>
<p>然后我们可以使用 softmax 将 logits 转换为概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="number">1</span>)</span><br><span class="line">end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>下一步与我们对小上下文所做的类似，但我们对两个块中的每一个都重复它。我们为所有可能的答案范围打分，然后取得分最高的范围：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">candidates = []</span><br><span class="line"><span class="keyword">for</span> start_probs, end_probs <span class="keyword">in</span> <span class="built_in">zip</span>(start_probabilities, end_probabilities):</span><br><span class="line">    scores = start_probs[:, <span class="literal">None</span>] * end_probs[<span class="literal">None</span>, :]</span><br><span class="line">    idx = torch.triu(scores).argmax().item()</span><br><span class="line"></span><br><span class="line">    start_idx = idx // scores.shape[<span class="number">1</span>]</span><br><span class="line">    end_idx = idx % scores.shape[<span class="number">1</span>]</span><br><span class="line">    score = scores[start_idx, end_idx].item()</span><br><span class="line">    candidates.append((start_idx, end_idx, score))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(candidates)</span><br><span class="line">[(<span class="number">0</span>, <span class="number">18</span>, <span class="number">0.33867</span>), (<span class="number">173</span>, <span class="number">184</span>, <span class="number">0.97149</span>)]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>torch.triu(input, diagonal=0, *,out=None) -&gt; Tensor</code></p>
<p>返回input 张量，对应对角线 (diagonal）取值的结果。 其余位置为0</p>
<p>参数:</p>
<ul>
<li>input (Tensor): 输入张量</li>
<li>diagonal (int,可选的) - 要考虑的对角线</li>
<li>out (Tensor, 可选的）- 输出张量</li>
</ul>
<p>参数 diagonal 控制要考虑的对角线。如果diagonal = 0，则保留主对角线之上和之上的所有元素。其余位置的元素为0。正值不包括主对角线上方的对角线，类似地，负值包括主对角线下方的对角线。主对角线是i∈[0,min{d1,d2}−1] 的索引集{(i,i)}，其中d1,d2 是矩阵的维度。</p>
</blockquote>
<p>这两个候选对应于模型能够在每个块中找到的最佳答案。该模型对第二部分中的正确答案更有信心（这是一个好兆头！）。现在我们只需将这两个标记范围映射到上下文中的字符范围（我们只需要映射第二个标记范围即可得到答案，但看看模型在第一个块中选择了什么是很有趣的）。</p>
<p>我们之前获取的<code>offsets</code>实际上是一个偏移量列表，每个文本块有一个列表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> candidate, offset <span class="keyword">in</span> <span class="built_in">zip</span>(candidates, offsets):</span><br><span class="line">    start_token, end_token, score = candidate</span><br><span class="line">    start_char, _ = offset[start_token]</span><br><span class="line">    _, end_char = offset[end_token]</span><br><span class="line">    answer = long_context[start_char:end_char]</span><br><span class="line">    result = &#123;<span class="string">&quot;answer&quot;</span>: answer, <span class="string">&quot;start&quot;</span>: start_char, <span class="string">&quot;end&quot;</span>: end_char, <span class="string">&quot;score&quot;</span>: score&#125;</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line">&#123;<span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;\n🤗 Transformers: State of the Art NLP&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">37</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.33867</span>&#125;</span><br><span class="line">&#123;<span class="string">&#x27;answer&#x27;</span>: <span class="string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>, <span class="string">&#x27;start&#x27;</span>: <span class="number">1892</span>, <span class="string">&#x27;end&#x27;</span>: <span class="number">1919</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.97149</span>&#125;</span><br></pre></td></tr></table></figure>
<p>如果我们忽略第一个结果，我们会得到与这个长上下文的管道相同的结果 - 耶！</p>
<h2 id="标准化和预标记化"><a class="markdownIt-Anchor" href="#标准化和预标记化"></a> 标准化和预标记化</h2>
<p>Transformer 模型中使用的三种最常见的子词标记化算法</p>
<ul>
<li>字节对编码 [BPE]</li>
<li>WordPiece</li>
<li>Unigram</li>
</ul>
<p>以下是标记化管道中步骤的高级概述：</p>
<p><img src="en_chapter6_tokenization_pipeline.svg" alt></p>
<h3 id="1normalization"><a class="markdownIt-Anchor" href="#1normalization"></a> 1.Normalization</h3>
<p><font color="red">规范化步骤涉及一些常规清理，例如删除不必要的空格、小写和/或删除重音符号。</font>如果您熟悉<a target="_blank" rel="noopener" href="http://www.unicode.org/reports/tr15/">Unicode 规范化</a>（例如 NFC 或 NFKC），这也是标记生成器可能应用的内容。</p>
<p>🤗 Transformers <code>tokenizer</code>有一个名为<code>backend_tokenizer</code>的属性，它提供对 🤗 Tokenizers 库中的底层 tokenizer 的访问：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rom transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tokenizer.backend_tokenizer))</span><br><span class="line"></span><br><span class="line">&lt;<span class="keyword">class</span> <span class="string">&#x27;tokenizers.Tokenizer&#x27;</span>&gt;</span><br></pre></td></tr></table></figure>
<p><code>tokenizer</code>对象的<code>normalizer</code>属性有一个<code>normalize_str()</code>方法，我们可以使用它来查看规范化是如何执行的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tokenizer.backend_tokenizer.normalizer.normalize_str(<span class="string">&quot;Héllò hôw are ü?&quot;</span>))</span><br><span class="line"><span class="string">&#x27;hello how are u?&#x27;</span></span><br></pre></td></tr></table></figure>
<p>在此示例中，由于我们选择了<code>bert-base-uncased</code>检查点，因此规范化应用了小写并删除了重音符号。</p>
<h3 id="2预标记化"><a class="markdownIt-Anchor" href="#2预标记化"></a> 2.预标记化</h3>
<p><font color="red">正如我们将在下一节中看到的，分词器不能直接在原始文本上进行训练。相反，我们首先需要将文本分割成小实体，例如单词。这就是预分词步骤的用武之地。</font>正如我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2">第 2 章</a>中看到的，基于单词的分词器可以简单地将原始文本拆分为空格和标点符号上的单词。这些单词将是标记器在训练期间可以学习的子标记的边界。</p>
<p>要查看快速分词器如何执行预分词化，我们可以使用<code>tokenizer</code>对象的<code>pre_tokenizer</code>属性的<code>pre_tokenize_str()</code>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;Hello&#x27;</span>, (<span class="number">0</span>, <span class="number">5</span>)), (<span class="string">&#x27;,&#x27;</span>, (<span class="number">5</span>, <span class="number">6</span>)), (<span class="string">&#x27;how&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)), (<span class="string">&#x27;are&#x27;</span>, (<span class="number">11</span>, <span class="number">14</span>)), (<span class="string">&#x27;you&#x27;</span>, (<span class="number">16</span>, <span class="number">19</span>)), (<span class="string">&#x27;?&#x27;</span>, (<span class="number">19</span>, <span class="number">20</span>))]</span><br></pre></td></tr></table></figure>
<p>请注意分词器如何跟踪偏移量，这就是它如何为我们提供上一节中使用的偏移量映射的方式。<font color="red">这里，分词器忽略两个空格，只用一个空格替换它们，但偏移量会在<code>are</code>和<code>you</code>之间跳转，以解释这一点。</font></p>
<p><font size="5">GPT-2分词器</font></p>
<p>由于我们使用的是 BERT 分词器，因此预分词涉及到空格和标点符号的分割。其他分词器对此步骤可以有不同的规则。例如，如果我们使用GPT-2分词器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>它也会根据空格和标点符号进行分割，但它会保留空格并用<code>Ġ</code>符号替换它们，这样如果我们解码标记，它就能恢复原始空格：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;Hello&#x27;</span>, (<span class="number">0</span>, <span class="number">5</span>)), (<span class="string">&#x27;,&#x27;</span>, (<span class="number">5</span>, <span class="number">6</span>)), (<span class="string">&#x27;Ġhow&#x27;</span>, (<span class="number">6</span>, <span class="number">10</span>)), (<span class="string">&#x27;Ġare&#x27;</span>, (<span class="number">10</span>, <span class="number">14</span>)), (<span class="string">&#x27;Ġ&#x27;</span>, (<span class="number">14</span>, <span class="number">15</span>)), (<span class="string">&#x27;Ġyou&#x27;</span>, (<span class="number">15</span>, <span class="number">19</span>)),</span><br><span class="line"> (<span class="string">&#x27;?&#x27;</span>, (<span class="number">19</span>, <span class="number">20</span>))]</span><br></pre></td></tr></table></figure>
<p>另请注意，与 BERT 分词器不同，此分词器不会忽略双空格。</p>
<p><font size="5">T5 分词器</font></p>
<p>最后一个例子，让我们看一下基于 SentencePiece 算法的 T5 分词器:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;t5-small&quot;</span>)</span><br><span class="line">tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(<span class="string">&quot;Hello, how are  you?&quot;</span>)</span><br><span class="line"></span><br><span class="line">[(<span class="string">&#x27;▁Hello,&#x27;</span>, (<span class="number">0</span>, <span class="number">6</span>)), (<span class="string">&#x27;▁how&#x27;</span>, (<span class="number">7</span>, <span class="number">10</span>)), (<span class="string">&#x27;▁are&#x27;</span>, (<span class="number">11</span>, <span class="number">14</span>)), (<span class="string">&#x27;▁you?&#x27;</span>, (<span class="number">16</span>, <span class="number">20</span>))]</span><br></pre></td></tr></table></figure>
<p>与 GPT-2 分词器一样，此分词器保留空格并将其替换为特定标记 ( <code>_</code> )，但 T5 分词器仅按空格而不是标点符号进行分割。另请注意，它默认在句子开头（ <code>Hello</code>之前）添加一个空格，并忽略<code>are</code>和<code>you</code>之间的双空格。</p>
<p>现在我们已经了解了一些不同的分词器如何处理文本，我们可以开始探索底层算法本身。我们首先快速浏览一下广泛适用的 SentencePiece；然后，在接下来的三个部分中，我们将研究用于子词标记化的三种主要算法是如何工作的。</p>
<h3 id="3sentencepiece"><a class="markdownIt-Anchor" href="#3sentencepiece"></a> 3.SentencePiece</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/google/sentencepiece">SentencePiece</a>是一种用于文本预处理的标记化算法，您可以将其与我们将在接下来的三节中看到的任何模型一起使用。它将文本视为 Unicode 字符序列，并用特殊字符▁替换空格与 Unigram 算法结合使用（参见<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter7/7">第 7 节</a>），它甚至不需要预标记化步骤，这对于不使用空格字符的语言（如中文或日语）非常有用。</p>
<p><font color="red">SentencePiece的另一个主要功能是<strong>可逆标记化</strong>：由于没有对空格进行特殊处理，因此只需将它们连接起来并用空格替换<code>_</code>即可对标记进行解码 - 这会产生标准化文本。正如我们之前看到的，BERT 分词器删除了重复空格，因此其分词是不可逆的。</font></p>
<h3 id="4算法综述"><a class="markdownIt-Anchor" href="#4算法综述"></a> 4.算法综述</h3>
<p>在以下部分中，我们将深入研究三种主要的子词标记化算法：BPE（由 GPT-2 等使用）、WordPiece（例如由 BERT 使用）和 Unigram（由 T5 等使用）。在开始之前，我们先简要概述一下它们各自的工作原理。如果您还没有理解此表，请在阅读完接下来的各节后立即返回此表</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BPE</th>
<th>WordPiece</th>
<th>Unigram</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training</td>
<td>Starts from a small vocabulary and learns rules to merge tokens</td>
<td>Starts from a small vocabulary and learns rules to merge tokens</td>
<td>Starts from a large vocabulary and learns rules to remove tokens</td>
</tr>
<tr>
<td>Training step</td>
<td>Merges the tokens corresponding to the most common pair</td>
<td>Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent</td>
<td>Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus</td>
</tr>
<tr>
<td>Learns</td>
<td>Merge rules and a vocabulary</td>
<td>Just a vocabulary</td>
<td>A vocabulary with a score for each token</td>
</tr>
<tr>
<td>Encoding</td>
<td>Splits a word into characters and applies the merges learned during training</td>
<td>Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word</td>
<td>Finds the most likely split into tokens, using the scores learned during training</td>
</tr>
</tbody>
</table>
<h3 id><a class="markdownIt-Anchor" href="#"></a> </h3>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">HUI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/09/20/NLP_Course(6.1)/">http://example.com/2024/09/20/NLP_Course(6.1)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">HUI</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/87788970_p0_master1200.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/20/NLP_Course(6.2)/" title="NLP课程（六-中）- 三种标记化"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NLP课程（六-中）- 三种标记化</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/20/NLP_Course(5)/" title="NLP课程（五）- Datasets库"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP课程（五）- Datasets库</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/87788970_p0_master1200.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HUI</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp%E8%AF%BE%E7%A8%8B%E5%85%AD-%E4%B8%8A-tokenizer%E5%BA%93"><span class="toc-text"> NLP课程（六-上）- Tokenizer库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E5%B7%B2%E6%9C%89%E7%9A%84tokenizer%E8%AE%AD%E7%BB%83%E6%96%B0%E7%9A%84tokenizer"><span class="toc-text"> 根据已有的tokenizer训练新的tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E5%87%86%E5%A4%87%E8%AF%AD%E6%96%99%E5%BA%93"><span class="toc-text"> 1.准备语料库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84tokenizer"><span class="toc-text"> 2.训练一个新的Tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E4%BF%9D%E5%AD%98tokenizer"><span class="toc-text"> 3.保存Tokenizer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BF%AB%E9%80%9F%E6%A0%87%E8%AE%B0%E5%99%A8%E7%9A%84%E7%89%B9%E6%AE%8A%E8%83%BD%E5%8A%9B"><span class="toc-text"> 快速标记器的特殊能力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E6%89%B9%E9%87%8F%E7%BC%96%E7%A0%81"><span class="toc-text"> 1.批量编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2token%E5%88%86%E7%B1%BB%E7%AE%A1%E9%81%93%E5%86%85%E9%83%A8"><span class="toc-text"> 2.token分类管道内部</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E7%AE%A1%E9%81%93%E8%8E%B7%E5%BE%97%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%9C"><span class="toc-text"> 通过管道获得基本结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8E%E8%BE%93%E5%85%A5%E5%88%B0%E9%A2%84%E6%B5%8B"><span class="toc-text"> 从输入到预测</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E7%BB%84%E5%AE%9E%E4%BD%93"><span class="toc-text"> 分组实体</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#qa-%E7%AE%A1%E9%81%93%E4%B8%AD%E7%9A%84%E5%BF%AB%E9%80%9F%E6%A0%87%E8%AE%B0%E5%99%A8"><span class="toc-text"> QA 管道中的快速标记器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BD%BF%E7%94%A8%E9%97%AE%E7%AD%94%E7%AE%A1%E9%81%93"><span class="toc-text"> 1. 使用问答管道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%97%AE%E7%AD%94"><span class="toc-text"> 2.使用模型进行问答</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%A4%84%E7%90%86%E9%95%BF%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="toc-text"> 3.处理长上下文</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="toc-text"> 标准化和预标记化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1normalization"><span class="toc-text"> 1.Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E9%A2%84%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="toc-text"> 2.预标记化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3sentencepiece"><span class="toc-text"> 3.SentencePiece</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0"><span class="toc-text"> 4.算法综述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-text"> </span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(7.1)/" title="NLP课程（七/一）- Token分类">NLP课程（七/一）- Token分类</a><time datetime="2024-09-20T14:40:33.000Z" title="发表于 2024-09-20 22:40:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(6.3)/" title="NLP课程（六-下）- 逐块构建分词器">NLP课程（六-下）- 逐块构建分词器</a><time datetime="2024-09-20T14:39:33.000Z" title="发表于 2024-09-20 22:39:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(6.2)/" title="NLP课程（六-中）- 三种标记化">NLP课程（六-中）- 三种标记化</a><time datetime="2024-09-20T14:35:33.000Z" title="发表于 2024-09-20 22:35:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(6.1)/" title="NLP课程（六-上）- Tokenizer库">NLP课程（六-上）- Tokenizer库</a><time datetime="2024-09-20T14:34:33.000Z" title="发表于 2024-09-20 22:34:33">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/20/NLP_Course(5)/" title="NLP课程（五）- Datasets库">NLP课程（五）- Datasets库</a><time datetime="2024-09-20T14:33:33.000Z" title="发表于 2024-09-20 22:33:33">2024-09-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By HUI</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2024/09/20/NLP_Course(6.1)/'
    this.page.identifier = '/2024/09/20/NLP_Course(6.1)/'
    this.page.title = 'NLP课程（六-上）- Tokenizer库'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>