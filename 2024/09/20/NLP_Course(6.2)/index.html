<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>NLP课程（六-中）- 三种标记化 | HUI</title><meta name="author" content="HUI"><meta name="copyright" content="HUI"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（六-中）- 三种标记化  字节对编码标记化(BPE) 字节对编码(BPE)最初被开发为一种压缩文本的算法,然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它,包括 GPT、GP">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP课程（六-中）- 三种标记化">
<meta property="og:url" content="http://example.com/2024/09/20/NLP_Course(6.2)/index.html">
<meta property="og:site_name" content="HUI">
<meta property="og:description" content="转载自：https:&#x2F;&#x2F;huggingface.co&#x2F;learn&#x2F;nlp-course&#x2F;zh-CN&#x2F; 原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。  NLP课程（六-中）- 三种标记化  字节对编码标记化(BPE) 字节对编码(BPE)最初被开发为一种压缩文本的算法,然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它,包括 GPT、GP">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/87788970_p0_master1200.jpg">
<meta property="article:published_time" content="2024-09-20T14:35:33.000Z">
<meta property="article:modified_time" content="2024-09-21T09:03:11.188Z">
<meta property="article:author" content="HUI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/87788970_p0_master1200.jpg"><link rel="shortcut icon" href="/img/122061154_p0_master1200.jpg"><link rel="canonical" href="http://example.com/2024/09/20/NLP_Course(6.2)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'NLP课程（六-中）- 三种标记化',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-21 17:03:11'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/bronya.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/87788970_p0_master1200.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="HUI"><img class="site-icon" src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"/><span class="site-name">HUI</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音樂</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 電影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 關於</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">NLP课程（六-中）- 三种标记化</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-20T14:35:33.000Z" title="发表于 2024-09-20 22:35:33">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-21T09:03:11.188Z" title="更新于 2024-09-21 17:03:11">2024-09-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/">HuggingFace</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/HuggingFace/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>47分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="NLP课程（六-中）- 三种标记化"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2024/09/20/NLP_Course(6.2)/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2024/09/20/NLP_Course(6.2)/" itemprop="commentCount"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>转载自：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/">https://huggingface.co/learn/nlp-course/zh-CN/</a></p>
<p><strong>原中文文档有很多地方翻译的太敷衍了，因此才有此系列文章。</strong></p>
<h1 id="nlp课程六-中-三种标记化"><a class="markdownIt-Anchor" href="#nlp课程六-中-三种标记化"></a> NLP课程（六-中）- 三种标记化</h1>
<h2 id="字节对编码标记化bpe"><a class="markdownIt-Anchor" href="#字节对编码标记化bpe"></a> 字节对编码标记化(BPE)</h2>
<p>字节对编码(BPE)最初被开发为一种压缩文本的算法,然后在预训练 GPT 模型时被 OpenAI 用于标记化。许多 Transformer 模型都使用它,包括 GPT、GPT-2、RoBERTa、BART 和 DeBERTa。</p>
<h3 id="1训练算法"><a class="markdownIt-Anchor" href="#1训练算法"></a> 1.训练算法</h3>
<p><font color="red">BPE训练算法首先计算语料库中使用的唯一单词集**(在完成标准化和预标记化步骤之后),**然后通过获取用于编写这些单词的所有符号来构建词汇表。</font>一个非常简单的例子,假设我们的语料库使用了这五个词:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;hug&quot;</span>, <span class="string">&quot;pug&quot;</span>, <span class="string">&quot;pun&quot;</span>, <span class="string">&quot;bun&quot;</span>, <span class="string">&quot;hugs&quot;</span></span><br></pre></td></tr></table></figure>
<p>基础词汇将是 <code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;]</code>。对于实际情况,基本词汇表将包含所有 ASCII 字符,至少,可能还包含一些 Unicode 字符。</p>
<blockquote>
<p>Unicode:<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25482087/article/details/117048596">计算机基础知识之Unicode-彻底弄懂 Unicode 编码-CSDN博客</a></p>
</blockquote>
<p><font color="red">如果您正在标记的示例使用不在训练语料库中的字符,则该字符将转换为未知标记。这就是为什么许多 NLP 模型在分析带有表情符号的内容方面非常糟糕的原因之一。</font></p>
<blockquote>
<p><font color="red">GPT-2 和 RoBERTa 标记器(非常相似)有一个聪明的方法来处理这个问题: 他们不把单词看成是用 Unicode 字符写的，而是用字节写的。这样,基本词汇表的大小很小(256),但你能想到的每个字符仍将被包含在内,而不会最终转换为未知标记。这个技巧被称为 <strong>字节级 BPE。</strong></font></p>
</blockquote>
<p>获得这个基本词汇后,我们添加新的标记,直到通过学习<strong>合并</strong>达到所需的词汇量,这是将现有词汇表的两个元素合并为一个新元素的规则。因此,在开始时,这些合并将创建具有两个字符的标记,然后,随着训练的进行,会创建更长的子词。</p>
<p><font color="red">在分词器训练期间的任何一步,BPE 算法都会搜索最常见的现有标记对 (“对”,这里我们指的是单词中的两个连续标记)。最频繁的一对将被合并,我们冲洗并重复下一步。</font></p>
<p>回到我们之前的例子,让我们假设单词具有以下频率:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>意味着 <code>&quot;hug&quot;</code> 在语料库中出现了10次, <code>&quot;pug&quot;</code> 5次, <code>&quot;pun&quot;</code> 12次, <code>&quot;bun&quot;</code> 4次, 以及 <code>&quot;hugs&quot;</code> 5次。我们通过将每个单词拆分为字符(形成我们初始词汇表的字符)来开始训练,这样我们就可以将每个单词视为一个标记列表:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;g&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们看成对。这对 <code>(&quot;h&quot;, &quot;u&quot;)</code> 出现在单词 <code>&quot;hug&quot;</code> 和 <code>&quot;hugs&quot;</code>中,所以语料库中总共有15次。不过,这并不是最频繁的一对:这个荣誉属于 <code>(&quot;u&quot;, &quot;g&quot;)</code>,它出现在 <code>&quot;hug&quot;</code>, <code>&quot;pug&quot;</code>, 以及 <code>&quot;hugs&quot;</code>中,在词汇表中总共 20 次。</p>
<p>因此,标记器学习的第一个合并规则是 <code>(&quot;u&quot;, &quot;g&quot;) -&gt; &quot;ug&quot;</code>,意思就是 <code>&quot;ug&quot;</code> 将被添加到词汇表中,并且这对应该合并到语料库的所有单词中。在这个阶段结束时,词汇表和语料库看起来像这样:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;ug&quot;</span>]</span><br><span class="line">Corpus: (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;u&quot;</span> <span class="string">&quot;n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们有一些导致标记长于两个字符的对: 例如 <code>(&quot;h&quot;, &quot;ug&quot;)</code>, 在语料库中出现15次。然而,这个阶段最频繁的对是 <code>(&quot;u&quot;, &quot;n&quot;)</code>,在语料库中出现16次,所以学到的第二个合并规则是 <code>(&quot;u&quot;, &quot;n&quot;) -&gt; &quot;un&quot;</code>。将其添加到词汇表并合并所有现有的这个对,将出现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;ug&quot;</span>, <span class="string">&quot;un&quot;</span>]</span><br><span class="line">Corpus: (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;ug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>现在最频繁的一对是 <code>(&quot;h&quot;, &quot;ug&quot;)</code>,所以我们学习了合并规则 <code>(&quot;h&quot;, &quot;ug&quot;) -&gt; &quot;hug&quot;</code>,这给了我们第一个三个字母的标记。合并后,语料库如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;ug&quot;</span>, <span class="string">&quot;un&quot;</span>, <span class="string">&quot;hug&quot;</span>]</span><br><span class="line">Corpus: (<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;ug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;un&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hug&quot;</span> <span class="string">&quot;s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>我们继续这样合并,直到达到我们所需的词汇量。</p>
<p><font color="YellowGreen">注意，合并后的词汇将会加入到词汇表中，二用于合并的两个词汇不会删除。</font></p>
<h3 id="2标记化算法"><a class="markdownIt-Anchor" href="#2标记化算法"></a> 2.标记化算法</h3>
<p>标记化紧跟训练过程,从某种意义上说,通过应用以下步骤对新输入进行标记:</p>
<ol>
<li>规范化</li>
<li>预标记化</li>
<li>将单词拆分为单个字符</li>
<li>将学习的合并规则按顺序应用于这些拆分</li>
</ol>
<p>让我们以我们在训练期间使用的示例为例,学习三个合并规则:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;u&quot;</span>, <span class="string">&quot;g&quot;</span>) -&gt; <span class="string">&quot;ug&quot;</span></span><br><span class="line">(<span class="string">&quot;u&quot;</span>, <span class="string">&quot;n&quot;</span>) -&gt; <span class="string">&quot;un&quot;</span></span><br><span class="line">(<span class="string">&quot;h&quot;</span>, <span class="string">&quot;ug&quot;</span>) -&gt; <span class="string">&quot;hug&quot;</span></span><br></pre></td></tr></table></figure>
<p>这个单词 <code>&quot;bug&quot;</code> 将被标记为 <code>[&quot;b&quot;, &quot;ug&quot;]</code>。然而 <code>&quot;mug&quot;</code>,将被标记为 <code>[&quot;[UNK]&quot;, &quot;ug&quot;]</code>,因为字母 <code>&quot;m&quot;</code> 不再基本词汇表中。同样,单词<code>&quot;thug&quot;</code> 会被标记为 <code>[&quot;[UNK]&quot;, &quot;hug&quot;]</code>: 字母 <code>&quot;t&quot;</code> 不在基本词汇表中,应用合并规则首先导致 <code>&quot;u&quot;</code> 和 <code>&quot;g&quot;</code> 被合并,然后是 <code>&quot;hu&quot;</code> 和 <code>&quot;g&quot;</code> 被合并。</p>
<h3 id="3实现bpe"><a class="markdownIt-Anchor" href="#3实现bpe"></a> 3.实现BPE</h3>
<p>现在让我们看一下 BPE 算法的实现。<font color="red">这不会是你可以在大型语料库上实际使用的优化版本;</font>我们只是想向你展示代码,以便你可以更好地理解算法</p>
<p><font size="5">创建一个简单的语料库</font></p>
<p>首先我们需要一个语料库,所以让我们用几句话创建一个简单的语料库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><font size="5">将该语料库预先标记为单词,计算语料库中<strong>每个单词</strong>的频率</font>,</p>
<p>接下来,我们需要将该语料库预先标记为单词。由于我们正在复现 BPE 标记器(如 GPT-2),我们将使用 <code>gpt2</code> 标记器作为预标记化的标记器:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;gpt2&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们在进行预标记化时计算语料库中<strong>每个单词</strong>的频率:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chl183/article/details/107446836">Python collections模块之defaultdict()详解_from collections import defaultdict-CSDN博客</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(word_freqs)</span><br><span class="line">defaultdict(<span class="built_in">int</span>, &#123;<span class="string">&#x27;This&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;Ġis&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;Ġthe&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;ĠHugging&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;ĠFace&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;ĠCourse&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;Ġchapter&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;Ġabout&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtokenization&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġsection&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġshows&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġseveral&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtokenizer&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġalgorithms&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;Hopefully&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;,&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġyou&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġwill&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġbe&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġable&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġto&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġunderstand&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġhow&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;Ġthey&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġare&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtrained&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġand&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġgenerate&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Ġtokens&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><font size="5">计算基本词汇</font></p>
<p>下一步是计算基本词汇,由语料库中使用的所有字符组成:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">alphabet = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys():</span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> word:</span><br><span class="line">        <span class="keyword">if</span> letter <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">            alphabet.append(letter)</span><br><span class="line">alphabet.sort()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(alphabet)</span><br><span class="line">[ <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;F&#x27;</span>, <span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>我们还在该词汇表的开头添加了模型使用的特殊标记。对于GPT-2,唯一的特殊标记是 <code>&quot;&lt;|endoftext|&gt;&quot;</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab = [<span class="string">&quot;&lt;|endoftext|&gt;&quot;</span>] + alphabet.copy()</span><br></pre></td></tr></table></figure>
<p><font size="5">将每个单词拆分为单独的字符,并计算每对的频率</font></p>
<p>我们现在需要将每个单词拆分为单独的字符,以便能够开始训练:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">splits = &#123;word: [c <span class="keyword">for</span> c <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys()&#125;</span><br></pre></td></tr></table></figure>
<p>现在我们已准备好进行训练,让我们编写一个函数来计算每对的频率。我们需要在训练的每个步骤中使用它:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_pair_freqs</span>(<span class="params">splits</span>):</span><br><span class="line">    pair_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(split) - <span class="number">1</span>):</span><br><span class="line">            pair = (split[i], split[i + <span class="number">1</span>])</span><br><span class="line">            pair_freqs[pair] += freq</span><br><span class="line">    <span class="keyword">return</span> pair_freqs</span><br></pre></td></tr></table></figure>
<p>让我们来看看这个字典在初始拆分后的一部分:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pair_freqs = compute_pair_freqs(splits)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> <span class="built_in">enumerate</span>(pair_freqs.keys()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;pair_freqs[key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> i &gt;= <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">(<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;h&#x27;</span>): <span class="number">3</span></span><br><span class="line">(<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>): <span class="number">3</span></span><br><span class="line">(<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>): <span class="number">5</span></span><br><span class="line">(<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;i&#x27;</span>): <span class="number">2</span></span><br><span class="line">(<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;t&#x27;</span>): <span class="number">7</span></span><br><span class="line">(<span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;h&#x27;</span>): <span class="number">3</span></span><br></pre></td></tr></table></figure>
<p><font size="5">找到最频繁的对并添加到词汇表</font></p>
<p>现在, 找到最频繁的对只需要一个快速的循环:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">max_freq = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items():</span><br><span class="line">    <span class="keyword">if</span> max_freq <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_freq &lt; freq:</span><br><span class="line">        best_pair = pair</span><br><span class="line">        max_freq = freq</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(best_pair, max_freq)</span><br><span class="line">(<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;t&#x27;</span>) <span class="number">7</span></span><br></pre></td></tr></table></figure>
<p>所以第一个要学习的合并是 <code>('Ġ', 't') -&gt; 'Ġt'</code>, 我们添加 <code>'Ġt'</code> 到词汇表:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">merges = &#123;(<span class="string">&quot;Ġ&quot;</span>, <span class="string">&quot;t&quot;</span>): <span class="string">&quot;Ġt&quot;</span>&#125;</span><br><span class="line">vocab.append(<span class="string">&quot;Ġt&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">在分词字典中应用该合并</font></p>
<p>要继续接下来的步骤,我们需要在我们的<code>分词</code>字典中应用该合并。让我们为此编写另一个函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_pair</span>(<span class="params">a, b, splits</span>):</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs:</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> split[i] == a <span class="keyword">and</span> split[i + <span class="number">1</span>] == b:</span><br><span class="line">                split = split[:i] + [a + b] + split[i + <span class="number">2</span> :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        splits[word] = split</span><br><span class="line">    <span class="keyword">return</span> splits</span><br></pre></td></tr></table></figure>
<p>我们可以看看第一次合并的结果。<font color="YellowGreen">注意，splits是更新过的</font>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">splits = merge_pair(<span class="string">&quot;Ġ&quot;</span>, <span class="string">&quot;t&quot;</span>, splits)</span><br><span class="line"><span class="built_in">print</span>(splits[<span class="string">&quot;Ġtrained&quot;</span>])</span><br><span class="line">[<span class="string">&#x27;Ġt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;d&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p><font size="5">在词汇量达到50前重复合并操作</font></p>
<p>现在我们有了循环所需的一切,直到我们学会了我们想要的所有合并。我们的目标是词汇量达到50:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(vocab) &lt; vocab_size:</span><br><span class="line">    pair_freqs = compute_pair_freqs(splits)</span><br><span class="line">    best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">    max_freq = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items():</span><br><span class="line">        <span class="keyword">if</span> max_freq <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_freq &lt; freq:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_freq = freq</span><br><span class="line">    splits = merge_pair(*best_pair, splits) <span class="comment"># 更新splits</span></span><br><span class="line">    merges[best_pair] = best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>]</span><br><span class="line">    vocab.append(best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>结果,我们学习了 19 条合并规则(初始词汇表的大小 31 — 30 字母字符,加上特殊标记):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(merges)</span><br><span class="line">&#123;(<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;t&#x27;</span>): <span class="string">&#x27;Ġt&#x27;</span>, (<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>): <span class="string">&#x27;is&#x27;</span>, (<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;r&#x27;</span>): <span class="string">&#x27;er&#x27;</span>, (<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;a&#x27;</span>): <span class="string">&#x27;Ġa&#x27;</span>, (<span class="string">&#x27;Ġt&#x27;</span>, <span class="string">&#x27;o&#x27;</span>): <span class="string">&#x27;Ġto&#x27;</span>, (<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;n&#x27;</span>): <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line"> (<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;h&#x27;</span>): <span class="string">&#x27;Th&#x27;</span>, (<span class="string">&#x27;Th&#x27;</span>, <span class="string">&#x27;is&#x27;</span>): <span class="string">&#x27;This&#x27;</span>, (<span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;u&#x27;</span>): <span class="string">&#x27;ou&#x27;</span>, (<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;e&#x27;</span>): <span class="string">&#x27;se&#x27;</span>, (<span class="string">&#x27;Ġto&#x27;</span>, <span class="string">&#x27;k&#x27;</span>): <span class="string">&#x27;Ġtok&#x27;</span>,</span><br><span class="line"> (<span class="string">&#x27;Ġtok&#x27;</span>, <span class="string">&#x27;en&#x27;</span>): <span class="string">&#x27;Ġtoken&#x27;</span>, (<span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;d&#x27;</span>): <span class="string">&#x27;nd&#x27;</span>, (<span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;is&#x27;</span>): <span class="string">&#x27;Ġis&#x27;</span>, (<span class="string">&#x27;Ġt&#x27;</span>, <span class="string">&#x27;h&#x27;</span>): <span class="string">&#x27;Ġth&#x27;</span>, (<span class="string">&#x27;Ġth&#x27;</span>, <span class="string">&#x27;e&#x27;</span>): <span class="string">&#x27;Ġthe&#x27;</span>,</span><br><span class="line"> (<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;n&#x27;</span>): <span class="string">&#x27;in&#x27;</span>, (<span class="string">&#x27;Ġa&#x27;</span>, <span class="string">&#x27;b&#x27;</span>): <span class="string">&#x27;Ġab&#x27;</span>, (<span class="string">&#x27;Ġtoken&#x27;</span>, <span class="string">&#x27;i&#x27;</span>): <span class="string">&#x27;Ġtokeni&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>词汇表由特殊标记、初始字母和所有合并结果组成:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(vocab)</span><br><span class="line">[<span class="string">&#x27;&lt;|endoftext|&gt;&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;F&#x27;</span>, <span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;m&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;o&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;Ġt&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;er&#x27;</span>, <span class="string">&#x27;Ġa&#x27;</span>, <span class="string">&#x27;Ġto&#x27;</span>, <span class="string">&#x27;en&#x27;</span>, <span class="string">&#x27;Th&#x27;</span>, <span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;ou&#x27;</span>, <span class="string">&#x27;se&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Ġtok&#x27;</span>, <span class="string">&#x27;Ġtoken&#x27;</span>, <span class="string">&#x27;nd&#x27;</span>, <span class="string">&#x27;Ġis&#x27;</span>, <span class="string">&#x27;Ġth&#x27;</span>, <span class="string">&#x27;Ġthe&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;Ġab&#x27;</span>, <span class="string">&#x27;Ġtokeni&#x27;</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>💡 在同一语料库上使用 <code>train_new_from_iterator()</code> 不会产生完全相同的词汇表。<font color="red">这是因为当有最频繁对的选择时,我们选择遇到的第一个, 而 🤗 Tokenizers 库根据内部ID选择第一个。</font></p>
</blockquote>
<p>为了对新文本进行分词,我们对其进行预分词、拆分，然后应用学到的所有合并规则:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> pre_tokenize_result]</span><br><span class="line">    splits = [[l <span class="keyword">for</span> l <span class="keyword">in</span> word] <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="keyword">for</span> pair, merge <span class="keyword">in</span> merges.items():</span><br><span class="line">        <span class="keyword">for</span> idx, split <span class="keyword">in</span> <span class="built_in">enumerate</span>(splits):</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> split[i] == pair[<span class="number">0</span>] <span class="keyword">and</span> split[i + <span class="number">1</span>] == pair[<span class="number">1</span>]:</span><br><span class="line">                    split = split[:i] + [merge] + split[i + <span class="number">2</span> :]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    i += <span class="number">1</span></span><br><span class="line">            splits[idx] = split</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(splits, [])</span><br></pre></td></tr></table></figure>
<p>我们可以在任何由字母表中的字符组成的文本上尝试这个:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenize(<span class="string">&quot;This is not a token.&quot;</span>)</span><br><span class="line"></span><br><span class="line">[<span class="string">&#x27;This&#x27;</span>, <span class="string">&#x27;Ġis&#x27;</span>, <span class="string">&#x27;Ġ&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;Ġa&#x27;</span>, <span class="string">&#x27;Ġtoken&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><font color="red">⚠️ 如果存在未知字符,我们的实现将抛出错误,因为我们没有做任何处理它们。GPT-2 实际上没有未知标记(使用字节级 BPE 时不可能得到未知字符),但这可能发生在这里,因为我们没有在初始词汇表中包含所有可能的字节。 BPE 的这方面超出了本节的范围,因此我们忽略了细节。</font></p>
</blockquote>
<h2 id="wordpiece-标记化"><a class="markdownIt-Anchor" href="#wordpiece-标记化"></a> WordPiece 标记化</h2>
<p>WordPiece 是 Google 为预训练 BERT 而开发的标记化算法。</p>
<h3 id="1训练算法-2"><a class="markdownIt-Anchor" href="#1训练算法-2"></a> 1.训练算法</h3>
<blockquote>
<p><font color="red">⚠️ Google 从未开源 WordPiece 训练算法的实现,因此以下是我们基于已发表文献的最佳猜测。它可能不是 100% 准确的。</font></p>
</blockquote>
<p>与 BPE一样,WordPiece 从一个小词汇表开始,包括模型使用的特殊标记和初始字母表。<font color="red">因为它通过添加前缀来识别子词 (如同 <code>##</code> 对于 BERT),每个单词最初是通过将该前缀添加到单词内的所有字符来拆分的。</font>所以,例如 <code>&quot;word&quot;</code> ,像这样拆分:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w <span class="comment">##o ##r ##d</span></span><br></pre></td></tr></table></figure>
<p>因此,初始字母表包含出现在单词开头的所有字符以及出现在单词内部的以WordPiece前缀<code>##</code>开头的字符。</p>
<p><font color="red">然后,再次像 BPE 一样,WordPiece 学习合并规则。主要区别在于选择要合并的对的方式。&lt;WordPiece 不是选择最频繁的对，而是使用以下公式计算每对的分数(每对出现的频率除以第一个词的频率与第二个词的频率的乘积)：</font></p>
<center>score=(freq_of_pair)/(freq_of_first_element × freq_of_second_element)<font color="red">通过将配对的频率除以其每个部分的频率的乘积, 该算法优先合并单个部分在词汇表中频率较低的对。</font>例如,它不一定会合并 <code>(&quot;un&quot;, &quot;##able&quot;)</code> 即使这对在词汇表中出现的频率很高,因为 <code>&quot;un&quot;</code> 和 <code>&quot;##able&quot;</code> 很可能每个词都出现在很多其他词中并且出现频率很高。相比之下,像 <code>(&quot;hu&quot;, &quot;##gging&quot;)</code> 可能会更快地合并 (假设 “hugging” 经常出现在词汇表中),因为 <code>&quot;hu&quot;</code> 和 <code>&quot;##gging&quot;</code> 这两个词单独出现地频率可能较低。<p></p>
<p><font size="5">示例</font></p>
<p>让我们看看我们在 BPE 训练示例中使用的相同词汇:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>这里的拆分将是:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span> <span class="string">&quot;##s&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>所以最初的词汇将是 <code>[&quot;b&quot;, &quot;h&quot;, &quot;p&quot;, &quot;##g&quot;, &quot;##n&quot;, &quot;##s&quot;, &quot;##u&quot;]</code> (如果我们暂时忘记特殊标记)。最频繁的一对是 <code>(&quot;##u&quot;, &quot;##g&quot;)</code> (目前20次),但 <code>&quot;##u&quot;</code> 单独出现的频率非常高,所以它的分数不是最高的(它是 1 / 36)。所有带有 <code>&quot;##u&quot;</code> 的对实际上都有相同的分数(1 / 36),所以分数最高的对是 <code>(&quot;##g&quot;, &quot;##s&quot;)</code> ——唯一没有 <code>&quot;##u&quot;</code> 的对—— 1 / 20,所以学习的第一个合并是 <code>(&quot;##g&quot;, &quot;##s&quot;) -&gt; (&quot;##gs&quot;)</code>。</p>
<p>请注意,当我们合并时,我们删除了两个标记之间的 <code>##</code>,所以我们添加 <code>&quot;##gs&quot;</code> 到词汇表中,并在语料库的单词中应用该合并:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;##g&quot;</span>, <span class="string">&quot;##n&quot;</span>, <span class="string">&quot;##s&quot;</span>, <span class="string">&quot;##u&quot;</span>, <span class="string">&quot;##gs&quot;</span>]</span><br><span class="line">Corpus: (<span class="string">&quot;h&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;h&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##gs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>在这一点中, <code>&quot;##u&quot;</code> 是在所有可能的对中,因此它们最终都具有相同的分数。假设在这种情况下,第一对被合并, <code>(&quot;h&quot;, &quot;##u&quot;) -&gt; &quot;hu&quot;</code>。这使得我们:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;##g&quot;</span>, <span class="string">&quot;##n&quot;</span>, <span class="string">&quot;##s&quot;</span>, <span class="string">&quot;##u&quot;</span>, <span class="string">&quot;##gs&quot;</span>, <span class="string">&quot;hu&quot;</span>]</span><br><span class="line">Corpus: (<span class="string">&quot;hu&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hu&quot;</span> <span class="string">&quot;##gs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>然后下一个最高的分数由 <code>(&quot;hu&quot;, &quot;##g&quot;)</code> 和 <code>(&quot;hu&quot;, &quot;##gs&quot;)</code> 共享(1/15,与其他所有对的 1/21 相比),因此合并得分最高的第一对:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary: [<span class="string">&quot;b&quot;</span>, <span class="string">&quot;h&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;##g&quot;</span>, <span class="string">&quot;##n&quot;</span>, <span class="string">&quot;##s&quot;</span>, <span class="string">&quot;##u&quot;</span>, <span class="string">&quot;##gs&quot;</span>, <span class="string">&quot;hu&quot;</span>, <span class="string">&quot;hug&quot;</span>]</span><br><span class="line">Corpus: (<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##g&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;p&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;b&quot;</span> <span class="string">&quot;##u&quot;</span> <span class="string">&quot;##n&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hu&quot;</span> <span class="string">&quot;##gs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>我们继续这样处理,直到达到我们所需的词汇量。</p>
<h3 id="2标记化算法-2"><a class="markdownIt-Anchor" href="#2标记化算法-2"></a> 2.标记化算法</h3>
<p><font color="red">WordPiece 和 BPE 中的标记化的不同在于 WordPiece 只保存最终词汇,而不是学习的合并规则。从要标记的单词开始,WordPiece 找到词汇表中最长的子词,然后对其进行拆分。</font>例如,如果我们使用上面例子中学到的词汇,对于单词 <code>&quot;hugs&quot;</code>,词汇表中从头开始的最长子词是 <code>&quot;hug&quot;</code>,所以我们在那里拆分并得到 <code>[&quot;hug&quot;, &quot;##s&quot;]</code>。 然后我们继续使用词汇表中的 <code>&quot;##s&quot;</code>,因此 <code>&quot;hugs&quot;</code> 的标记化是 <code>[&quot;hug&quot;, &quot;##s&quot;]</code>.</p>
<p>使用 BPE, 我们将按顺序应用学习到的合并并将其标记为 <code>[&quot;hu&quot;, &quot;##gs&quot;]</code>,因此编码是不同的。</p>
<blockquote>
<p>再举一个例子,让我们看看 <code>&quot;bugs&quot;</code> 将如何被标记化。 <code>&quot;b&quot;</code> 是从词汇表中单词开头开始的最长子词,所以我们在那里拆分并得到 <code>[&quot;b&quot;, &quot;##ugs&quot;]</code>。然后 <code>&quot;##u&quot;</code> 是词汇表中从 <code>&quot;##ugs&quot;</code> 开始的最长的子词,所以我们在那里拆分并得到 <code>[&quot;b&quot;, &quot;##u, &quot;##gs&quot;]</code>。最后, <code>&quot;##gs&quot;</code> 在词汇表中,所以最后一个列表是 <code>&quot;bugs&quot;</code> 的标记化。</p>
</blockquote>
<p>当分词达到无法在词汇表中找到子词的阶段时, 整个词被标记为未知 — 例如, <code>&quot;mug&quot;</code> 将被标记为 <code>[&quot;[UNK]&quot;]</code>,就像 <code>&quot;bum&quot;</code> (<font color="red">即使我们可以以 “<code>b</code>” 和 “<code>##u</code>” 开始, “<code>##m</code>” 不在词汇表中,由此产生的标记将只是 [<code>&quot;[UNK]&quot;</code>], 不是 <code>[&quot;b&quot;, &quot;##u&quot;, &quot;[UNK]&quot;]</code></font>)。这是与 BPE 的另一个区别,BPE 只会将不在词汇表中的单个字符分类为未知。</p>
<h3 id="3实现wordpiece"><a class="markdownIt-Anchor" href="#3实现wordpiece"></a> 3.实现WordPiece</h3>
<p>现在让我们看一下 WordPiece 算法的实现。<font color="red">与BPE一样,这只是教学,你将无法在大型语料库中使用它。</font></p>
<p>我们将使用与 BPE 示例中相同的语料库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>首先,我们需要将语料库预先标记为单词。由于我们正在复制 WordPiece 标记器 (如 BERT),因此我们将使用 <code>bert-base-cased</code> 标记器用于预标记化:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-cased&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>然后我们在进行预标记化时计算语料库中每个单词的频率:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">word_freqs</span><br><span class="line"></span><br><span class="line">defaultdict(</span><br><span class="line">    <span class="built_in">int</span>, &#123;<span class="string">&#x27;This&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;is&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;the&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Hugging&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Face&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Course&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;chapter&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;about&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;tokenization&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;section&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;shows&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;several&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;tokenizer&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;algorithms&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;Hopefully&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;,&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;you&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;will&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;be&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;able&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;to&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;understand&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;how&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;they&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;are&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;trained&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;and&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;generate&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;tokens&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>正如我们之前看到的,<font color="red">字母表是由单词的所有第一个字母组成的唯一集合,以及出现在前缀为 <code>##</code> 的其他字母:</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">alphabet = []</span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys():</span><br><span class="line">    <span class="keyword">if</span> word[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">        alphabet.append(word[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> word[<span class="number">1</span>:]:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">f&quot;##<span class="subst">&#123;letter&#125;</span>&quot;</span> <span class="keyword">not</span> <span class="keyword">in</span> alphabet:</span><br><span class="line">            alphabet.append(<span class="string">f&quot;##<span class="subst">&#123;letter&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">alphabet.sort()</span><br><span class="line">alphabet</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(alphabet)</span><br><span class="line">[<span class="string">&#x27;##a&#x27;</span>, <span class="string">&#x27;##b&#x27;</span>, <span class="string">&#x27;##c&#x27;</span>, <span class="string">&#x27;##d&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;##f&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##k&#x27;</span>, <span class="string">&#x27;##l&#x27;</span>, <span class="string">&#x27;##m&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##p&#x27;</span>, <span class="string">&#x27;##r&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##v&#x27;</span>, <span class="string">&#x27;##w&#x27;</span>, <span class="string">&#x27;##y&#x27;</span>, <span class="string">&#x27;##z&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;F&#x27;</span>, <span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;y&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>我们还在该词汇表的开头添加了模型使用的特殊标记。在使用 BERT 的情况下,它是列表 <code>[&quot;[PAD]&quot;, &quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[MASK]&quot;]</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab = [<span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>] + alphabet.copy()</span><br></pre></td></tr></table></figure>
<p>接下来我们需要拆分每个单词, 所有不是第一个字母的字母都以 <code>##</code> 为前缀:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">splits = &#123;</span><br><span class="line">    word: [c <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> <span class="string">f&quot;##<span class="subst">&#123;c&#125;</span>&quot;</span> <span class="keyword">for</span> i, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(word)]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs.keys()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在我们已经准备好训练了,让我们编写一个函数来计算每对的分数。我们需要在训练的每个步骤中使用它:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_pair_scores</span>(<span class="params">splits</span>):</span><br><span class="line">    letter_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    pair_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            letter_freqs[split[<span class="number">0</span>]] += freq</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(split) - <span class="number">1</span>):</span><br><span class="line">            pair = (split[i], split[i + <span class="number">1</span>])</span><br><span class="line">            letter_freqs[split[i]] += freq</span><br><span class="line">            pair_freqs[pair] += freq</span><br><span class="line">        letter_freqs[split[-<span class="number">1</span>]] += freq</span><br><span class="line"></span><br><span class="line">    scores = &#123;</span><br><span class="line">        pair: freq / (letter_freqs[pair[<span class="number">0</span>]] * letter_freqs[pair[<span class="number">1</span>]])</span><br><span class="line">        <span class="keyword">for</span> pair, freq <span class="keyword">in</span> pair_freqs.items()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<p>让我们来看看这个字典在初始拆分后的一部分:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pair_scores = compute_pair_scores(splits)</span><br><span class="line"><span class="keyword">for</span> i, key <span class="keyword">in</span> <span class="built_in">enumerate</span>(pair_scores.keys()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>: <span class="subst">&#123;pair_scores[key]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> i &gt;= <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">(<span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>): <span class="number">0.125</span></span><br><span class="line">(<span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>): <span class="number">0.03409090909090909</span></span><br><span class="line">(<span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>): <span class="number">0.02727272727272727</span></span><br><span class="line">(<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>): <span class="number">0.1</span></span><br><span class="line">(<span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>): <span class="number">0.03571428571428571</span></span><br><span class="line">(<span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>): <span class="number">0.011904761904761904</span></span><br></pre></td></tr></table></figure>
<p>现在,找到得分最高的对只需要一个快速循环:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">best_pair = <span class="string">&quot;&quot;</span></span><br><span class="line">max_score = <span class="literal">None</span></span><br><span class="line"><span class="keyword">for</span> pair, score <span class="keyword">in</span> pair_scores.items():</span><br><span class="line">    <span class="keyword">if</span> max_score <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_score &lt; score:</span><br><span class="line">        best_pair = pair</span><br><span class="line">        max_score = score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(best_pair, max_score)</span><br><span class="line">(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;##b&#x27;</span>) <span class="number">0.2</span></span><br></pre></td></tr></table></figure>
<p>所以第一个要学习的合并是 <code>('a', '##b') -&gt; 'ab'</code>, 并且我们添加 <code>'ab'</code> 到词汇表中:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vocab.append(<span class="string">&quot;ab&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>要继续接下来的步骤,我们需要在我们的 <code>拆分</code> 字典中应用该合并。让我们为此编写另一个函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge_pair</span>(<span class="params">a, b, splits</span>):</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_freqs:</span><br><span class="line">        split = splits[word]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(split) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(split) - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> split[i] == a <span class="keyword">and</span> split[i + <span class="number">1</span>] == b:</span><br><span class="line">                merge = a + b[<span class="number">2</span>:] <span class="keyword">if</span> b.startswith(<span class="string">&quot;##&quot;</span>) <span class="keyword">else</span> a + b</span><br><span class="line">                split = split[:i] + [merge] + split[i + <span class="number">2</span> :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        splits[word] = split</span><br><span class="line">    <span class="keyword">return</span> splits</span><br></pre></td></tr></table></figure>
<p>我们可以看看第一次合并的结果:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">splits = merge_pair(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;##b&quot;</span>, splits)</span><br><span class="line">splits[<span class="string">&quot;about&quot;</span>]</span><br><span class="line">[<span class="string">&#x27;ab&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>现在我们有了循环所需的一切,直到我们学会了我们想要的所有合并。我们的目标词汇量为70:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">70</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(vocab) &lt; vocab_size:</span><br><span class="line">    scores = compute_pair_scores(splits)</span><br><span class="line">    best_pair, max_score = <span class="string">&quot;&quot;</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> pair, score <span class="keyword">in</span> scores.items():</span><br><span class="line">        <span class="keyword">if</span> max_score <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> max_score &lt; score:</span><br><span class="line">            best_pair = pair</span><br><span class="line">            max_score = score</span><br><span class="line">    splits = merge_pair(*best_pair, splits)</span><br><span class="line">    new_token = (</span><br><span class="line">        best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>][<span class="number">2</span>:]</span><br><span class="line">        <span class="keyword">if</span> best_pair[<span class="number">1</span>].startswith(<span class="string">&quot;##&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span> best_pair[<span class="number">0</span>] + best_pair[<span class="number">1</span>]</span><br><span class="line">    )</span><br><span class="line">    vocab.append(new_token)</span><br></pre></td></tr></table></figure>
<p>然后我们可以查看生成的词汇表:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(vocab)</span><br><span class="line">[<span class="string">&#x27;[PAD]&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>, <span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>, <span class="string">&#x27;[MASK]&#x27;</span>, <span class="string">&#x27;##a&#x27;</span>, <span class="string">&#x27;##b&#x27;</span>, <span class="string">&#x27;##c&#x27;</span>, <span class="string">&#x27;##d&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;##f&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>, <span class="string">&#x27;##h&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##k&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##l&#x27;</span>, <span class="string">&#x27;##m&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##p&#x27;</span>, <span class="string">&#x27;##r&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>, <span class="string">&#x27;##t&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##v&#x27;</span>, <span class="string">&#x27;##w&#x27;</span>, <span class="string">&#x27;##y&#x27;</span>, <span class="string">&#x27;##z&#x27;</span>, <span class="string">&#x27;,&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;F&#x27;</span>, <span class="string">&#x27;H&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;T&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;ab&#x27;</span>,<span class="string">&#x27;##fu&#x27;</span>, <span class="string">&#x27;Fa&#x27;</span>, <span class="string">&#x27;Fac&#x27;</span>, <span class="string">&#x27;##ct&#x27;</span>, <span class="string">&#x27;##ful&#x27;</span>, <span class="string">&#x27;##full&#x27;</span>, <span class="string">&#x27;##fully&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;Th&#x27;</span>, <span class="string">&#x27;ch&#x27;</span>, <span class="string">&#x27;##hm&#x27;</span>, <span class="string">&#x27;cha&#x27;</span>, <span class="string">&#x27;chap&#x27;</span>, <span class="string">&#x27;chapt&#x27;</span>, <span class="string">&#x27;##thm&#x27;</span>, <span class="string">&#x27;Hu&#x27;</span>, <span class="string">&#x27;Hug&#x27;</span>, <span class="string">&#x27;Hugg&#x27;</span>, <span class="string">&#x27;sh&#x27;</span>, <span class="string">&#x27;th&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;##thms&#x27;</span>, <span class="string">&#x27;##za&#x27;</span>, <span class="string">&#x27;##zat&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##ut&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>正如我们所看到的,与 BPE 相比,这个标记器将单词的一部分作为标记学习得更快一些。</p>
<blockquote>
<p>💡 在同一语料库上使用 <code>train_new_from_iterator()</code> 不会产生完全相同的词汇表。这是因为 🤗 Tokenizers 库没有为训练实现 WordPiece(因为我们不完全确定它的内部结构),而是使用 BPE。</p>
</blockquote>
<p>为了对新文本进行分词,我们对其进行预分词、拆分,然后对每个单词应用分词算法。也就是说,我们从第一个词的开头寻找最大的子词并将其拆分,然后我们在第二部分重复这个过程,对于该词的其余部分和文本中的以下词,依此类推:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encode_word</span>(<span class="params">word</span>):</span><br><span class="line">    tokens = []</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(word) &gt; <span class="number">0</span>:</span><br><span class="line">        i = <span class="built_in">len</span>(word)</span><br><span class="line">        <span class="keyword">while</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> word[:i] <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="string">&quot;[UNK]&quot;</span>]</span><br><span class="line">        tokens.append(word[:i])</span><br><span class="line">        word = word[i:]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(word) &gt; <span class="number">0</span>:</span><br><span class="line">            word = <span class="string">f&quot;##<span class="subst">&#123;word&#125;</span>&quot;</span></span><br><span class="line">    <span class="keyword">return</span> tokens</span><br></pre></td></tr></table></figure>
<p>让我们用词汇表中的一个单词和另一个不在词汇表中的单词进行测试:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;Hugging&quot;</span>))</span><br><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;HOgging&quot;</span>))</span><br><span class="line">[<span class="string">&#x27;Hugg&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;[UNK]&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>现在,让我们编写一个标记文本的函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line">    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> pre_tokenize_result]</span><br><span class="line">    encoded_words = [encode_word(word) <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(encoded_words, [])</span><br></pre></td></tr></table></figure>
<p>我们可以在任何文本上尝试:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenize(<span class="string">&quot;This is the Hugging Face course!&quot;</span>)</span><br><span class="line">[<span class="string">&#x27;Th&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;th&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;Hugg&#x27;</span>, <span class="string">&#x27;##i&#x27;</span>, <span class="string">&#x27;##n&#x27;</span>, <span class="string">&#x27;##g&#x27;</span>, <span class="string">&#x27;Fac&#x27;</span>, <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;##o&#x27;</span>, <span class="string">&#x27;##u&#x27;</span>, <span class="string">&#x27;##r&#x27;</span>, <span class="string">&#x27;##s&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;##e&#x27;</span>, <span class="string">&#x27;[UNK]&#x27;</span>]</span><br></pre></td></tr></table></figure>
<h2 id="unigram标记化"><a class="markdownIt-Anchor" href="#unigram标记化"></a> Unigram标记化</h2>
<p>在 SentencePiece 中经常使用 Unigram 算法,该算法是 AlBERT、T5、mBART、Big Bird 和 XLNet 等模型使用的标记化算法。</p>
<h3 id="1训练算法-3"><a class="markdownIt-Anchor" href="#1训练算法-3"></a> 1.训练算法</h3>
<p>与 BPE 和 WordPiece 相比,Unigram 在另一个方向上工作:<font color="red">它从一个较大的词汇表开始,然后从中删除标记,直到达到所需的词汇表大小。有多种选项可用于构建基本词汇表:例如,我们可以采用预标记化单词中最常见的子串,或者在具有大词汇量的初始语料库上应用BPE。</font></p>
<p><font color="red">在训练的每一步,Unigram 算法都会在给定当前词汇的情况下计算语料库的损失。然后,对于词汇表中的每个符号,算法计算如果删除该符号,整体损失会增加多少,并寻找增加最少的符号。</font>这些符号对语料库的整体损失影响较小,因此从某种意义上说,它们“不太需要”并且是移除的最佳候选者。</p>
<p>这都是一个非常昂贵的操作，<font color="red">因为我们不仅删除与最低损失增加相关的单个符号，而且还删除p((p)是您可以控制的超参数)通常是与最低损失增加相关的符号的10或20%。</font>然后重复这个过程，直到词汇量达到所需的大小。</p>
<p><font color="red">请注意,我们从不删除基本字符,以确保可以标记任何单词。</font></p>
<p>现在,这仍然有点模糊:算法的主要部分是计算语料库的损失,并查看当我们从词汇表中删除一些标记时它会如何变化,但我们还没有解释如何做到这一点。这一步依赖于 Unigram 模型的标记化算法,因此我们接下来将深入研究。</p>
<p>我们将重用前面示例中的语料库:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>对于此示例,我们将采用初始词汇表的所有<strong>严格子字符串</strong><font color="red">（一个字符到所有字符的字符串全部包括）</font>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&quot;h&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;g&quot;</span>, <span class="string">&quot;hu&quot;</span>, <span class="string">&quot;ug&quot;</span>, <span class="string">&quot;p&quot;</span>, <span class="string">&quot;pu&quot;</span>, <span class="string">&quot;n&quot;</span>, <span class="string">&quot;un&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;bu&quot;</span>, <span class="string">&quot;s&quot;</span>, <span class="string">&quot;hug&quot;</span>, <span class="string">&quot;gs&quot;</span>, <span class="string">&quot;ugs&quot;</span>]</span><br></pre></td></tr></table></figure>
<h3 id="2标记化算法-3"><a class="markdownIt-Anchor" href="#2标记化算法-3"></a> 2.标记化算法</h3>
<p><font color="red">Unigram 模型是一种语言模型,它认为每个标记都独立于它之前的标记。它是最简单的语言模型,从某种意义上说, 给定先前上下文的标记 X 的概率就是标记 X 的概率。因此,如果我们使用 Unigram 语言模型生成文本,我们将始终预测最常见的标记。</font></p>
<p>给定标记的概率是它在原始语料库中的频率(我们找到它的次数),除以词汇表中所有标记的所有频率的总和(以确保概率总和为 1)。例如, <code>&quot;ug&quot;</code> 在 <code>&quot;hug&quot;</code> 、 <code>&quot;pug&quot;</code> 以及 <code>&quot;hugs&quot;</code> 中,所以它在我们的语料库中的频率为 20。</p>
<p>以下是词汇表中所有可能的子词的出现频率:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;h&quot;</span>, <span class="number">15</span>) (<span class="string">&quot;u&quot;</span>, <span class="number">36</span>) (<span class="string">&quot;g&quot;</span>, <span class="number">20</span>) (<span class="string">&quot;hu&quot;</span>, <span class="number">15</span>) (<span class="string">&quot;ug&quot;</span>, <span class="number">20</span>) (<span class="string">&quot;p&quot;</span>, <span class="number">17</span>) (<span class="string">&quot;pu&quot;</span>, <span class="number">17</span>) (<span class="string">&quot;n&quot;</span>, <span class="number">16</span>)</span><br><span class="line">(<span class="string">&quot;un&quot;</span>, <span class="number">16</span>) (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>) (<span class="string">&quot;bu&quot;</span>, <span class="number">4</span>) (<span class="string">&quot;s&quot;</span>, <span class="number">5</span>) (<span class="string">&quot;hug&quot;</span>, <span class="number">15</span>) (<span class="string">&quot;gs&quot;</span>, <span class="number">5</span>) (<span class="string">&quot;ugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>所以,所有频率之和为210, 并且子词 <code>&quot;ug&quot;</code> 出现的概率是 20/210。</p>
<p>现在,为了对给定的单词进行标记,我们将所有可能的分割视为token,并根据 Unigram 模型计算每个分割的概率。由于所有标记都被认为是独立的,所以这个概率只是每个token概率的乘积。例如, <code>&quot;pug&quot;</code> 的标记化 <code>[&quot;p&quot;, &quot;u&quot;, &quot;g&quot;]</code> 的概率为:</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msup><mo stretchy="false">[</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mo separator="true">,</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>u</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mo separator="true">,</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><msup><mo stretchy="false">(</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>p</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>×</mo><mi>P</mi><msup><mo stretchy="false">(</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>u</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>×</mo><mi>P</mi><msup><mo stretchy="false">(</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>5</mn><mn>210</mn></mfrac><mo>×</mo><mfrac><mn>36</mn><mn>210</mn></mfrac><mo>×</mo><mfrac><mn>20</mn><mn>210</mn></mfrac><mo>=</mo><mn>0.000389</mn></mrow><annotation encoding="application/x-tex">P([&#x27;p&#x27;, &#x27;u&#x27;, &#x27;g&#x27;]) = P(&#x27;p&#x27;) \times P(&#x27;u&#x27;) \times P(&#x27;g&#x27;) = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mopen"><span class="mopen">[</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct"><span class="mpunct">,</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct"><span class="mpunct">,</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">]</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen"><span class="mopen">(</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen"><span class="mopen">(</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen"><span class="mopen">(</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">1</span><span class="mord">0</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">1</span><span class="mord">0</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">3</span><span class="mord">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">1</span><span class="mord">0</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">3</span><span class="mord">8</span><span class="mord">9</span></span></span></span></span></p>
<p>相比之下,标记化 <code>[&quot;pu&quot;, &quot;g&quot;]</code> 的概率为:</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msup><mo stretchy="false">[</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi>p</mi><msup><mi>u</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mo separator="true">,</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><msup><mo stretchy="false">(</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mi>p</mi><msup><mi>u</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>×</mo><mi>P</mi><msup><mo stretchy="false">(</mo><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi>g</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>5</mn><mn>210</mn></mfrac><mo>×</mo><mfrac><mn>20</mn><mn>210</mn></mfrac><mo>=</mo><mn>0.0022676</mn></mrow><annotation encoding="application/x-tex">P([&#x27;pu&#x27;,&#x27;g&#x27;]) = P(&#x27;pu&#x27;) \times P(&#x27;g&#x27;) = \frac{5}{210} \times \frac{20}{210} = 0.0022676
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mopen"><span class="mopen">[</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord mathnormal">p</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct"><span class="mpunct">,</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">]</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen"><span class="mopen">(</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord mathnormal">p</span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen"><span class="mopen">(</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">1</span><span class="mord">0</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.00744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">1</span><span class="mord">0</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">0</span><span class="mord">0</span><span class="mord">2</span><span class="mord">2</span><span class="mord">6</span><span class="mord">7</span><span class="mord">6</span></span></span></span></span></p>
<p>所以一个更有可能。一般来说,具有尽可能少的标记的标记化将具有最高的概率(因为每个标记重复除以 210),这对应于我们直观想要的:将一个单词分成尽可能少的标记。</p>
<p>那么用 Unigram 模型对单词进行标记化就是概率最高的标记化。在<code>&quot;pug&quot;</code>的示例中，以下是我们为每个可能的分割得到的概率：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&quot;p&quot;</span>, <span class="string">&quot;u&quot;</span>, <span class="string">&quot;g&quot;</span>] : <span class="number">0.000389</span></span><br><span class="line">[<span class="string">&quot;p&quot;</span>, <span class="string">&quot;ug&quot;</span>] : <span class="number">0.0022676</span></span><br><span class="line">[<span class="string">&quot;pu&quot;</span>, <span class="string">&quot;g&quot;</span>] : <span class="number">0.0022676</span></span><br></pre></td></tr></table></figure>
<p>所以, <code>&quot;pug&quot;</code> 将被标记为 <code>[&quot;p&quot;, &quot;ug&quot;]</code> 或者 <code>[&quot;pu&quot;, &quot;g&quot;]</code>, 取决于首先遇到这些分割中的哪一个(请注意,在更大的语料库中,这样的相等的情况很少见)。</p>
<p><font color="red">(感觉上面的概率算错了，P(‘p’)应该是17/210，P(‘pu’)应该是17/210)</font></p>
<p>在这种情况下,很容易找到所有可能的分割并计算它们的概率,但一般来说会有点困难。有一种用于此的经典算法,称为 <em><strong>维特比(Viterbi)算法</strong></em>。本质上,<font color="red">我们可以构建一个图来检测给定单词的可能分割,如果从a到b的子词在词汇表中,则表示存在从字符a到字符b的分支,并将子词的概率归因于该分支。</font></p>
<p><font color="red">为了在该图中找到将具有最佳分数的路径,维特比算法为单词中的每个位置确定在该位置结束的具有最佳分数的分段。由于我们从头到尾，可以通过循环遍历以当前位置结尾的所有子词，然后使用该子词开始位置的最佳标记化分数来找到最佳分数。</font>然后，我们只需展开到达终点的路径即可。<font color="YellowGreen">(比如对于一个五个字符的单词，在第四个位置上，看怎么分割前四个字母能得到最高的分数)</font></p>
<p>让我们看一个使用我们的词汇表和单词 <code>&quot;unhug&quot;</code> 的例子。对于每个位置,以最好的分数结尾的子词如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Character <span class="number">0</span> (u): <span class="string">&quot;u&quot;</span> (score <span class="number">0.171429</span>) <span class="comment"># 36/210</span></span><br><span class="line">Character <span class="number">1</span> (n): <span class="string">&quot;un&quot;</span> (score <span class="number">0.076191</span>) <span class="comment"># 16/210</span></span><br><span class="line">Character <span class="number">2</span> (h): <span class="string">&quot;un&quot;</span> <span class="string">&quot;h&quot;</span> (score <span class="number">0.005442</span>) <span class="comment">#</span></span><br><span class="line">Character <span class="number">3</span> (u): <span class="string">&quot;un&quot;</span> <span class="string">&quot;hu&quot;</span> (score <span class="number">0.005442</span>)</span><br><span class="line">Character <span class="number">4</span> (g): <span class="string">&quot;un&quot;</span> <span class="string">&quot;hug&quot;</span> (score <span class="number">0.005442</span>)</span><br></pre></td></tr></table></figure>
<p>因此 <code>&quot;unhug&quot;</code> 将被标记为 <code>[&quot;un&quot;, &quot;hug&quot;]</code>。</p>
<h3 id="3回到训练"><a class="markdownIt-Anchor" href="#3回到训练"></a> 3.回到训练</h3>
<p>现在我们已经了解了标记化的工作原理,我们可以更深入地研究训练期间使用的损失。<font color="red">在任何给定的阶段,这个损失是通过对语料库中的每个单词进行标记来计算的,使用当前词汇表和由语料库中每个标记的频率确定的 Unigram 模型(如前所述)。</font></p>
<p>语料库中的每个词都有一个分数,损失是这些分数的负对数似然 — 即所有词的语料库中所有词的总和 <code>-log(P(word))</code>。</p>
<p>让我们用以下语料库回到我们的例子:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&quot;hug&quot;</span>, <span class="number">10</span>), (<span class="string">&quot;pug&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;pun&quot;</span>, <span class="number">12</span>), (<span class="string">&quot;bun&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;hugs&quot;</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>每个单词的标记化及其各自的分数是:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;hug&quot;</span>: [<span class="string">&quot;hug&quot;</span>] (score <span class="number">0.071428</span>)</span><br><span class="line"><span class="string">&quot;pug&quot;</span>: [<span class="string">&quot;pu&quot;</span>, <span class="string">&quot;g&quot;</span>] (score <span class="number">0.007710</span>)</span><br><span class="line"><span class="string">&quot;pun&quot;</span>: [<span class="string">&quot;pu&quot;</span>, <span class="string">&quot;n&quot;</span>] (score <span class="number">0.006168</span>)</span><br><span class="line"><span class="string">&quot;bun&quot;</span>: [<span class="string">&quot;bu&quot;</span>, <span class="string">&quot;n&quot;</span>] (score <span class="number">0.001451</span>)</span><br><span class="line"><span class="string">&quot;hugs&quot;</span>: [<span class="string">&quot;hug&quot;</span>, <span class="string">&quot;s&quot;</span>] (score <span class="number">0.001701</span>)</span><br></pre></td></tr></table></figure>
<p>所以损失是:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10</span> * (-log(<span class="number">0.071428</span>)) + <span class="number">5</span> * (-log(<span class="number">0.007710</span>)) + <span class="number">12</span> * (-log(<span class="number">0.006168</span>)) + <span class="number">4</span> * (-log(<span class="number">0.001451</span>)) + <span class="number">5</span> * (-log(<span class="number">0.001701</span>)) = <span class="number">169.8</span></span><br></pre></td></tr></table></figure>
<p>现在我们需要计算删除每个标记如何影响损失。这相当乏味,所以我们在这里只对两个标记进行操作,并保存整个过程以备有代码来帮助我们。在这个(非常)特殊的情况下,我们对所有单词有两个等效的标记:正如我们之前看到的,例如, <code>&quot;pug&quot;</code> 可以以相同的分数被标记为 <code>[&quot;p&quot;, &quot;ug&quot;]</code>。因此,去除词汇表中的 <code>&quot;pu&quot;</code> 标记将给出完全相同的损失。</p>
<p>另一方面,去除 <code>&quot;hug&quot;</code> 损失变得更糟, 因为 <code>&quot;hug&quot;</code> 和 <code>&quot;hugs&quot;</code> 的标记化会变成:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;hug&quot;</span>: [<span class="string">&quot;hu&quot;</span>, <span class="string">&quot;g&quot;</span>] (score <span class="number">0.006802</span>)</span><br><span class="line"><span class="string">&quot;hugs&quot;</span>: [<span class="string">&quot;hu&quot;</span>, <span class="string">&quot;gs&quot;</span>] (score <span class="number">0.001701</span>)</span><br></pre></td></tr></table></figure>
<p>这些变化将导致损失增加:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- <span class="number">10</span> * (-log(<span class="number">0.071428</span>)) + <span class="number">10</span> * (-log(<span class="number">0.006802</span>)) = <span class="number">23.5</span></span><br></pre></td></tr></table></figure>
<p>因此, 标记 <code>&quot;pu&quot;</code>可能会从词汇表中删除,但不会删除 <code>&quot;hug&quot;</code>.</p>
<h3 id="4实现unigram"><a class="markdownIt-Anchor" href="#4实现unigram"></a> 4.实现Unigram</h3>
<p>现在让我们在代码中实现我们迄今为止看到的所有内容。与 BPE 和 WordPiece 一样,这不是 Unigram 算法的有效实现(恰恰相反),但它应该可以帮助你更好地理解它。</p>
<p><font size="5">选择语料库</font></p>
<p>我们将使用与之前相同的语料库作为示例:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">corpus = [</span><br><span class="line">    <span class="string">&quot;This is the Hugging Face Course.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This chapter is about tokenization.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This section shows several tokenizer algorithms.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p><font size="5">选择tokenizer</font></p>
<p>这一次,我们将使用 <code>xlnet-base-cased</code> 作为我们的模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;xlnet-base-cased&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">计算语料库中每个单词的出现次数</font></p>
<p>与 BPE 和 WordPiece 一样,我们首先计算语料库中每个单词的出现次数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">word_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    new_words = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> new_words:</span><br><span class="line">        word_freqs[word] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">word_freqs</span><br><span class="line"></span><br><span class="line">defaultdict(<span class="built_in">int</span>,</span><br><span class="line">            &#123;<span class="string">&#x27;▁This&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">             <span class="string">&#x27;▁is&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">             <span class="string">&#x27;▁the&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁Hugging&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁Face&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁Course.&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁chapter&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁about&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁tokenization.&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁section&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁shows&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁several&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁tokenizer&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁algorithms.&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁Hopefully,&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁you&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁will&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁be&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁able&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁to&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁understand&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁how&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁they&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁are&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁trained&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁and&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁generate&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">             <span class="string">&#x27;▁tokens.&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure>
<p><font size="5">词汇表初始化,按频率进行排序</font></p>
<p><font color="red">然后,我们需要将我们的词汇表初始化为大于我们最终想要的词汇量。我们必须包含所有基本字符(否则我们将无法标记每个单词),但对于较大的子字符串,我们将只保留最常见的字符,因此我们按频率对它们进行排序:</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">char_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">subwords_freqs = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"><span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word)):</span><br><span class="line">        char_freqs[word[i]] += freq <span class="comment"># 单个字符频率</span></span><br><span class="line">        <span class="comment"># Loop through the subwords of length at least 2</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">2</span>, <span class="built_in">len</span>(word) + <span class="number">1</span>): <span class="comment"># 两个以上字符的频率</span></span><br><span class="line">            subwords_freqs[word[i:j]] += freq</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sort subwords by frequency</span></span><br><span class="line">sorted_subwords = <span class="built_in">sorted</span>(subwords_freqs.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>) <span class="comment"># 按频率从大到小排序</span></span><br><span class="line">sorted_subwords[:<span class="number">10</span>]</span><br><span class="line">[(<span class="string">&#x27;▁t&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;is&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;er&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;▁a&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;▁to&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;en&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;▁T&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;▁Th&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;▁Thi&#x27;</span>, <span class="number">3</span>)]</span><br></pre></td></tr></table></figure>
<p>我们用最优的子词对字符进行分组,以获得大小为 300 的初始词汇表:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">token_freqs = <span class="built_in">list</span>(char_freqs.items()) + sorted_subwords[: <span class="number">300</span> - <span class="built_in">len</span>(char_freqs)] <span class="comment"># 单个字符加上频率由高到低的多字符，总共300</span></span><br><span class="line">token_freqs = &#123;token: freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><font color="red">💡 SentencePiece 使用一种称为增强后缀数组(ESA)的更高效算法来创建初始词汇表。</font></p>
</blockquote>
<p><font size="5">计算所有频率的总和，将频率转换为概率</font></p>
<p>接下来，我们计算所有频率的总和，将频率转换为概率。对于我们的模型，我们将存储概率的对数，因为对数相加比小数相乘在数值上更稳定<font color="YellowGreen">（好像在位置编码那篇博客里看过）</font>，这将简化模型损失的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line">total_sum = <span class="built_in">sum</span>([freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()])</span><br><span class="line">model = &#123;token: -log(freq / total_sum) <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()&#125;</span><br></pre></td></tr></table></figure>
<p><font size="5">使用<strong>维特比算法</strong>对单词进行标记</font></p>
<p>现在的主要功能是使用<strong>维特比算法</strong>对单词进行标记。正如我们之前所看到的，该算法计算单词的每个子串的最佳分割方式，我们将其存储在名为<code>best_segmentations</code>的变量中。<font color="red">我们将为单词中的每个位置（从 0 到其总长度）存储一个字典，有两个键：最佳分割中最后一个标记的开头索引，以及最佳分割的分数。通过最后一个标记开始的索引，一旦列表完全填充，我们将能够检索完整的分段。</font></p>
<p><font color="red">只需两个循环即可填充列表：主循环遍历每个起始位置，第二个循环尝试从该起始位置开始的所有子字符串。如果子字符串在词汇表中，我们会对单词进行新的分段，直到结束位置，</font>我们将其与<code>best_segmentations</code>中的内容进行比较。</p>
<p>主循环完成后，我们只需从末尾开始，从一个起始位置跳到下一个起始位置，同时记录标记，直到到达单词的开头：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">encode_word</span>(<span class="params">word, model</span>):</span><br><span class="line">    best_segmentations = [&#123;<span class="string">&quot;start&quot;</span>: <span class="number">0</span>, <span class="string">&quot;score&quot;</span>: <span class="number">1</span>&#125;] + [</span><br><span class="line">        &#123;<span class="string">&quot;start&quot;</span>: <span class="literal">None</span>, <span class="string">&quot;score&quot;</span>: <span class="literal">None</span>&#125; <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word))</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">for</span> start_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word)):</span><br><span class="line">        <span class="comment"># This should be properly filled by the previous steps of the loop</span></span><br><span class="line">        best_score_at_start = best_segmentations[start_idx][<span class="string">&quot;score&quot;</span>]</span><br><span class="line">        <span class="keyword">for</span> end_idx <span class="keyword">in</span> <span class="built_in">range</span>(start_idx + <span class="number">1</span>, <span class="built_in">len</span>(word) + <span class="number">1</span>):</span><br><span class="line">            token = word[start_idx:end_idx]</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> model <span class="keyword">and</span> best_score_at_start <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                score = model[token] + best_score_at_start</span><br><span class="line">                <span class="comment"># If we have found a better segmentation ending at end_idx, we update</span></span><br><span class="line">                <span class="keyword">if</span> (</span><br><span class="line">                    best_segmentations[end_idx][<span class="string">&quot;score&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">                    <span class="keyword">or</span> best_segmentations[end_idx][<span class="string">&quot;score&quot;</span>] &gt; score</span><br><span class="line">                ):</span><br><span class="line">                    best_segmentations[end_idx] = &#123;<span class="string">&quot;start&quot;</span>: start_idx, <span class="string">&quot;score&quot;</span>: score&#125;</span><br><span class="line"></span><br><span class="line">    segmentation = best_segmentations[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> segmentation[<span class="string">&quot;score&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># We did not find a tokenization of the word -&gt; unknown</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="string">&quot;&lt;unk&gt;&quot;</span>], <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    score = segmentation[<span class="string">&quot;score&quot;</span>]</span><br><span class="line">    start = segmentation[<span class="string">&quot;start&quot;</span>]</span><br><span class="line">    end = <span class="built_in">len</span>(word)</span><br><span class="line">    tokens = []</span><br><span class="line">    <span class="keyword">while</span> start != <span class="number">0</span>:</span><br><span class="line">        tokens.insert(<span class="number">0</span>, word[start:end])</span><br><span class="line">        next_start = best_segmentations[start][<span class="string">&quot;start&quot;</span>]</span><br><span class="line">        end = start</span><br><span class="line">        start = next_start</span><br><span class="line">    tokens.insert(<span class="number">0</span>, word[start:end])</span><br><span class="line">    <span class="keyword">return</span> tokens, score</span><br></pre></td></tr></table></figure>
<p>我们已经可以在一些词上尝试我们的初始模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;Hopefully&quot;</span>, model))</span><br><span class="line"><span class="built_in">print</span>(encode_word(<span class="string">&quot;This&quot;</span>, model))</span><br><span class="line">([<span class="string">&#x27;H&#x27;</span>, <span class="string">&#x27;o&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;ll&#x27;</span>, <span class="string">&#x27;y&#x27;</span>], <span class="number">41.5157494601402</span>)</span><br><span class="line">([<span class="string">&#x27;This&#x27;</span>], <span class="number">6.288267030694535</span>)</span><br></pre></td></tr></table></figure>
<p><font size="5">计算模型在语料库上的损失</font></p>
<p>现在很容易计算模型在语料库上的损失!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_loss</span>(<span class="params">model</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> word_freqs.items():</span><br><span class="line">        _, word_loss = encode_word(word, model)</span><br><span class="line">        loss += freq * word_loss</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>我们可以检查它是否适用于我们拥有的模型:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">compute_loss(model)</span><br><span class="line"></span><br><span class="line"><span class="number">413.10377642940875</span></span><br></pre></td></tr></table></figure>
<p><font size="5">计算通过删除每个标记获得的模型的损失</font></p>
<p>计算每个标记的分数也不是很难;我们只需要计算通过删除每个标记获得的模型的损失:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_scores</span>(<span class="params">model</span>):</span><br><span class="line">    scores = &#123;&#125;</span><br><span class="line">    model_loss = compute_loss(model)</span><br><span class="line">    <span class="keyword">for</span> token, score <span class="keyword">in</span> model.items():</span><br><span class="line">        <span class="comment"># We always keep tokens of length 1,即不删除基本字符</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(token) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        model_without_token = copy.deepcopy(model)</span><br><span class="line">        _ = model_without_token.pop(token)</span><br><span class="line">        scores[token] = compute_loss(model_without_token) - model_loss</span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<p>我们可以在给定的标记上尝试:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores = compute_scores(model)</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;ll&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(scores[<span class="string">&quot;his&quot;</span>])</span><br></pre></td></tr></table></figure>
<p>自从 <code>&quot;ll&quot;</code> 用于标记化 <code>&quot;Hopefully&quot;</code>, 删除它可能会让我们使用标记 <code>&quot;l&quot;</code> 两次相反,我们预计它将产生正损失。 <code>&quot;his&quot;</code> 仅在单词<code>&quot;This&quot;</code> 内使用,它被标记为自身,所以我们期望它的损失为零。结果如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">6.376412403623874</span></span><br><span class="line"><span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>💡 这种方法非常低效,因此 SentencePiece 使用了没有token X 的模型损失的近似值:它不是从头开始,而是通过其在剩余词汇表中的分段替换标记 X。这样,所有分数可以与模型损失同时计算。</p>
</blockquote>
<p><font color="red">完成所有这些后,我们需要做的最后一件事是将模型使用的特殊标记添加到词汇表中,然后循环直到我们从词汇表中修剪了足够的标记以达到我们想要的大小:</font></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">percent_to_remove = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">len</span>(model) &gt; <span class="number">100</span>:</span><br><span class="line">    scores = compute_scores(model)</span><br><span class="line">    sorted_scores = <span class="built_in">sorted</span>(scores.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># Remove percent_to_remove tokens with the lowest scores.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(<span class="built_in">len</span>(model) * percent_to_remove)):</span><br><span class="line">        _ = token_freqs.pop(sorted_scores[i][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    total_sum = <span class="built_in">sum</span>([freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()])</span><br><span class="line">    model = &#123;token: -log(freq / total_sum) <span class="keyword">for</span> token, freq <span class="keyword">in</span> token_freqs.items()&#125;</span><br></pre></td></tr></table></figure>
<p>然后,为了标记一些文本,我们只需要应用预标记化,然后使用我们的 <code>encode_word()</code> 函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text, model</span>):</span><br><span class="line">    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)</span><br><span class="line">    pre_tokenized_text = [word <span class="keyword">for</span> word, offset <span class="keyword">in</span> words_with_offsets]</span><br><span class="line">    encoded_words = [encode_word(word, model)[<span class="number">0</span>] <span class="keyword">for</span> word <span class="keyword">in</span> pre_tokenized_text]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(encoded_words, [])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenize(<span class="string">&quot;This is the Hugging Face course.&quot;</span>, model)</span><br><span class="line">[<span class="string">&#x27;▁This&#x27;</span>, <span class="string">&#x27;▁is&#x27;</span>, <span class="string">&#x27;▁the&#x27;</span>, <span class="string">&#x27;▁Hugging&#x27;</span>, <span class="string">&#x27;▁Face&#x27;</span>, <span class="string">&#x27;▁&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;ou&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;.&#x27;</span>]</span><br></pre></td></tr></table></figure>
</center></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">HUI</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/09/20/NLP_Course(6.2)/">http://example.com/2024/09/20/NLP_Course(6.2)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">HUI</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/87788970_p0_master1200.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/09/20/NLP_Course(6.3)/" title="NLP课程（六-下）- 逐块构建分词器"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">NLP课程（六-下）- 逐块构建分词器</div></div></a></div><div class="next-post pull-right"><a href="/2024/09/20/NLP_Course(6.1)/" title="NLP课程（六-上）- Tokenizer库"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">NLP课程（六-上）- Tokenizer库</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/87788970_p0_master1200.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">HUI</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#nlp%E8%AF%BE%E7%A8%8B%E5%85%AD-%E4%B8%AD-%E4%B8%89%E7%A7%8D%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="toc-text"> NLP课程（六-中）- 三种标记化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%97%E8%8A%82%E5%AF%B9%E7%BC%96%E7%A0%81%E6%A0%87%E8%AE%B0%E5%8C%96bpe"><span class="toc-text"> 字节对编码标记化(BPE)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95"><span class="toc-text"> 1.训练算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E6%A0%87%E8%AE%B0%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-text"> 2.标记化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%AE%9E%E7%8E%B0bpe"><span class="toc-text"> 3.实现BPE</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#wordpiece-%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="toc-text"> WordPiece 标记化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95-2"><span class="toc-text"> 1.训练算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E6%A0%87%E8%AE%B0%E5%8C%96%E7%AE%97%E6%B3%95-2"><span class="toc-text"> 2.标记化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%AE%9E%E7%8E%B0wordpiece"><span class="toc-text"> 3.实现WordPiece</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unigram%E6%A0%87%E8%AE%B0%E5%8C%96"><span class="toc-text"> Unigram标记化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E8%AE%AD%E7%BB%83%E7%AE%97%E6%B3%95-3"><span class="toc-text"> 1.训练算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E6%A0%87%E8%AE%B0%E5%8C%96%E7%AE%97%E6%B3%95-3"><span class="toc-text"> 2.标记化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E5%9B%9E%E5%88%B0%E8%AE%AD%E7%BB%83"><span class="toc-text"> 3.回到训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E5%AE%9E%E7%8E%B0unigram"><span class="toc-text"> 4.实现Unigram</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/09/25/SwinTransformer-code/" title="SwinTransformer代码详解"><img src="/img/183711454.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SwinTransformer代码详解"/></a><div class="content"><a class="title" href="/2024/09/25/SwinTransformer-code/" title="SwinTransformer代码详解">SwinTransformer代码详解</a><time datetime="2024-09-25T07:13:45.000Z" title="发表于 2024-09-25 15:13:45">2024-09-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/23/SwinTransformer/" title="SwinTransformer论文精读"><img src="/img/172840755.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SwinTransformer论文精读"/></a><div class="content"><a class="title" href="/2024/09/23/SwinTransformer/" title="SwinTransformer论文精读">SwinTransformer论文精读</a><time datetime="2024-09-23T07:07:46.000Z" title="发表于 2024-09-23 15:07:46">2024-09-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/22/Diffusion_Course(1)/" title="Diffusion课程（一）- 介绍">Diffusion课程（一）- 介绍</a><time datetime="2024-09-22T09:47:07.000Z" title="发表于 2024-09-22 17:47:07">2024-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/22/Bert/" title="Bert原文笔记"><img src="/img/IEKTIWS4.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bert原文笔记"/></a><div class="content"><a class="title" href="/2024/09/22/Bert/" title="Bert原文笔记">Bert原文笔记</a><time datetime="2024-09-22T06:48:25.000Z" title="发表于 2024-09-22 14:48:25">2024-09-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/09/21/hexo/" title="使用hexo+github搭建个人博客"><img src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="使用hexo+github搭建个人博客"/></a><div class="content"><a class="title" href="/2024/09/21/hexo/" title="使用hexo+github搭建个人博客">使用hexo+github搭建个人博客</a><time datetime="2024-09-21T09:41:00.000Z" title="发表于 2024-09-21 17:41:00">2024-09-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By HUI</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat-btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2024/09/20/NLP_Course(6.2)/'
    this.page.identifier = '/2024/09/20/NLP_Course(6.2)/'
    this.page.title = 'NLP课程（六-中）- 三种标记化'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      GLOBAL_CONFIG_SITE.isPost && getCount()
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>