<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer原文笔记 | 歪比巴卜</title><meta name="author" content="歪比巴卜"><meta name="copyright" content="歪比巴卜"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer原文笔记Transformer中Self-Attention以及Multi-Head Attention详解_哔哩哔哩_bilibili 详解Transformer中Self-Attention以及Multi-Head Attention_transformer multi head-CSDN博客 Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili 摘">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer原文笔记">
<meta property="og:url" content="http://example.com/2024/09/13/Transformer/index.html">
<meta property="og:site_name" content="歪比巴卜">
<meta property="og:description" content="Transformer原文笔记Transformer中Self-Attention以及Multi-Head Attention详解_哔哩哔哩_bilibili 详解Transformer中Self-Attention以及Multi-Head Attention_transformer multi head-CSDN博客 Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili 摘">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg">
<meta property="article:published_time" content="2024-09-13T13:14:01.000Z">
<meta property="article:modified_time" content="2024-09-16T10:11:16.426Z">
<meta property="article:author" content="歪比巴卜">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/09/13/Transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer原文笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-16 18:11:16'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="歪比巴卜"><span class="site-name">歪比巴卜</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer原文笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-13T13:14:01.000Z" title="发表于 2024-09-13 21:14:01">2024-09-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-16T10:11:16.426Z" title="更新于 2024-09-16 18:11:16">2024-09-16</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer原文笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Transformer原文笔记"><a href="#Transformer原文笔记" class="headerlink" title="Transformer原文笔记"></a>Transformer原文笔记</h1><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15v411W78M/?spm_id_from=333.999.0.0&vd_source=6f5eec9fed4d83b5f5ededb2b9c7f548">Transformer中Self-Attention以及Multi-Head Attention详解_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/117691873">详解Transformer中Self-Attention以及Multi-Head Attention_transformer multi head-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.337.search-card.all.click&vd_source=6f5eec9fed4d83b5f5ededb2b9c7f548">Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><span style="color: #ff2020">主要序列转导模型基于复杂的循环或卷积神经网络，包括编码器和解码器。性能最好的模型还通过注意力机制连接编码器和解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制，完全不需要递归和卷积。对两个机器翻译任务的实验表明，这些模型具有卓越的质量，同时具有更高的并行性，并且需要的训练时间显着减少。</span>我们的模型在 WMT 2014 英德翻译任务中获得了 28.4 BLEU，比现有的最佳结果（包括集成）提高了 2 BLEU 以上。在 WMT 2014 英法翻译任务中，我们的模型在 8 个 GPU 上训练 3.5 天后，建立了新的单模型最先进 BLEU 分数 41.8，这只是最佳模型训练成本的一小部分文献中的模型。我们通过将 Transformer 成功应用于具有大量和有限训练数据的英语选区解析，证明 Transformer 可以很好地推广到其他任务。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>循环神经网络，特别是<span style="color: #ffcb00">长短期记忆（LSTM)</span> 和<span style="color: #ffcb00">门控循环 (gRNN）</span> 神经网络，已被牢固地确立为序列建模和转导问题（例如语言建模和机器翻译）中最先进的方法。此后，人们做出了许多努力，不断突破循环语言模型和编码器-解码器架构的界限。</p>
<p><span style="color: #ff2020">循环模型通常会沿着输入和输出序列的符号位置进行计算。将位置与计算时间中的步骤对齐，它们生成一系列隐藏状态 h</span><sub>t</sub><span style="color: #ff2020">，作为先前隐藏状态 h</span><sub>t−1</sub><span style="color: #ff2020"> 和位置 t 输入的函数。这种固有的顺序性质阻碍了训练示例中的并行化（h</span><sub>t</sub><span style="color: #ff2020">必须等h</span><sub>t-1</sub><span style="color: #ff2020">计算完），这在较长的序列长度上变得至关重要，因为内存限制限制了示例之间的批处理（就是并行处理）。</span>最近的工作通过分解技巧和条件计算显着提高了计算效率，同时还提高了后者的模型性能。然而，顺序计算的基本限制仍然存在。</p>
<p>注意力机制已经成为各种任务中引人注目的序列建模和转导模型的一个组成部分，允许对依赖关系进行建模，而不考虑它们在输入或输出序列中的距离。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。</p>
<p><span style="color: #ff2020">在这项工作中，我们提出了 Transformer，这是一种避免重复的模型架构，而是完全依赖注意力机制来绘制输入和输出之间的全局依赖关系。 Transformer 允许显着提高并行度，并且在 8 个 P100 GPU 上进行短短 12 小时的训练后，可以在翻译质量方面达到新的水平。</span></p>
<p><span style="color: #4eb31c">（顺序计算限制了并行处理的能力，完全利用注意力机制可以提高并行度）</span></p>
<h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2 背景"></a>2 背景</h2><p><span class="citation" data-citation="%7B%22citationItems%22%3A%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F13051947%2Fitems%2FCIFFGW26%22%5D%2C%22locator%22%3A%222%22%7D%5D%2C%22properties%22%3A%7B%7D%7D" ztype="zcitation">(<span class="citation-item"><a href="zotero://select/library/items/CIFFGW26">Vaswani 等, 2023, p. 2</a></span>)</span> 减少顺序计算的目标也构成了扩展神经 GPU 、ByteNet和 ConvS2S的基础，所有这些都使用<span style="color: #ffcb00">卷积神经网络</span>作为基本构建块，并行计算所有输入和的隐藏表示。输出位置。<span style="color: #ff2020">在这些模型中，关联来自两个任意输入或输出位置的信号所需的操作数量随着位置之间的距离而增加，对于 ConvS2S 呈线性增长，对于 ByteNet 呈对数增长。这使得学习遥远位置之间的依赖关系变得更加困难。</span>在 Transformer 中，这被减少到恒定数量的操作，尽管由于平均注意力加权位置而导致有效分辨率降低，我们用多头注意力（模拟卷积神经网络多输出通道的效果）来抵消这种影响，如第 3.2 节所述。</p>
<p><span style="color: #ffcb00">自注意力（有时称为内部注意力）</span>是一种将单个序列的不同位置相关联的注意力机制，以便计算序列的表示。自注意力已成功应用于各种任务，包括阅读理解、抽象概括、文本蕴涵和学习与任务无关的句子表示。</p>
<p>端到端记忆网络基于循环注意机制而不是序列对齐循环，并且已被证明在简单语言问答和语言建模任务上表现良好。</p>
<p>然而，据我们所知，<span style="color: #ff2020">Transformer 是第一个完全依赖自注意力</span>来计算其输入和输出表示而不使用序列对齐 RNN 或卷积的转换模型。在接下来的章节中，我们将描述 Transformer，激发 self-attention 并讨论其相对于其他模型的优势。</p>
<h2 id="3-模型架构"><a href="#3-模型架构" class="headerlink" title="3 模型架构"></a>3 模型架构</h2><p><span style="color: #ff2020">大多数竞争性神经序列转导模型都具有编码器-解码器结构</span>。这里，<span style="color: #ffcb00">编码器</span>将符号表示的输入序列 (x1, …, xn) 映射到连续表示序列 z &#x3D; (z1, …, zn)。<span style="color: #ff2020">z</span><sub>n</sub><span style="color: #ff2020">是x</span><sub>n</sub><span style="color: #ff2020">的向量表示。</span>给定 z，解码器然后每次生成一个元素的符号输出序列 (y1, …, ym)。（<span style="color: #ff2020">注意n和m是不同的，解码器是一个一个生成的，而编码器有可能可以一次看清整个句子）</span>在每个步骤中，模型都是自回归的，在生成下一个时将先前生成的符号用作附加输入<span style="color: #ff2020">（即输入又是你的输出）</span>。</p>
<p><span style="color: #ff2020">Transformer 遵循这一整体架构，对编码器和解码器使用堆叠自注意力和逐点、全连接层，</span>分别如图 1 的左半部分<span style="color: #ffcb00">（编码器）</span>和右半部分<span style="color: #ffcb00">（解码器，outputs实际是输入，解码器在做预测的时候是没有输入的，它实际上是解码器在之前一些时刻的输出）</span>所示。</p>
<p>![2024-08-22 140622](.&#x2F;Transformer.&#x2F;2024-08-22 140622.png)</p>
<center>图1 Transformer模型架构


<h3 id="3-1-编码器和解码器堆栈"><a href="#3-1-编码器和解码器堆栈" class="headerlink" title="3.1 编码器和解码器堆栈"></a>3.1 编码器和解码器堆栈</h3><p><strong>编码器：</strong>编码器由 <span style="color: #ffcb00">N &#x3D; 6</span> 个相同层的堆栈组成。每层有两个子层。第一个是<span style="color: #ffcb00">多头自注意力机制（图中的Multi-Head Attention)</span>，第二个是简单的<span style="color: #ffcb00">位置全连接前馈网络(图中的 Feed Forward，其实就是mlp）。</span>我们在两个子层周围采用<span style="color: #ffcb00">残差连接（图中的Add &#x26; Norm）</span>，然后进行层归一化。即每个子层的输出为<span style="color: #ffcb00">LayerNorm(x + Sublayer(x))</span>，其中<span style="color: #ffcb00">Sublayer(x)是子层本身实现的函数</span>。为了促进这些残差连接<span style="color: #ffcb00">（残差连接要维度相同）</span>，模型中的所有子层以及嵌入层都会生成维度 <span style="color: #ffcb00">dmodel &#x3D; 512</span> 的输出。</p>
<p><span style="color: #4eb31c">（*batchnorm（变长应用不使用batchnorm）和layernorm的区别：考虑最简单二维输入的情况，每一行是一个样本，每一列是一个特征，batchnorm是每一次把我的每一个特征在一个小的mini-batch（特征所指的列向量）里面均值变成0，方差变为1，具体操作就是减去均值然后除以方差，训练的时候是小批量，预测的时候会把全局均值算出来），batchnrom还可以学习两个参数使得可以将其放成任意均值任意方差的向量。layernorm是把每一个样本（行向量）变成均值为0，方差为1。</span></p>
<p><span style="color: #4eb31c">但是在RNN或者Transformer里面它的输入是三维（一个输入是一个句子，句子有n个词，一个词是一个向量，再加上batch（样本）就是一个3d的东西）batch（多少句话）：样本个数，seq（一句话里有多少词，n）：序列长度，feature（d）：词的特征或者说词所表示的向量，如果继续用batchnorm，对每个特征取（垂直平面，蓝色线），layernorm是对batch取（水平平面，黄色线）。对于时序序列，每一个batch（句子）它的长度可能是不一样的（缺的补0），batchnorm在句子长度变化较大的情况下它的抖动相对较大，因为在做预测示要全局均值和方差，在预测时雨果遇上了一个很长的句子，那么之前在训练过程中没遇到这么长的句子，在预测是会不那么好用。layernorm是每个样本自己来算我的均值和方差，因而也不用存全局均值和方差，不管样本长还是短，它相对稳定些。）</span></p>
<p><img src="/.com//62N3ENFN.png"></p>
<p><strong>解码器：</strong><span style="color: #ff2020">解码器也是由 N &#x3D; 6 个相同层的堆栈组成。除了每个编码器层中的两个子层之外，解码器还插入第三个子层（Masked Multi-head attention），该子层对编码器堆栈的输出执行多头注意力。与编码器类似，我们在每个子层周围采用残差连接，然后进行层归一化。</span>我们还修改了解码器堆栈中的自注意力子层，以防止预测t位置关注后续位置<span style="color: #4eb31c">（在解码器训练时，在预测t时刻的输出时，不应该看到t时刻之后的输入，在注意力机制能它每一次能看到整个完整的输入）</span>。这种掩蔽与输出嵌入偏移一个位置的事实相结合，确保位置 i 的预测只能依赖于小于 i 的位置处的已知输出。</p>
<h3 id="3-2-注意力机制"><a href="#3-2-注意力机制" class="headerlink" title="3.2 注意力机制"></a>3.2 注意力机制</h3><p>注意力函数可以描述为将<span style="color: #ffcb00">查询</span>和一组<span style="color: #ffcb00">键值对</span>映射到输出，其中<span style="color: #ffcb00">查询Q</span>、<span style="color: #ffcb00">键K</span>、<span style="color: #ffcb00">值V</span>和输出都是向量。output是Value的加权和所以输出的维度和value是一样的。其中分配给每个值（value）的权重是通过查询与相应键的<span style="color: #ffcb00">兼容性函数</span><span style="color: #4eb31c">（相似度，不同的注意力机制由不同的算法）</span>计算的。<span style="color: #4eb31c">例子如下：给定三个V，三个K，有一个query离第一个K比较近，输出是三个V相加，但是第一个V的权重会大一些(可以理解为value是分数，query表示你更想看谁的分数）</span></p>
<p><img src="/.com//ZDNZ6GGS.png" alt="ZDNZ6GGS"></p>
<p><img src="/.com//YNBLK3ZX.png" alt="YNBLK3ZX"></p>
<p>PS：左图MatMul是矩阵乘法，Scale是指除以根号dk，mask<span style="color: #4eb31c">（Mask是为了避免在第t时间看到t时间之后的东西。假设Q,K同长都为n，且时间上是能对应起来的，那么对于t时刻Q</span><sub>t</sub><span style="color: #4eb31c">，那么做计算时应该看K</span><sub>1</sub><span style="color: #4eb31c">到K</span><sub>t-1，</sub><span style="color: #4eb31c">但是在注意力机制中是能从K</span><sub>1</sub><span style="color: #4eb31c">看到K</span><sub>n</sub><span style="color: #4eb31c">的，其实计算的时候可以全部都算，只要保证输出的时候不要用到K</span><sub>t</sub><span style="color: #4eb31c">到K</span><sub>n</sub><span style="color: #4eb31c">即可，所以把Qt与Kt之后计算的那些值换成一个很大的负数，即负的很多次方：-1e</span><sup>10</sup><span style="color: #4eb31c">，这样在softmax取对数之后它会变成0，保证了t时刻的Q只能看到1到t-1时刻的K-V对）</span>之后做SoftMax，之后再跟V做矩阵乘法。</p>
<p><span style="color: #4eb31c">右图QKV先经过一个线性层投影（就是卷积）到较低的维度，Scale Dot-Product Attention就是左图整个部分，做h次会有h个输出，之后把输出concat（合并到一起）最后经过一个线性层输出。（之所以要这么做是因为左图没有什么能够学习到的参数，有时候为了学习不一样的模式，希望有一些不一样的计算像素的办法，右图通过线性层投影或者说卷积能够学习卷积核的参数w，希望能够学习到不一样的投影方法，使得投影进去的度量函数里面能够匹配它需要的一些相似函数）</span></p>
<h3 id="3-2-1-Scaled-Dot-Product-Attention（点乘注意力机制）"><a href="#3-2-1-Scaled-Dot-Product-Attention（点乘注意力机制）" class="headerlink" title="3.2.1 Scaled Dot-Product Attention（点乘注意力机制）"></a><strong>3.2.1 Scaled Dot-Product Attention（点乘注意力机制）</strong></h3><p><img src="/.com//RBDKEJZH.png"></p>
<p>假设输入的序列长度为2，输入就两个节点x<sub>1</sub> , x<sub>2</sub>，然后通过<span style="color: #ffcb00">Input Embedding（Embedding层也是嵌入层，它是可以理解为一种通过矩阵乘法对数据进行升维或者降维的层）</span>也就是图中的f (x)将输入映射到a<sub>1 </sub>, a<sub>2</sub>。紧接着分别将a<sub>1 </sub>, a<sub>2</sub>分别通过三个变换矩阵Wq , W<sub>k</sub> , W<sub>v</sub> （这三个参数是可训练的，对于所有的a是共享的）得到对应的q<sub>i</sub> , k<sub>i</sub> , v<sub>i</sub>（Wq , W<sub>k</sub> , W<sub>v</sub> 在源码中是直接使用全连接层实现的，这里为了方便理解，忽略偏执）。</p>
<p>其中</p>
<p>·q代表query，后续会去和每一个k进行匹配</p>
<p>·k代表key，后续会被每个q匹配</p>
<p>·v代表从a中提取得到的信息</p>
<p>·后续q和k匹配的过程可以理解成计算两者的相关性，相关性越大对应v的权重也就越大</p>
<p>为了方便理解，这里假设了a<sub>1</sub>,W<sup>q</sup>从而通过矩阵乘法得到q<sup>1</sup>。<span style="color: rgb(77, 77, 77)"><span style="background-color: rgb(255, 255, 255)">Transformer是可以并行化的，所以将</span></span>a<sub>1 </sub>, a<sub>2</sub>拼接起来可以得到拼接起来的q<sup>1</sup>，q<sup>2</sup>。也就是原论文中的Q，KV同理。</p>
<p><img src="/.com//JEW3PTXL.png"></p>
<p><span style="color: #ff2020">接着先拿q</span><sup>1</sup><span style="color: #ff2020">和每个k进行match，即点乘（注意是点乘）操作</span>，接着除以根号d得到对应的α，其中根号d代表向量k<sup>i </sup>的长度，在本示例中等于2，除以d的原因在论文中的解释是“进行点乘后的数值很大，导致通过softmax后梯度变的很小”，所以通过除以根号d来进行缩放。softmax之后乘上V矩阵：</p>
<p><img src="/.com//YKUPKUPJ.png"></p>
<p>从而实现了Attention（Q,K,V），见图片右下角。</p>
<p><img src="/.com//X329GWLF.png"></p>
<p>我们将我们的特别注意力机制称为“缩放点积注意力”（图 2）。输入由维度 dk 的查询q和键k以及维度 dv 的值v组成(故输出也是dv）。我们使用所有<span style="color: #ffcb00">键k</span>和<span style="color: #ffcb00">查询q</span>的点积（内积）来计算相似度，将<span style="color: #ffcb00">每个键除以 √dk</span>（防止内积过大），然后应用 <span style="color: #ffcb00">softmax 函数</span>来获取值的权重。</p>
<p>在实践中，我们同时计算一组查询的注意力函数，将其打包到矩阵 Q 中。键和值也打包到矩阵 K 和 V 中。我们将输出矩阵计算为：</p>
<p><img src="/.com//K5GN8YKH.png"></p>
<p><img src="/.com//53TMMHUZ.png"></p>
<p>QK<sup>T</sup>是一个n*m矩阵，softmax是对矩阵的每一行做softxmax，V是m行d<sub>v</sub>列的矩阵，输出是n*d<sub>v</sub></p>
<p><span style="color: #ff2020">两种最常用的注意力函数是</span><span style="color: #ffcb00">加性注意力 [2] </span><span style="color: #ff2020">和</span><span style="color: #ffcb00">点积（乘法）注意力</span>。除了缩放因子 1 √dk 之外，点积注意力与我们的算法相同。加性注意力使用具有单个隐藏层的前馈网络来计算兼容性函数（相似度）。<span style="color: #ff2020">虽然两者在理论复杂性上相似，但点积注意力在实践中更快、更节省空间，因为它可以使用高度优化的矩阵乘法代码来实现。</span></p>
<p><span style="color: #ff2020">虽然对于较小的dk值，两种机制的表现相似，但dk比较大时，加性注意力优于点积注意力，若不对较大的dk值进行缩放 [3]。我们怀疑，对于较大的dk值，点积的幅度会变大，从而将 softmax函数推入梯度极小的区域 。为了抵消这种影响，我们将点积缩放1√dk 。</span><span style="color: #4eb31c">（向量长度比较长时,点积的值会比较大，会导致点积比较大的值在softmax后更加靠近于1，剩下的更加靠近于0，差距比较大，会导致算梯度时梯度比较小，因为softmax希望置信的更靠近1，不置信的更靠近0，这样导致初始的时候就接近了收敛的情况，使得梯度较小。)</span></p>
<h3 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a><strong>3.2.2 Multi-Head Attention</strong></h3><p>我们发现，使用不同的学习线性投影将查询、键和值分别线性投影到 dk、dk 和 dv 维度 h 次是有益的，而不是使用 dmodel 维度的键、值和查询执行单个注意力函数<span style="color: #4eb31c">（与其做单个的注意力函数，不如把QKV投影到低维h次，然后再做h次的注意力函数，每一个函数的输出并在一起，再投影回来得到我们最终的输出）</span>。然后，我们对查询、键和值的每个投影版本并行执行注意力函数，产生dv维输出值。将它们连接并再次投影，得到最终值，如图 2 所示。</p>
<p>多头注意力允许模型共同关注来自不同位置的不同表示子空间的信息。对于单一注意力头，平均会抑制这种情况。</p>
<p><img src="/.com//ZRWK4TZB.png"></p>
<p>其中投影是参数矩阵 W<sup>Q</sup> <sub>i</sub> ∈ R<sup>dmodel×dk</sup> 、 W<sup>K</sup> <sub>i</sub> ∈ R<sup>dmodel×dk</sup> 、 W<sup>V</sup><sub>i </sub>∈ R<sup>dmodel×dv</sup> 和 W<sup>O</sup>∈ R<sup>hdv×dmodel</sup>。</p>
<p>在这项工作中，我们采用 h &#x3D; 8 个并行注意力层或头。对于每一个，我们使用 d<sub>k</sub> &#x3D; d<sub>v</sub> &#x3D; d<sub>model</sub>&#x2F;h &#x3D; 64。由于每个头的维度减少，总计算成本与全维度的单头注意力相似。</p>
<p><img src="/.com//KVDA6U2K.png"></p>
<p><span style="color: rgb(77, 77, 77)"><span style="background-color: rgb(255, 255, 255)">实际使用中基本使用的还是Multi-Head Attention模块。原论文中说使用多头注意力机制能够联合来自不同head部分学习到的信息。</span></span></p>
<p><img src="/.com//ZABVTXQ4.png"></p>
<p>首先还是和Self-Attention模块一样将a <sub>i</sub>分别通过W<sup>q </sup>, W<sup>k</sup> , W<sup>v</sup>得到对应的q<sup>i</sup> , k<sup>i </sup>, v<sup>i</sup>，然后再根据使用的head的数目h进一步把得到的q<sup>i</sup> , k<sup>i</sup> , v<sup>i</sup>均分成h份。比如下图中假设h &#x3D; 2 然后q<sup>1</sup>拆分成q<sup>1 , 1</sup>和q<sup>1 , 2</sup> 那么q<sup>1 , 1</sup> 就属于head1，q<sup>1 , 2</sup>属于head2。论文中论文中写的通过W<sub>i</sub><sup>Q</sup>,W<sub>i</sub><sup>K</sup>,W<sub>i</sub><sup>V</sup>映射得到每个head的Q<sub>i</sub>,K<sub>i</sub>,V<sub>i</sub>，源码中就是均分，也可以将其设置为对应值来实现均分。</p>
<p><img src="/.com//B4WW5UVF.png"></p>
<p>图中qkv的第二个上标为i的组成headi，分别进行self attention操作得到结果。</p>
<p><img src="/.com//E7I72IR7.png"></p>
<p><span style="color: rgb(77, 77, 77)"><span style="background-color: rgb(255, 255, 255)">接着将每个head得到的结果进行concat拼接</span></span></p>
<p><img src="/.com//FKSVJW38.png"></p>
<p><span style="color: rgb(77, 77, 77)"><span style="background-color: rgb(255, 255, 255)">之后经过矩阵W</span></span><sup>O</sup><span style="color: rgb(77, 77, 77)"><span style="background-color: rgb(255, 255, 255)">得到最终Multihead(Q,K,V)。注意W</span></span><sup>O</sup><span style="color: rgb(77, 77, 77)"><span style="background-color: rgb(255, 255, 255)">的大小要保证输入输出Multihead的向量长度不变。</span></span></p>
<p><img src="/.com//SAWZJZSF.png"></p>
<h3 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a><strong>3.2.3 Applications of Attention in our Model</strong></h3><p>Transformer 以三种不同的方式使用多头注意力：</p>
<p>• 在“编码器-解码器注意力”层中，查询Q来自前一个解码器层，内存键和值来自编码器的输出。这允许解码器中的每个位置都参与输入序列中的所有位置。这模仿了序列到序列模型中典型的编码器-解码器注意机制，例如[38,2,9]。<span style="color: #4eb31c">（假如输入的是一个句子有n个词，批量大小为1，实际输入编码器的是n个长度为d的向量，输入多头注意力层的从左到右是K，V，Q，一个线分为三个意思是同样一个输入它即作为K，也作为V，也作为Q，故称为自注意力机制，也就是说KVQ就是自己本身。一个向量就是一个query，每个query会得到一个输出，输入输出维度都为d相同，输出也是n个长为d的向量。输出是value的加权和，权重是query和key的相似度。所以说输出是输入的加权和，不同的输入向量计算相似度，它跟自己算相似度最大。以上是不考虑多头和有投影的情况，如果有多头和投影，这里会学习h个不一样的距离空间出来）</span></p>
<p>• 编码器包含自注意力层。在自注意力层中，所有键、值和查询都来自同一位置，在本例中是编码器中前一层的输出。编码器中的每个位置可以关注编码器上一层中的所有位置。</p>
<p>• 类似地，解码器中的自注意力层允许解码器中的每个位置关注解码器中直到并包括该位置的所有位置。我们需要防止解码器中的左向信息流以保留自回归属性。我们通过屏蔽（设置为−∞）softmax 输入中与非法连接相对应的所有值来实现缩放点积注意力的内部。参见图 2。<span style="color: #4eb31c">（解码器和编码器的区别在于输入维度m可能与编码器中输入维度n不一样。注意图2右侧上面的注意力子层，它不再是自注意力，它的KV来自于编码器的输出（n个长为d的向量，Q来自解码器下一个attention的输入（m个长为d的向量），这时候输出的权重取决于解码器输出的query和编码器输出的key的相似度。就是说根据编码器的输出，通过query把想要的一部分的东西拎出来）</span></p>
<p><img src="/.com//YA4ZPVPC.jpg"></p>
<p><span style="color: #4eb31c">图中的input与ouputembedding就是token embedding，再将其与positional encoding相加</span></p>
<h3 id="3-3-位置式前馈网络（就是MLP）"><a href="#3-3-位置式前馈网络（就是MLP）" class="headerlink" title="3.3 位置式前馈网络（就是MLP）"></a><strong>3.3 位置式前馈网络（就是MLP）</strong></h3><p>除了注意力子层之外，我们的编码器和解码器中的每个层都包含一个完全连接的前馈网络，该网络单独且相同地应用于每个位置<span style="color: #4eb31c">（position就是词，输入是句子，它是一个序列，有很多个词，一个词就是一个点，就是一个position，意思就是把每一个词都用MLP作用一遍，这个MLP是相同的。相当于把MLP作用在最后一个维度）</span>。这由两个线性变换组成，中间有一个 ReLU 激活。<span style="color: #4eb31c">（下图公式就是线性层加Relu之后再加了一个线性层，x是维度为512的向量，W1把它扩成2048的维度，因为有残差连接又要投影回去，W2把2048又降为了512）</span></p>
<p>$$<br>FFN(x) &#x3D; max(0, xW1 + b1)W2 + b2<br>$$</p>
<p>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另一种描述方式是内核大小为 1 的两个卷积。输入和输出的维度为 d<sub>model</sub> &#x3D; 512，内层的维度为 d<sub>ff </sub>&#x3D; 2048。</p>
<p><span style="color: #4eb31c">考虑最简单情况：没有残差连接，没有layernorm，单头attention，没有投影</span></p>
<p><img src="/.com//XEL34JVN.jpg"></p>
<h3 id="3-4-嵌入和Softmax"><a href="#3-4-嵌入和Softmax" class="headerlink" title="3.4 嵌入和Softmax"></a>3.4 嵌入和Softmax</h3><p>与其他序列转导模型类似，我们使用学习嵌入将输入标记（token，词）和输出标记转换为维度 d<sub>model </sub>的向量。我们还使用通常学习的线性变换和 softmax 函数将解码器输出转换为下一个token的预测概率。在我们的模型中，我们在两个嵌入层和 pre-softmax 线性变换之间共享相同的权重矩阵<span style="color: #4eb31c">（embedding层权重一样）</span>，类似于[30]。<span style="color: #ff2020">在嵌入层中，我们将这些权重乘以 √dmodel</span><span style="color: #4eb31c">（在学embeding时，多多少少会把每个向量的L2long学成相对来说比较小，比如1，无论维度多大都会学成1，维度越大学到的权重值越小，之后加上位置编码。它不会随着你的长度变长把你的long固定住，这样使得加上位置编码时在scale上差不多）。</span></p>
<h3 id="3-5-位置编码"><a href="#3-5-位置编码" class="headerlink" title="3.5 位置编码"></a>3.5 位置编码</h3><p>由于我们的模型不包含递归和卷积，为了使模型能够利用序列的顺序，我们必须注入一些有关序列中标记的相对或绝对位置的信息。为此，我们将“位置编码”添加到编码器和解码器堆栈底部的输入嵌入中。位置编码与嵌入具有相同的维度 d<sub>model</sub>，因此可以将两者相加。位置编码有多种选择，有学习的和固定的。<span style="color: #4eb31c">（attention是没有时序信息的，输出是加权和，权重是query和key之间的距离，与序列信息无关，这意味着给了一句话把顺序任意打乱，attention之后出来的结果是一样的，所以要把时序信息加进来，就是把词的位置i加到embedding里面去。词的维度是512，我们也用一个维度512的向量来表示词的位置。不论怎么打乱相加后的向量，经过attention后的值都不变，它已经把时序信息揉进向量的值里面了）</span></p>
<p>在这项工作中，我们使用不同频率的正弦和余弦函数： </p>
<p><img src="/.com//2NTHR2JZ.png"></p>
<p>其中 pos 是位置，i 是尺寸。也就是说，位置编码的每个维度对应于正弦曲线。波长形成从 2π 到 10000·2π 的几何级数。我们选择这个函数是因为我们假设它可以让模型轻松学习关注的相对位置，因为对于任何固定偏移 k，P E<sub>pos+k </sub>可以表示为PE</p>
<p>的线性函数。我们还尝试使用学习的位置嵌入，发现这两个版本产生几乎相同的结果（参见表 3 行 (E)）。我们选择正弦版本，因为它可以允许模型推断出比训练期间遇到的序列长度更长的序列长度。</p>
<p>如果仔细观察刚刚讲的Self-Attention和Multi-Head Attention模块，在计算中是没有考虑到位置信息的。假设在Self-Attention模块中，输入a1，a2,a3得到b1，b2，b3。对于a1而言，a2和a3离他&#x3D;它一样近且没有先后顺序，假设将输入的顺序改为a1，a2，a3，对结果b1没有任何影响。</p>
<p><img src="/.com//XIHQUM78.png"></p>
<p>因此，在a<sub>i</sub>上加入位置编码pe<sub>i</sub>，即pe&#x3D;{pe1，pe2，… ,pen} ，a&#x3D;{a1,a2,…,an}，pe与a有相同的维度大小。关于位置编码在原论文中有提出两种方案，一种是原论文中使用的固定编码，即论文中给出的sine and cosine functions方法，按照该方法可计算出位置编码；另一种是可训练的位置编码，作者说尝试了两种方法发现结果差不多。</p>
<p><img src="/.com//59N5SWK9.png"></p>
<h2 id="4-为什么自注意力机制"><a href="#4-为什么自注意力机制" class="headerlink" title="4 为什么自注意力机制"></a>4 为什么自注意力机制</h2><p>在本节中，我们将自注意力层的各个方面与循环层和卷积层进行比较，这些层通常用于将符号表示的一个可变长度序列（x1，…，xn）映射到另一个等长度序列（z1，.. ., zn)，其中 xi, zi ∈ R<sub>d</sub>，（n为序列长度，d为向量长度）11，例如典型序列转导编码器或解码器中的隐藏层。为了激发我们使用自我注意力，我们考虑了三个需求。</p>
<p>一是每层的<span style="color: #ffcb00">总计算复杂度。</span>另一个是可以<span style="color: #ffcb00">并行化的计算量，以所需的最小顺序操作数来衡量。</span><span style="color: #4eb31c">（顺序计算越少越好，顺序计算是指下一步计算要等前面多少步计算完成，越不要等代表并行度越高）</span></p>
<p>第三个是<span style="color: #ffcb00">网络中远程依赖之间的路径长度</span><span style="color: #4eb31c">（信息从一个数据点走到另外一个数据点要走多远，也是越短越好）</span><span style="color: #ffcb00">。</span>学习远程依赖性是许多序列转导任务中的一个关键挑战。<span style="color: #ff2020">影响学习这种依赖性的能力的一个关键因素是前向和后向信号在网络中必须经过的路径的长度。输入和输出序列中的任意位置组合之间的路径越短，学习远程依赖关系就越容易[12]。</span>因此，我们还比较了由不同层类型组成的网络中任意两个输入和输出位置之间的最大路径长度。</p>
<p>如表 1 所示，自注意力层通过恒定数量的顺序执行操作连接所有位置，而循环层需要 O(n) 顺序操作。<span style="color: #ff2020">就计算复杂度而言，当序列长度 n 小于表示维度d时，自注意力层比循环层更快。这种情况最常见于机器翻译中最先进模型所使用的句子表示，例如单词片段 [38] 和字节对 [31]的表示。为了提高涉及很长序列的任务的计算性能，自注意力可以限制为仅考虑输入序列中以相应输出位置为中心的大小为 r 的邻域。这会将最大路径长度增加到 O(n&#x2F;r)。</span>我们计划在未来的工作中进一步研究这种方法。</p>
<p><img src="/.com//3IQTSF8P.png"></p>
<p><span style="color: #4eb31c">（自注意力：K,Q都是n*<em>，得到的矩阵中一个元素d次乘法，一共n</em>n个元素</span><em><span style="color: #4eb31c">，</span></em><span style="color: #4eb31c">故自注意力是$n^{2} * d$的复杂度，矩阵计算可以认为并行度比较高，故并行计算量为1，attention里面一个query可以跟所有key做计算，输出是所有value的加权和，故任何query和任何一个很远的key-value对只需要一次计算就可以得到，故长度为1。</span></p>
<p><span style="color: #4eb31c">RNN：计算n次（d,d）*（d，1）复杂度为$nd^{2}$&lt;，后面两项很明显，所以说RNN对长序列的效果不太好</span></p>
<p><span style="color: #4eb31c">CNN：在序列上用的是1d的卷积，故它的卷积核大小就是k（一般是3，5，不大，可认为是常数），n是长度，d是输入输出通道的维数，卷积在k长度的窗口内可以一次性传递信息，若为n长度大于k的话就要取log</span></p>
<p><span style="color: #4eb31c">限制的自注意力：是指query只跟最近的r个邻居去做计算，这样会导致比较远的点要多进行几次运算才能传递信息）</span></p>
<p><span style="color: #ff2020">内核宽度 k &#x3C; n 的单个卷积层不会连接所有输入和输出位置对。这样做需要在连续卷积核的情况下要求一堆 O(n&#x2F;k) 卷积层，或者在扩张卷积的情况下需要 O(logk(n)) [18]，从而在网络中增加任意两个位置之间最长路径的长度。卷积层通常比循环层贵k倍。然而，可分离卷积[6]大大降低了复杂性，达到O(k·n·d + n·d2)。然而，即使k&#x3D;n，可分离卷积的复杂度也等于自注意力层和逐点前馈层的组合，这是我们在模型中采用的方法。</span></p>
<p>作为附带好处，自注意力可以产生更多可解释的模型。我们检查模型中的注意力分布，并在附录中展示和讨论示例。个体注意力头不仅清楚​​地学习执行不同的任务，而且许多注意力头似乎表现出与句子的句法和语义结构相关的行为。</p>
<h2 id="5-训练"><a href="#5-训练" class="headerlink" title="5 训练"></a>5 训练</h2><p>本节描述我们模型的训练制度。</p>
<h3 id="5-1-训练数据和批处理"><a href="#5-1-训练数据和批处理" class="headerlink" title="5.1 训练数据和批处理"></a>5.1 训练数据和批处理</h3><p>我们使用标准 <span style="color: #ffcb00">WMT2014英语-德语数据集</span>进行训练，该数据集包含约450万个句子对。句子使用bpe<span style="color: #4eb31c">（提取词根的方法，如果词有后缀ing，en，ed等等都算成不同的token，会导致token种类很多）</span>进行编码，该编码具有约37000个token的共享源目标词汇表<span style="color: #4eb31c">（英语德语是共享的，即不再对两种语言都构造一个字典，好处在于编码器和解码器的embedding可以用同一个字典）</span>。对于英语-法语，我们使用了更大的 WMT2014英语-法语数据集，其中包含3600万个句子，并将标记拆分为32000个单词片段词汇表 [38]。句子对按大致序列长度分批在一起。每个训练批次包含一组句子对，其中包含大约25000个源token和25000个目标token。</p>
<h3 id="5-2-硬件和时间表"><a href="#5-2-硬件和时间表" class="headerlink" title="5.2 硬件和时间表"></a>5.2 硬件和时间表</h3><p>我们在一台配备8个NVIDIA P100 GPU 的机器上训练我们的模型。对于我们使用的整篇论文中描述的超参数的基本模型，每个训练步骤大约需要0.4秒。我们对基础模型进行了总计100,000步或12小时的训练。对于我们的大型模型（如表 3 的底线所述），一个batch训练·时间1.0秒。大模型接受了300,000步（3.5 天）的训练。</p>
<h2 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 优化器</h2><p>我们使用 Adam 优化器 [20]，其中 β<sub>1</sub> &#x3D; 0.9、β<sub>2 </sub>&#x3D; 0.98 和 ε &#x3D; 10−9。我们根据以下公式在训练过程中改变学习率：</p>
<p><img src="/.com//WARC2PFH.png"></p>
<p>这对应于第一个Warmup_steps训练步骤的学习率线性增加，然后与步骤数的平方根倒数成比例地减少。我们使用了warmup_steps&#x3D;4000。</p>
<h2 id="5-4-正则化"><a href="#5-4-正则化" class="headerlink" title="5.4 正则化"></a>5.4 正则化</h2><p>我们在训练期间采用三种类型的正则化：</p>
<p>**残差 Dropout **我们将 dropout应用于每个子层的输出<span style="color: #4eb31c">（在进入残差连接和layernorm之前）</span>，然后将其添加到子层输入并进行归一化。此外，我们将 dropout 应用于编码器和解码器堆栈中的嵌入和位置编码的总和。对于基本模型，我们使用 P<sub>drop </sub>&#x3D; 0.1 的比率。</p>
<p><strong>标签平滑</strong> 在训练过程中，我们采用了值 ε<sub>ls</sub> &#x3D; 0.1 的标签平滑。这会提高模型的困惑度<span style="color: #4eb31c">（模型的不确新度，log</span><sup>loss</sup><span style="color: #4eb31c">）</span>，因为模型会变得更加不确定，但会提高准确性和 BLEU 分数。<span style="color: #4eb31c">（就是说对于正确的那个词在softmax判断的时候置信度为0.01就行了）</span></p>
<h2 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h2><h3 id="6-1-机器翻译"><a href="#6-1-机器翻译" class="headerlink" title="6.1 机器翻译"></a><strong>6.1 机器翻译</strong></h3><p><span style="color: #ff2020">在 WMT 2014 英德翻译任务中，大型 Transformer 模型（表 2 中的 Transformer (big)）比之前报告的最佳模型（包括集成）的性能高出 2.0 BLEU 以上</span>，建立了新的最佳模型艺术 BLEU 得分为 28.4。该模型的配置列于表 3 的最后一行。在 8 个 P100 GPU 上训练花费了 3.5 天。<span style="color: #ff2020">甚至我们的基础模型也超越了之前发布的所有模型和集成，而训练成本只是任何竞争模型的一小部分。</span></p>
<p>在 WMT 2014 英法翻译任务中，我们的大模型获得了 41.0 的 BLEU 分数，优于之前发布的所有单一模型，而训练成本不到之前最先进模型的 1&#x2F;4模型。针对英语到法语训练的 Transformer（大）模型使用的辍学率 P<sub>drop </sub>&#x3D; 0.1，而不是 0.3。</p>
<p>对于基本模型，我们使用通过对最后 5 个检查点进行平均而获得的单个模型，这些检查点以 10 分钟的间隔写入。对于大型模型，我们对最后 20 个检查点进行了平均。我们使用波束搜索，波束大小为 4，长度惩罚 α &#x3D; 0.6。这些超参数是在开发集上进行实验后选择的。我们将推理期间的最大输出长度设置为输入长度+ 50，但尽可能提前终止[38]。</p>
<p>表 2 总结了我们的结果，并将我们的翻译质量和训练成本与文献中的其他模型架构进行了比较。我们通过将训练时间、使用的 GPU 数量以及每个 GPU 5 的持续单精度浮点容量的估计相乘来估计用于训练模型的浮点运算数量。</p>
<h3 id="6-2-模型变化"><a href="#6-2-模型变化" class="headerlink" title="6.2 模型变化"></a>6.2 模型变化</h3><p>为了评估Transformer不同组件的重要性，我们以不同的方式改变了我们的基本模型，测量了英语到德语翻译的性能变化开发集，newstest2013。我们使用了上一节中描述的波束搜索，但没有检查点平均。我们在表 3 中列出了这些结果。</p>
<p>在表 3 行 (A) 中，我们改变注意力头的数量以及注意力键和值维度，保持计算量恒定，如第 3.2.2 节所述。虽然单头注意力比最佳设置差 0.9 BLEU，但<span style="color: #ff2020">头数过多质量也会下降</span>。</p>
<p>在表 3 行 (B) 中<span style="color: #ff2020">，我们观察到减少注意力键值大小dk会损害模型质量。</span>这表明确定兼容性并不容易，并且比点积更复杂的兼容性函数可能是有益的。我们在 (C) 和 (D) 行中进一步观察到，<span style="color: #ff2020">正如预期的那样，模型越大越好，并且 dropout 对于避免过度拟合非常有帮助。在行 (E) 中，我们用学习的位置嵌入替换正弦位置编码 [9]，并观察到与基本模型几乎相同的结果。</span></p>
<p><img src="/.com//JULPYMQV.png"></p>
<p>dff：MLP中间隐藏层的输出大小，train steps：epoch</p>
<p>看它的big模型，模型更大了于是把dropout也变大了，模型更大收敛速度也会更慢，因为学习率是变低了的，所以把epoch也变大了。</p>
<h3 id="6-3-英语选区分析"><a href="#6-3-英语选区分析" class="headerlink" title="6.3 英语选区分析"></a>6.3 英语选区分析</h3><p>为了评估 Transformer 是否可以推广到其他任务，我们对英语选区解析进行了实验。这项任务提出了具体的挑战：输出受到强大的结构约束，并且明显长于输入。此外，RNN 序列到序列模型尚未能够在小数据情况下获得最先进的结果 [37]。</p>
<p>我们在 Penn Treebank [25] 的《华尔街日报》（WSJ）部分训练了一个 dmodel &#x3D; 1024 的 4 层 Transformer，大约 40K 训练句子。我们还在半监督环境中对其进行了训练，使用了更大的高置信度和 BerkleyParser 语料库，其中包含大约 1700 万个句子 [37]。我们在仅《华尔街日报》设置中使用了 16K 个标记的词汇表，在半监督设置中使用了 32K 个标记的词汇表。</p>
<p>我们仅进行了少量实验来选择第 22 节开发集上的 dropout、注意力和残差（第 5.4 节）、学习率和波束大小，所有其他参数与英语到德语的基础翻译模型保持不变。在推理过程中，我们将最大输出长度增加到输入长度 + 300。对于仅 WSJ 和半监督设置，我们使用了 21 的波束大小和 α &#x3D; 0.3。</p>
<p>表 4 中的结果表明，<span style="color: #ff2020">尽管缺乏特定于任务的调整，我们的模型表现出奇的好，比之前报告的所有模型（循环神经网络语法除外）[8] 产生了更好的结果。</span></p>
<p>与 RNN 序列到序列模型 [37] 相比，即使仅在 WSJ 40K 句子训练集上进行训练，Transformer 的性能也优于 BerkeleyParser [29]。</p>
<h2 id="7-结论"><a href="#7-结论" class="headerlink" title="7 结论"></a>7 结论</h2><p><span style="color: #ff2020">在这项工作中，我们提出了 Transformer，这是第一个完全基于注意力的序列转换模型，用多头自注意力取代了编码器-解码器架构中最常用的循环层。</span></p>
<p><span style="color: #ff2020">对于翻译任务，Transformer 的训练速度明显快于基于循环层或卷积层的架构。在 WMT 2014 英语-德语和 WMT 2014 英语-法语翻译任务中，我们都达到了新的最先进水平。在前一项任务中，我们最好的模型甚至优于所有先前报告的集成。</span></p>
<p>我们对纯基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。我们计划将 Transformer 扩展到涉及文本以外的输入和输出模式的问题，并研究局部的、受限的注意力机制，以有效地处理图像、音频和视频等大型输入和输出。让生成的文本不那么时序化是我们的另一个研究目标。</p>
<p>我们用于训练和评估模型的代码可在 <a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor">https://github.com/tensorflow/tensor2tensor</a> 上找到。</p>
<p>**致谢 **我们感谢 Nal Kalchbrenner 和 Stephan Gouws 富有成效的评论、更正和启发。</p>
</center></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">歪比巴卜</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/09/13/Transformer/">http://example.com/2024/09/13/Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">歪比巴卜</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/09/11/hello-world/" title="Hello World"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Hello World</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/319E33068A7ED73BAE7EB48FCE321DD4.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">歪比巴卜</div><div class="author-info__description">歪比巴卜的博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kalabiqlx" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:kalabiqlx@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E5%8E%9F%E6%96%87%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">Transformer原文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">1 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%83%8C%E6%99%AF"><span class="toc-number">1.3.</span> <span class="toc-text">2 背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">1.4.</span> <span class="toc-text">3 模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8%E5%A0%86%E6%A0%88"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 编码器和解码器堆栈</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Scaled-Dot-Product-Attention%EF%BC%88%E7%82%B9%E4%B9%98%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%89"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.2.1 Scaled Dot-Product Attention（点乘注意力机制）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Multi-Head-Attention"><span class="toc-number">1.4.4.</span> <span class="toc-text">3.2.2 Multi-Head Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-Applications-of-Attention-in-our-Model"><span class="toc-number">1.4.5.</span> <span class="toc-text">3.2.3 Applications of Attention in our Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E4%BD%8D%E7%BD%AE%E5%BC%8F%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%EF%BC%88%E5%B0%B1%E6%98%AFMLP%EF%BC%89"><span class="toc-number">1.4.6.</span> <span class="toc-text">3.3 位置式前馈网络（就是MLP）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%B5%8C%E5%85%A5%E5%92%8CSoftmax"><span class="toc-number">1.4.7.</span> <span class="toc-text">3.4 嵌入和Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.4.8.</span> <span class="toc-text">3.5 位置编码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.5.</span> <span class="toc-text">4 为什么自注意力机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.</span> <span class="toc-text">5 训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-number">1.6.1.</span> <span class="toc-text">5.1 训练数据和批处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E7%A1%AC%E4%BB%B6%E5%92%8C%E6%97%B6%E9%97%B4%E8%A1%A8"><span class="toc-number">1.6.2.</span> <span class="toc-text">5.2 硬件和时间表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.7.</span> <span class="toc-text">5.3 优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.8.</span> <span class="toc-text">5.4 正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E7%BB%93%E6%9E%9C"><span class="toc-number">1.9.</span> <span class="toc-text">6 结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="toc-number">1.9.1.</span> <span class="toc-text">6.1 机器翻译</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%A8%A1%E5%9E%8B%E5%8F%98%E5%8C%96"><span class="toc-number">1.9.2.</span> <span class="toc-text">6.2 模型变化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E8%8B%B1%E8%AF%AD%E9%80%89%E5%8C%BA%E5%88%86%E6%9E%90"><span class="toc-number">1.9.3.</span> <span class="toc-text">6.3 英语选区分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%BB%93%E8%AE%BA"><span class="toc-number">1.10.</span> <span class="toc-text">7 结论</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/13/Transformer/" title="Transformer原文笔记">Transformer原文笔记</a><time datetime="2024-09-13T13:14:01.000Z" title="发表于 2024-09-13 21:14:01">2024-09-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/11/hello-world/" title="Hello World">Hello World</a><time datetime="2024-09-11T08:10:16.905Z" title="发表于 2024-09-11 16:10:16">2024-09-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By 歪比巴卜</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>